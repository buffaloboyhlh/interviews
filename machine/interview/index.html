
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Buffalo">
      
      
        <link rel="canonical" href="https://github.com/buffaloboyhlh/interviews/machine/interview/">
      
      
      
        <link rel="next" href="../../deeplearning/interview/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>机器学习 - 人工智能面试集合</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2a3383ac.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
      <link rel="stylesheet" href="../../styles/custom.css">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_2" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="人工智能面试集合" class="md-header__button md-logo" aria-label="人工智能面试集合" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            人工智能面试集合
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              机器学习
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber"  aria-label="切换至夜间模式"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="切换至夜间模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="切换至日间模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="切换至日间模式" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/buffaloboyhlh/interviews" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  机器学习

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../deeplearning/interview/" class="md-tabs__link">
        
  
  
    
  
  深度学习

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../nlp/chart01/" class="md-tabs__link">
          
  
  
    
  
  从零开始学NLP

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="人工智能面试集合" class="md-nav__button md-logo" aria-label="人工智能面试集合" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    人工智能面试集合
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/buffaloboyhlh/interviews" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    机器学习
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    机器学习
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      一、机器学习模型
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、机器学习模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 有监督学习模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 无监督学习模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 概率模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-vs" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 生成模型 VS 判别模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 模型训练流程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      二、数据预处理
    </span>
  </a>
  
    <nav class="md-nav" aria-label="二、数据预处理">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 数据清洗
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 数据转换
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 数据分割
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 其他处理
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_37" class="md-nav__link">
    <span class="md-ellipsis">
      三、线性模型
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三、线性模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 线性回归
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 逻辑回归
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32_2" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 面试题
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_55" class="md-nav__link">
    <span class="md-ellipsis">
      四、模型验证
    </span>
  </a>
  
    <nav class="md-nav" aria-label="四、模型验证">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 过拟合 &amp; 欠拟合
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 交叉验证
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 网格搜索
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 随机搜索
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#45" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 贝叶斯优化
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_66" class="md-nav__link">
    <span class="md-ellipsis">
      五、分类
    </span>
  </a>
  
    <nav class="md-nav" aria-label="五、分类">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 损失函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 评估指标
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 多分类
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 多标签
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_83" class="md-nav__link">
    <span class="md-ellipsis">
      六、回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="六、回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 损失函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 评估指标
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_104" class="md-nav__link">
    <span class="md-ellipsis">
      七、特征工程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="七、特征工程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 特征选择
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 特征提取
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_116" class="md-nav__link">
    <span class="md-ellipsis">
      八、决策树
    </span>
  </a>
  
    <nav class="md-nav" aria-label="八、决策树">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 教程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#knn" class="md-nav__link">
    <span class="md-ellipsis">
      九、KNN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="九、KNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91-knn" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 KNN教程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#svm" class="md-nav__link">
    <span class="md-ellipsis">
      十、SVM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="十、SVM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-svm" class="md-nav__link">
    <span class="md-ellipsis">
      10.1 SVM教程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_118" class="md-nav__link">
    <span class="md-ellipsis">
      十一、集成学习
    </span>
  </a>
  
    <nav class="md-nav" aria-label="十一、集成学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111" class="md-nav__link">
    <span class="md-ellipsis">
      11.1 教程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_119" class="md-nav__link">
    <span class="md-ellipsis">
      十二、无监督学习
    </span>
  </a>
  
    <nav class="md-nav" aria-label="十二、无监督学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121" class="md-nav__link">
    <span class="md-ellipsis">
      12.1 聚类
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122" class="md-nav__link">
    <span class="md-ellipsis">
      12.2 降维
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_120" class="md-nav__link">
    <span class="md-ellipsis">
      十三、 概率模型
    </span>
  </a>
  
    <nav class="md-nav" aria-label="十三、 概率模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#131" class="md-nav__link">
    <span class="md-ellipsis">
      13.1 朴素贝叶斯
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_122" class="md-nav__link">
    <span class="md-ellipsis">
      十四、其他问题
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deeplearning/interview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    深度学习
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    从零开始学NLP
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            从零开始学NLP
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nlp/chart01/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第 1 章：文本表示与词向量
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nlp/chart02/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第 2 章：RNN/LSTM 序列模型
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nlp/chart03/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第 3 章：Transformer 与 Self-Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nlp/chart04/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第 4 章：BERT / GPT 原理与微调
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nlp/chart05/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第 5 章：NLP 实战与部署
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      一、机器学习模型
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、机器学习模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 有监督学习模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 无监督学习模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 概率模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-vs" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 生成模型 VS 判别模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 模型训练流程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      二、数据预处理
    </span>
  </a>
  
    <nav class="md-nav" aria-label="二、数据预处理">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 数据清洗
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 数据转换
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 数据分割
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 其他处理
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_37" class="md-nav__link">
    <span class="md-ellipsis">
      三、线性模型
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三、线性模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 线性回归
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 逻辑回归
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32_2" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 面试题
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_55" class="md-nav__link">
    <span class="md-ellipsis">
      四、模型验证
    </span>
  </a>
  
    <nav class="md-nav" aria-label="四、模型验证">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 过拟合 &amp; 欠拟合
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 交叉验证
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 网格搜索
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 随机搜索
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#45" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 贝叶斯优化
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_66" class="md-nav__link">
    <span class="md-ellipsis">
      五、分类
    </span>
  </a>
  
    <nav class="md-nav" aria-label="五、分类">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 损失函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 评估指标
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 多分类
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 多标签
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_83" class="md-nav__link">
    <span class="md-ellipsis">
      六、回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="六、回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 损失函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 评估指标
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_104" class="md-nav__link">
    <span class="md-ellipsis">
      七、特征工程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="七、特征工程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 特征选择
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 特征提取
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_116" class="md-nav__link">
    <span class="md-ellipsis">
      八、决策树
    </span>
  </a>
  
    <nav class="md-nav" aria-label="八、决策树">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 教程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#knn" class="md-nav__link">
    <span class="md-ellipsis">
      九、KNN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="九、KNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91-knn" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 KNN教程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#svm" class="md-nav__link">
    <span class="md-ellipsis">
      十、SVM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="十、SVM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-svm" class="md-nav__link">
    <span class="md-ellipsis">
      10.1 SVM教程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_118" class="md-nav__link">
    <span class="md-ellipsis">
      十一、集成学习
    </span>
  </a>
  
    <nav class="md-nav" aria-label="十一、集成学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111" class="md-nav__link">
    <span class="md-ellipsis">
      11.1 教程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_119" class="md-nav__link">
    <span class="md-ellipsis">
      十二、无监督学习
    </span>
  </a>
  
    <nav class="md-nav" aria-label="十二、无监督学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121" class="md-nav__link">
    <span class="md-ellipsis">
      12.1 聚类
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122" class="md-nav__link">
    <span class="md-ellipsis">
      12.2 降维
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_120" class="md-nav__link">
    <span class="md-ellipsis">
      十三、 概率模型
    </span>
  </a>
  
    <nav class="md-nav" aria-label="十三、 概率模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#131" class="md-nav__link">
    <span class="md-ellipsis">
      13.1 朴素贝叶斯
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_122" class="md-nav__link">
    <span class="md-ellipsis">
      十四、其他问题
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="_1">机器学习面试题<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<h2 id="_2">一、机器学习模型<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="11">1.1 有监督学习模型<a class="headerlink" href="#11" title="Permanent link">&para;</a></h3>
<p>有监督学习模型是一种利用已知标签的训练数据来学习输入与输出之间映射关系的机器学习方法，其核心在于通过训练集中的输入特征和对应标签来调整模型参数，从而实现对新数据的准确预测。该模型主要应用于分类和回归任务，其中分类用于将数据分配到预定义类别，回归则用于预测连续数值。</p>
<p><strong>主要算法及特点</strong></p>
<table>
<thead>
<tr>
<th>算法</th>
<th>核心原理</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持向量机(SVM)</td>
<td>寻找最优决策边界以最大化类别间隔</td>
<td>在高维空间表现良好，适合小样本数据</td>
<td>对大规模数据训练时间长，对缺失数据敏感</td>
</tr>
<tr>
<td>人工神经网络(ANN)</td>
<td>模拟人脑神经元工作方式，通过多层结构处理非线性关系</td>
<td>强大的非线性建模能力，适用于复杂问题</td>
<td>训练时间长，易过拟合，对参数敏感</td>
</tr>
<tr>
<td>决策树(DT)</td>
<td>通过树形结构进行特征划分</td>
<td>易于理解和解释，可处理数值和类别数据</td>
<td>易过拟合，对数据变化敏感</td>
</tr>
<tr>
<td>朴素贝叶斯(NB)</td>
<td>基于贝叶斯定理的类条件独立性假设</td>
<td>计算效率高，适合高维数据</td>
<td>特征独立性假设在现实中往往不成立</td>
</tr>
<tr>
<td>K近邻(KNN)</td>
<td>基于距离度量找到最近的K个训练样本进行分类</td>
<td>简单易懂，无需训练过程</td>
<td>对K值选择敏感，计算量大</td>
</tr>
</tbody>
</table>
<p><img alt="有监督模型.png" src="../../imgs/machine/%E6%9C%89%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%9E%8B.png" /></p>
<h3 id="12">1.2 无监督学习模型<a class="headerlink" href="#12" title="Permanent link">&para;</a></h3>
<p>无监督学习模型是机器学习的重要分支，其核心在于直接从未标记的数据中挖掘潜在结构与内在规律，无需人工标注标签。 该模型主要任务包括聚类分析、降维处理、异常检测和关联规则学习等，广泛应用于客户细分、商品推荐、异常检测等领域。</p>
<p><img alt="无监督模型.png" src="../../imgs/machine/%E6%97%A0%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%9E%8B.png" /></p>
<h3 id="13">1.3 概率模型<a class="headerlink" href="#13" title="Permanent link">&para;</a></h3>
<p>概率模型是一类利用概率论与统计学描述数据生成机制与变量关系的数学模型。它通过联合概率分布 <span class="arithmatex">\(P(X, Y)\)</span> 建模输入 <span class="arithmatex">\(X\)</span> 与输出 <span class="arithmatex">\(Y\)</span> 的不确定性，支持推理、预测与决策。</p>
<blockquote>
<p>✅ 核心思想：将复杂系统中的不确定性显式建模，实现“在不确定中求确定”。</p>
</blockquote>
<p><img alt="概率模型.png" src="../../imgs/machine/%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.png" /></p>
<h4 id="_3">核心概率模型类型<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h4>
<p><strong>1. 贝叶斯网络（Bayesian Network）</strong></p>
<ul>
<li>类型：有向图模型（DAG）</li>
<li>结构：节点 = 随机变量，边 = 因果依赖</li>
<li>应用：医疗诊断、推荐系统、语音识别</li>
</ul>
<blockquote>
<p>🌰 示例：<br />
节点：<code>Rain</code>（是否下雨）、<code>Sprinkler</code>（洒水器是否开启）、<code>Wet Grass</code>（草地是否湿）<br />
边：<code>Rain → Wet Grass</code>，<code>Sprinkler → Wet Grass</code><br />
可计算“已知草地湿，下雨的概率”——即后验概率推理。</p>
</blockquote>
<hr />
<p><strong>2. 隐马尔可夫模型（HMM）</strong></p>
<ul>
<li>本质：结构最简单的动态贝叶斯网络</li>
<li>适用：时序数据建模（如语音、文本）</li>
<li>两大变量：<ul>
<li>隐状态序列 <span class="arithmatex">\(y_1, y_2, ..., y_n\)</span>（不可观测）</li>
<li>观测序列 <span class="arithmatex">\(x_1, x_2, ..., x_n\)</span>（可观测）</li>
</ul>
</li>
</ul>
<blockquote>
<p>📌 联合概率分解为：
$$
P(x_1,y_1,...,x_n,y_n) = P(y_1)P(x_1|y_1)\prod_{i=2}^n P(y_i|y_{i-1})P(x_i|y_i)
$$</p>
</blockquote>
<hr />
<p><strong>3. 马尔可夫随机场（MRF）与条件随机场（CRF）</strong></p>
<table>
<thead>
<tr>
<th>模型</th>
<th>类型</th>
<th>特点</th>
<th>应用</th>
</tr>
</thead>
<tbody>
<tr>
<td>MRF</td>
<td>无向图模型</td>
<td>建模变量间对称依赖（如图像像素）</td>
<td>图像处理、基因分析</td>
</tr>
<tr>
<td>CRF</td>
<td>判别式无向模型</td>
<td>直接建模 $P(Y</td>
<td>X)$，常用于序列标注</td>
</tr>
</tbody>
</table>
<h4 id="_4">概率模型的核心学习方法<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h4>
<p><strong>1. 极大似然估计（MLE）</strong></p>
<ul>
<li>目标：找到使观测数据出现概率最大的参数 <span class="arithmatex">\(\theta\)</span></li>
<li>公式：  </li>
<li>
<div class="arithmatex">\[
  \hat{\theta}_{MLE} = \arg\max_\theta P(D|\theta)
\]</div>
</li>
<li>
<p>实例：抛硬币10次得7次正面 → 估计正面概率为0.7</p>
<blockquote>
<p>🔍 实践技巧：常对似然取对数（对数似然），便于优化。</p>
</blockquote>
</li>
</ul>
<hr />
<p><strong>2. 贝叶斯学习（Bayesian Learning）</strong></p>
<ul>
<li>核心理念：参数 <span class="arithmatex">\(\theta\)</span> 是一个随机变量，具有先验分布 <span class="arithmatex">\(P(\theta)\)</span></li>
<li>更新过程：利用贝叶斯公式得到后验 <span class="arithmatex">\(P(\theta|D)\)</span>
  $$
  P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
  $$</li>
<li>优势：可融合先验知识，适合小样本场景</li>
</ul>
<blockquote>
<p>🧠 关键概念：</p>
<ul>
<li>先验概率：建模前的知识（如“某病发病率低”）</li>
<li>后验概率：观测数据后的更新信念</li>
<li>奥卡姆剃刀原理：简单模型优先，防止过拟合</li>
</ul>
</blockquote>
<hr />
<h3 id="14-vs">1.4 生成模型 VS 判别模型<a class="headerlink" href="#14-vs" title="Permanent link">&para;</a></h3>
<h4 id="1-discriminative-model">1️⃣ 判别模型（Discriminative Model）<a class="headerlink" href="#1-discriminative-model" title="Permanent link">&para;</a></h4>
<p><strong>核心思想</strong>：直接学习 <strong>条件概率</strong> ( P(y|x) )，即给定输入 (x)，预测输出 (y) 的概率。</p>
<ul>
<li>目标：<strong>区分不同类别</strong></li>
<li>重点：<strong>边界/分类</strong></li>
<li>常见方法：<ul>
<li>逻辑回归（Logistic Regression）</li>
<li>支持向量机（SVM）</li>
<li>条件随机场（CRF）</li>
<li>神经网络分类器</li>
</ul>
</li>
</ul>
<p><strong>数学表达</strong>：</p>
<div class="arithmatex">\[
\hat{y} = \arg\max_y P(y|x)
\]</div>
<p>训练时直接优化损失函数（比如交叉熵）：</p>
<div class="arithmatex">\[
\mathcal{L} = - \sum_i y_i \log P(y_i|x_i)
\]</div>
<p><strong>直观理解</strong>：</p>
<p>判别模型像一个法官，专注于 <strong>判断 A 和 B 哪个可能性更大</strong>，不关心输入是怎么生成的。</p>
<h4 id="2-generative-model">2️⃣ 生成模型（Generative Model）<a class="headerlink" href="#2-generative-model" title="Permanent link">&para;</a></h4>
<p><strong>核心思想</strong>：学习 <strong>联合概率</strong> ( P(x, y) ) 或者数据分布 ( P(x) )，从而能生成数据。</p>
<ul>
<li>目标：<strong>建模数据分布，生成新样本</strong></li>
<li>重点：<strong>数据本身</strong></li>
<li>
<p>常见方法：</p>
<ul>
<li>高斯混合模型（GMM）</li>
<li>朴素贝叶斯（Naive Bayes）</li>
<li>隐马尔可夫模型（HMM）</li>
<li>变分自编码器（VAE）</li>
<li>生成对抗网络（GAN）</li>
<li>大语言模型（LLM，如 GPT 系列）</li>
</ul>
</li>
</ul>
<p><strong>数学表达</strong>：</p>
<ol>
<li>
<p>对于分类任务：
$$
   P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$</p>
</li>
<li>
<p>对于生成任务（无标签）：
$$
   P(x) \quad \text{或者} \quad P(x|z) \text{，其中 z 是潜变量}
$$</p>
</li>
</ol>
<p><strong>直观理解</strong>：</p>
<p>生成模型像一个画家，不仅能说“这是猫还是狗”，还能 <strong>画出一只新的猫或狗</strong>。</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>判别模型</th>
<th>生成模型</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>学习目标</td>
<td>条件概率 (P(y</td>
<td>x))</td>
<td>联合概率 (P(x, y)) 或 (P(x))</td>
</tr>
<tr>
<td>预测能力</td>
<td>分类、回归</td>
<td>生成、分类</td>
<td></td>
</tr>
<tr>
<td>数据建模</td>
<td>不关心数据分布</td>
<td>学习数据分布</td>
<td></td>
</tr>
<tr>
<td>优势</td>
<td>边界清晰，分类精度高</td>
<td>可以生成新样本，适应半监督学习</td>
<td></td>
</tr>
<tr>
<td>劣势</td>
<td>不能生成样本</td>
<td>分类精度可能低于判别模型</td>
<td></td>
</tr>
<tr>
<td>示例</td>
<td>Logistic Regression, SVM, DNN 分类器</td>
<td>Naive Bayes, GMM, HMM, VAE, GAN, GPT</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="15">1.5 模型训练流程<a class="headerlink" href="#15" title="Permanent link">&para;</a></h3>
<p>机器学习模型训练可以抽象为 <strong>一个迭代优化过程</strong>，大致流程如下：</p>
<ol>
<li><strong>明确问题与目标</strong></li>
<li><strong>收集数据</strong></li>
<li><strong>数据预处理与特征工程</strong></li>
<li><strong>选择模型</strong></li>
<li><strong>定义损失函数和优化器</strong></li>
<li><strong>训练模型（模型拟合）</strong></li>
<li><strong>模型评估与调参</strong></li>
<li><strong>模型部署与监控</strong></li>
</ol>
<hr />
<h4 id="1">1️⃣ 明确问题与目标<a class="headerlink" href="#1" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>任务类型</strong>：</p>
<ul>
<li>分类（Classification）：预测类别 <span class="arithmatex">\((y \in {0,1,...,K})\)</span></li>
<li>回归（Regression）：预测连续值 <span class="arithmatex">\((y \in \mathbb{R})\)</span></li>
<li>排序/推荐、聚类、生成等</li>
</ul>
</li>
<li>
<p><strong>目标指标</strong>：</p>
<ul>
<li>分类：准确率（Accuracy）、F1、ROC-AUC</li>
<li>回归：MSE、MAE、R²</li>
</ul>
</li>
<li>
<p><strong>约束条件</strong>：</p>
<ul>
<li>训练时间、模型大小、可解释性等</li>
</ul>
</li>
</ul>
<blockquote>
<p>🔹 小贴士：问题定义直接决定后续数据收集、模型选择和评估方法。</p>
</blockquote>
<hr />
<h4 id="2">2️⃣ 数据收集<a class="headerlink" href="#2" title="Permanent link">&para;</a></h4>
<ul>
<li>数据是 ML 的核心，质量决定模型上限</li>
<li>
<p>来源：</p>
<ul>
<li>公开数据集（Kaggle、UCI）</li>
<li>企业业务数据（数据库、日志）</li>
<li>传感器或爬虫采集</li>
</ul>
</li>
<li>
<p>注意：</p>
<ul>
<li>数据量是否足够</li>
<li>标签是否准确（监督学习）</li>
</ul>
</li>
</ul>
<hr />
<h4 id="3">3️⃣ 数据预处理与特征工程<a class="headerlink" href="#3" title="Permanent link">&para;</a></h4>
<h5 id="_5">数据清洗：<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h5>
<ul>
<li>缺失值处理：填充、删除或标记</li>
<li>异常值处理：去除或修正</li>
<li>数据类型转换</li>
</ul>
<h5 id="_6">特征处理：<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h5>
<ul>
<li>数值归一化/标准化</li>
<li>类别变量编码（One-hot、Label Encoding）</li>
<li>特征组合或降维（PCA、SVD）</li>
</ul>
<h5 id="_7">特征选择：<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h5>
<ul>
<li>相关性分析、方差选择、树模型特征重要性</li>
<li>避免高维稀疏或噪声特征</li>
</ul>
<blockquote>
<p>🔹 小贴士：好的特征比复杂模型更重要。</p>
</blockquote>
<hr />
<h4 id="4">4️⃣ 选择模型<a class="headerlink" href="#4" title="Permanent link">&para;</a></h4>
<p>根据问题类型和数据特性选择合适的算法：</p>
<table>
<thead>
<tr>
<th>问题类型</th>
<th>经典算法</th>
<th>深度学习算法</th>
</tr>
</thead>
<tbody>
<tr>
<td>分类</td>
<td>逻辑回归、SVM、决策树、随机森林</td>
<td>MLP、CNN、Transformer</td>
</tr>
<tr>
<td>回归</td>
<td>线性回归、岭回归、树回归</td>
<td>MLP、LSTM</td>
</tr>
<tr>
<td>聚类</td>
<td>K-Means、GMM</td>
<td>自编码器 + 聚类</td>
</tr>
<tr>
<td>生成</td>
<td>Naive Bayes、GMM</td>
<td>GAN、VAE、Diffusion Model</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="5">5️⃣ 定义损失函数与优化器<a class="headerlink" href="#5" title="Permanent link">&para;</a></h4>
<h5 id="loss-function">损失函数（Loss Function）：<a class="headerlink" href="#loss-function" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>分类</strong>：交叉熵损失
$$
  \mathcal{L} = - \sum_i y_i \log \hat{y}_i
$$</li>
<li><strong>回归</strong>：均方误差（MSE）
$$
  \mathcal{L} = \frac{1}{n}\sum_i (\hat{y}_i - y_i)^2
$$</li>
</ul>
<h5 id="optimizer">优化器（Optimizer）：<a class="headerlink" href="#optimizer" title="Permanent link">&para;</a></h5>
<ul>
<li>通过梯度下降优化模型参数 (\theta)</li>
<li>
<p>常用：</p>
<ul>
<li>SGD、Momentum、Adam、RMSProp</li>
</ul>
</li>
</ul>
<h5 id="_8">数学本质：<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h5>
<ul>
<li>找到最优参数 (\theta^*)：</li>
</ul>
<div class="arithmatex">\[
  \theta^* = \arg\min_\theta \mathcal{L}(\theta)
\]</div>
<hr />
<h4 id="6">6️⃣ 模型训练（拟合）<a class="headerlink" href="#6" title="Permanent link">&para;</a></h4>
<ul>
<li>将训练数据输入模型</li>
<li>计算预测值 (\hat{y})</li>
<li>根据损失函数计算梯度</li>
<li>更新参数（梯度下降）</li>
<li><strong>迭代多次（epoch）</strong>，直到收敛或达到指定轮数</li>
</ul>
<p>💡 注意：</p>
<ul>
<li>批量训练（Batch） vs 随机梯度下降（SGD）</li>
<li>
<p>防止过拟合：</p>
<ul>
<li>正则化（L1、L2）</li>
<li>Dropout（深度学习）</li>
<li>提前停止（Early Stopping）</li>
</ul>
</li>
</ul>
<hr />
<h4 id="7">7️⃣ 模型评估与调参<a class="headerlink" href="#7" title="Permanent link">&para;</a></h4>
<h5 id="_9">评估方法：<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h5>
<ul>
<li>
<p>拆分数据集：</p>
<ul>
<li>训练集 / 验证集 / 测试集</li>
</ul>
</li>
<li>
<p>交叉验证（K-Fold CV）</p>
</li>
<li>
<p>指标选择：</p>
<ul>
<li>分类：Accuracy、Precision、Recall、F1</li>
<li>回归：MSE、MAE、R²</li>
</ul>
</li>
</ul>
<h5 id="_10">超参数调优：<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h5>
<ul>
<li>网格搜索（Grid Search）</li>
<li>随机搜索（Random Search）</li>
<li>贝叶斯优化</li>
<li>AutoML 工具</li>
</ul>
<blockquote>
<p>🔹 小贴士：不要在测试集上调参，只在验证集上优化模型。</p>
</blockquote>
<hr />
<h4 id="8">8️⃣ 模型部署与监控<a class="headerlink" href="#8" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>部署方式：</p>
<ul>
<li>本地服务（Flask/FastAPI）</li>
<li>云服务（AWS Sagemaker, Azure ML）</li>
</ul>
</li>
<li>
<p>模型监控：</p>
<ul>
<li>精度随时间下降（概念漂移）</li>
<li>输入分布变化</li>
</ul>
</li>
<li>
<p>定期更新模型，保持性能</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code>问题定义 → 数据收集 → 数据清洗/特征工程 → 模型选择 → 损失函数+优化器
→ 训练模型 → 模型评估与调参 → 部署与监控
</code></pre></div>
<hr />
<h2 id="_11">二、数据预处理<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h2>
<h3 id="21">2.1 数据清洗<a class="headerlink" href="#21" title="Permanent link">&para;</a></h3>
<h4 id="_12">缺失值处理：删除、填充（均值、中位数、众数）、插值法等<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h4>
<p>在实际数据中，经常会遇到 <strong>部分数据缺失</strong> 的情况，例如：</p>
<table>
<thead>
<tr>
<th>姓名</th>
<th>年龄</th>
<th>工资</th>
</tr>
</thead>
<tbody>
<tr>
<td>张三</td>
<td>25</td>
<td>5000</td>
</tr>
<tr>
<td>李四</td>
<td>NaN</td>
<td>6000</td>
</tr>
<tr>
<td>王五</td>
<td>30</td>
<td>NaN</td>
</tr>
</tbody>
</table>
<p>这里的 <code>NaN</code> 就表示缺失值（Not a Number）。</p>
<p>缺失值会导致：</p>
<ul>
<li>统计指标偏差（均值、方差不准确）</li>
<li>机器学习模型报错或性能下降</li>
</ul>
<p>所以需要 <strong>合理处理缺失值</strong>。</p>
<p><strong>先分析缺失情况</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="n">df</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># 缺失比例</span>
</code></pre></div>
<hr />
<h5 id="1-deletion">1️⃣ 删除法（Deletion）<a class="headerlink" href="#1-deletion" title="Permanent link">&para;</a></h5>
<p><strong>思路</strong>：直接删除缺失值所在的行或列。</p>
<ul>
<li>
<p><strong>删除行（Row-wise deletion）</strong></p>
<ul>
<li>方法：<code>dropna(axis=0)</code></li>
<li>适用场景：缺失值较少，删除不会丢失太多信息</li>
<li>缺点：丢失信息，如果缺失值很多，会导致数据量严重不足</li>
</ul>
</li>
<li>
<p><strong>删除列（Column-wise deletion）</strong></p>
<ul>
<li>方法：<code>dropna(axis=1)</code></li>
<li>适用场景：某列缺失值过多且不重要</li>
<li>缺点：可能丢失有价值特征</li>
</ul>
</li>
</ul>
<p><strong>示例（Pandas）</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;姓名&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;张三&#39;</span><span class="p">,</span><span class="s1">&#39;李四&#39;</span><span class="p">,</span><span class="s1">&#39;王五&#39;</span><span class="p">],</span>
    <span class="s1">&#39;年龄&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span>
    <span class="s1">&#39;工资&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">6000</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># 删除含有缺失值的行</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 删除含有缺失值的列</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h5 id="2-imputation">2️⃣ 填充法（Imputation）<a class="headerlink" href="#2-imputation" title="Permanent link">&para;</a></h5>
<p><strong>思路</strong>：用某个合理的值代替缺失值。</p>
<h6 id="21-mean-imputation">2.1 均值填充（Mean Imputation）<a class="headerlink" href="#21-mean-imputation" title="Permanent link">&para;</a></h6>
<ul>
<li>将缺失值用该列的平均值填充</li>
<li>适合<strong>连续型数据</strong></li>
<li>优点：简单，易实现</li>
<li>缺点：会降低数据方差，可能影响模型</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">df</span><span class="p">[</span><span class="s1">&#39;年龄&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;年龄&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h6 id="22-median-imputation">2.2 中位数填充（Median Imputation）<a class="headerlink" href="#22-median-imputation" title="Permanent link">&para;</a></h6>
<ul>
<li>将缺失值用该列的中位数填充</li>
<li>适合<strong>有异常值的连续型数据</strong></li>
<li>优点：不受极端值影响</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">df</span><span class="p">[</span><span class="s1">&#39;年龄&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;年龄&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h6 id="23-mode-imputation">2.3 众数填充（Mode Imputation）<a class="headerlink" href="#23-mode-imputation" title="Permanent link">&para;</a></h6>
<ul>
<li>将缺失值用该列最常出现的值填充</li>
<li>适合<strong>类别型数据</strong></li>
<li>优点：保留类别特征分布</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">df</span><span class="p">[</span><span class="s1">&#39;性别&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;性别&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mode</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h6 id="24-constant-imputation">2.4 固定值填充（Constant Imputation）<a class="headerlink" href="#24-constant-imputation" title="Permanent link">&para;</a></h6>
<ul>
<li>用固定值填充，例如 0、-1、"未知"</li>
<li>适合<strong>缺失本身有含义的情况</strong></li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<hr />
<h5 id="3-interpolation">3️⃣ 插值法（Interpolation）<a class="headerlink" href="#3-interpolation" title="Permanent link">&para;</a></h5>
<p><strong>思路</strong>：利用已有数据的趋势或模式来预测缺失值</p>
<ul>
<li>适合<strong>时间序列数据或连续数据</strong></li>
<li>
<p>常见方法：</p>
<ul>
<li>线性插值（Linear）</li>
<li>多项式插值（Polynomial）</li>
<li>时间序列插值（Time）</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>优点：保留数据趋势，适合连续型和时间序列</li>
<li>缺点：不适合类别型数据；可能引入偏差</li>
</ul>
<hr />
<h5 id="4_1">4️⃣ 高级填充方法<a class="headerlink" href="#4_1" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>KNN 填充</strong>：用相似样本的平均值填充</li>
<li><strong>回归填充</strong>：用其他特征预测缺失值</li>
<li><strong>多重插补（MICE）</strong>：用多次预测填充，保留数据分布</li>
</ul>
<blockquote>
<p>这些方法可以提高预测精度，但计算复杂度更高。</p>
</blockquote>
<hr />
<p><strong>缺失值处理选择指南</strong></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>适用场景</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>删除</td>
<td>缺失值少、数据量大</td>
<td>简单</td>
<td>丢失信息、可能引入偏差</td>
</tr>
<tr>
<td>均值/中位数</td>
<td>连续型特征</td>
<td>简单、易实现</td>
<td>方差降低、可能引入偏差</td>
</tr>
<tr>
<td>众数</td>
<td>类别型特征</td>
<td>保留类别分布</td>
<td>无法处理连续型</td>
</tr>
<tr>
<td>插值</td>
<td>时间序列、连续型数据</td>
<td>保留趋势</td>
<td>不适合类别型数据</td>
</tr>
<tr>
<td>高级方法</td>
<td>对精度要求高、缺失模式复杂</td>
<td>更合理、保留数据分布</td>
<td>计算复杂、实现复杂</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="_13">异常值处理：删除、视为缺失值、修正或保留（根据业务逻辑）<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h4>
<p><strong>异常值（Outlier）</strong> 是指在数据集中 <strong>显著偏离其他观测值的数据点</strong>。</p>
<ul>
<li>例子：工资为 10 万元，而大多数员工工资在 3-5 千元之间。</li>
<li>
<p>异常值可能来源：</p>
<ul>
<li>数据录入错误</li>
<li>仪器测量错误</li>
<li>真实的极端值（罕见事件）</li>
</ul>
</li>
</ul>
<blockquote>
<p>异常值如果不处理，可能导致统计指标失真或模型性能下降。</p>
</blockquote>
<p><strong>先检测，再处理</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">boxplot</span><span class="p">()</span>
</code></pre></div>
<h5 id="_14">检测异常值的方法<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h5>
<h6 id="1_1">1️⃣ 基于统计量<a class="headerlink" href="#1_1" title="Permanent link">&para;</a></h6>
<ul>
<li><strong>标准差法</strong>：
$$
  x \text{ 是异常值 if } |x-\bar{x}| &gt; k\sigma
$$
  常用 (k=3)</li>
<li><strong>IQR法（四分位距）</strong>：
$$
  \text{IQR} = Q_3 - Q_1
$$
$$
  x \text{ 是异常值 if } x &lt; Q_1 - 1.5 \cdot IQR \text{ 或 } x &gt; Q_3 + 1.5 \cdot IQR
$$</li>
</ul>
<h6 id="2_1">2️⃣ 基于模型<a class="headerlink" href="#2_1" title="Permanent link">&para;</a></h6>
<ul>
<li><strong>Z-score</strong></li>
<li><strong>Isolation Forest</strong></li>
<li><strong>Local Outlier Factor (LOF)</strong></li>
</ul>
<hr />
<h5 id="_15">异常值处理方法<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h5>
<h6 id="1_2">1️⃣ 删除法<a class="headerlink" href="#1_2" title="Permanent link">&para;</a></h6>
<p><strong>思路</strong>：直接删除异常值对应的行。</p>
<ul>
<li>
<p>优点：</p>
<ul>
<li>简单、快速</li>
<li>适合异常值很少的情况</li>
</ul>
</li>
<li>
<p>缺点：</p>
<ul>
<li>丢失信息</li>
<li>不适合异常值可能有实际意义的情况</li>
</ul>
</li>
</ul>
<p><strong>Pandas 示例</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;工资&#39;</span><span class="p">:[</span><span class="mi">5000</span><span class="p">,</span><span class="mi">6000</span><span class="p">,</span><span class="mi">7000</span><span class="p">,</span><span class="mi">100000</span><span class="p">]})</span>
<span class="n">Q1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">Q3</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">IQR</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">-</span> <span class="n">Q1</span>
<span class="n">df_clean</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">Q1</span> <span class="o">-</span> <span class="mf">1.5</span><span class="o">*</span><span class="n">IQR</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">Q3</span> <span class="o">+</span> <span class="mf">1.5</span><span class="o">*</span><span class="n">IQR</span><span class="p">)]</span>
</code></pre></div>
<hr />
<h6 id="2-nan">2️⃣ 视为缺失值（NaN）<a class="headerlink" href="#2-nan" title="Permanent link">&para;</a></h6>
<p><strong>思路</strong>：把异常值标记为缺失值，再用缺失值处理方法填充。</p>
<ul>
<li>
<p>优点：</p>
<ul>
<li>可以结合均值/中位数/插值等方法</li>
<li>保留数据量</li>
</ul>
</li>
<li>
<p>缺点：</p>
<ul>
<li>填充值可能不准确</li>
<li>需要合理选择填充值</li>
</ul>
</li>
</ul>
<p><strong>Pandas 示例</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">Q3</span> <span class="o">+</span> <span class="mf">1.5</span><span class="o">*</span><span class="n">IQR</span><span class="p">),</span> <span class="s1">&#39;工资&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<hr />
<h6 id="3-capping-winsorization">3️⃣ 修正法（Capping 或 Winsorization）<a class="headerlink" href="#3-capping-winsorization" title="Permanent link">&para;</a></h6>
<p><strong>思路</strong>：把异常值替换为上限或下限值。</p>
<ul>
<li>
<p>优点：</p>
<ul>
<li>保留数据量</li>
<li>减少异常值对模型的影响</li>
</ul>
</li>
<li>
<p>缺点：</p>
<ul>
<li>可能扭曲数据分布</li>
</ul>
</li>
</ul>
<p><strong>Pandas 示例</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="n">lower_bound</span> <span class="o">=</span> <span class="n">Q1</span> <span class="o">-</span> <span class="mf">1.5</span><span class="o">*</span><span class="n">IQR</span>
<span class="n">upper_bound</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">+</span> <span class="mf">1.5</span><span class="o">*</span><span class="n">IQR</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;工资&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span>
</code></pre></div>
<hr />
<h6 id="4_2">4️⃣ 保留法<a class="headerlink" href="#4_2" title="Permanent link">&para;</a></h6>
<p><strong>思路</strong>：对异常值不做处理，直接保留</p>
<ul>
<li>
<p>适用场景：</p>
<ul>
<li>异常值是 <strong>真实的极端事件</strong>，对业务有意义</li>
<li>如金融风控中的欺诈交易</li>
</ul>
</li>
<li>
<p>优点：</p>
<ul>
<li>保留完整信息</li>
</ul>
</li>
<li>
<p>缺点：</p>
<ul>
<li>可能影响模型训练和统计指标</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>方法</th>
<th>适用场景</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>删除</td>
<td>异常值少，可能是错误数据</td>
<td>简单快速</td>
<td>丢失信息</td>
</tr>
<tr>
<td>视为缺失值</td>
<td>异常值可用填充替代</td>
<td>可结合缺失值处理方法</td>
<td>填充值可能不准确</td>
</tr>
<tr>
<td>修正（Capping）</td>
<td>异常值对模型影响大，但数据量不能丢</td>
<td>保留数据量，减少影响</td>
<td>可能改变分布</td>
</tr>
<tr>
<td>保留</td>
<td>异常值是真实有效事件</td>
<td>保留信息</td>
<td>可能影响模型</td>
</tr>
</tbody>
</table>
<h4 id="_16">重复数据处理：识别并删除完全重复的样本<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h4>
<p><strong>重复数据</strong> 是指在数据集中 <strong>完全相同或部分字段相同的样本行</strong>。
例如：</p>
<table>
<thead>
<tr>
<th>姓名</th>
<th>年龄</th>
<th>城市</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>张三</td>
<td>25</td>
<td>北京</td>
<td></td>
</tr>
<tr>
<td>张三</td>
<td>25</td>
<td>北京</td>
<td>← 重复行</td>
</tr>
<tr>
<td>李四</td>
<td>30</td>
<td>上海</td>
<td></td>
</tr>
</tbody>
</table>
<p>重复数据可能来源：</p>
<ul>
<li>数据多次导入</li>
<li>采集系统错误</li>
<li>数据合并（merge/concat）时未去重</li>
</ul>
<h5 id="_17">重复数据的识别<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h5>
<p>在 <strong>Pandas</strong> 中常用方法是 <code>duplicated()</code>：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;姓名&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;张三&#39;</span><span class="p">,</span> <span class="s1">&#39;张三&#39;</span><span class="p">,</span> <span class="s1">&#39;李四&#39;</span><span class="p">,</span> <span class="s1">&#39;王五&#39;</span><span class="p">,</span> <span class="s1">&#39;李四&#39;</span><span class="p">],</span>
    <span class="s1">&#39;年龄&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span>
    <span class="s1">&#39;城市&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;北京&#39;</span><span class="p">,</span> <span class="s1">&#39;北京&#39;</span><span class="p">,</span> <span class="s1">&#39;上海&#39;</span><span class="p">,</span> <span class="s1">&#39;广州&#39;</span><span class="p">,</span> <span class="s1">&#39;上海&#39;</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># 判断是否重复（返回布尔值）</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">duplicated</span><span class="p">())</span>

<span class="c1"># 查看重复的样本</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">duplicated</span><span class="p">()])</span>
</code></pre></div>
<p>输出：</p>
<div class="highlight"><pre><span></span><code>0    False
1     True
2    False
3    False
4     True
dtype: bool
</code></pre></div>
<hr />
<h6 id="1_3">1️⃣ 删除完全重复的行<a class="headerlink" href="#1_3" title="Permanent link">&para;</a></h6>
<div class="highlight"><pre><span></span><code><span class="n">df_clean</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span>
</code></pre></div>
<blockquote>
<p>默认根据所有列去重，仅保留第一次出现的记录。</p>
</blockquote>
<hr />
<h6 id="2_2">2️⃣ 指定列进行去重<a class="headerlink" href="#2_2" title="Permanent link">&para;</a></h6>
<p>有时只想根据某几列判断是否重复，比如 “姓名 + 城市”：</p>
<div class="highlight"><pre><span></span><code><span class="n">df_clean</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;姓名&#39;</span><span class="p">,</span> <span class="s1">&#39;城市&#39;</span><span class="p">])</span>
</code></pre></div>
<hr />
<h6 id="3_1">3️⃣ 保留最后一次出现的记录<a class="headerlink" href="#3_1" title="Permanent link">&para;</a></h6>
<div class="highlight"><pre><span></span><code><span class="n">df_clean</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="n">keep</span><span class="o">=</span><span class="s1">&#39;last&#39;</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><code>keep='first'</code>（默认）：保留第一次出现的记录</li>
<li><code>keep='last'</code>：保留最后一次出现的记录</li>
<li><code>keep=False</code>：删除所有重复项</li>
</ul>
<hr />
<h6 id="4_3">4️⃣ 查看重复的数量<a class="headerlink" href="#4_3" title="Permanent link">&para;</a></h6>
<div class="highlight"><pre><span></span><code><span class="n">duplicate_count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">duplicated</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;共有 </span><span class="si">{</span><span class="n">duplicate_count</span><span class="si">}</span><span class="s2"> 条重复样本&quot;</span><span class="p">)</span>
</code></pre></div>
<hr />
<table>
<thead>
<tr>
<th>方法</th>
<th>代码示例</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>检查重复</td>
<td><code>df.duplicated()</code></td>
<td>返回布尔Series</td>
</tr>
<tr>
<td>查看重复行</td>
<td><code>df[df.duplicated()]</code></td>
<td>显示所有重复记录</td>
</tr>
<tr>
<td>删除重复</td>
<td><code>df.drop_duplicates()</code></td>
<td>删除重复记录</td>
</tr>
<tr>
<td>指定列去重</td>
<td><code>df.drop_duplicates(subset=['列名'])</code></td>
<td>按指定列判断重复</td>
</tr>
<tr>
<td>保留最后</td>
<td><code>keep='last'</code></td>
<td>保留最后一条重复记录</td>
</tr>
<tr>
<td>删除所有重复</td>
<td><code>keep=False</code></td>
<td>所有重复的都删除</td>
</tr>
</tbody>
</table>
<h3 id="22">2.2 数据转换<a class="headerlink" href="#22" title="Permanent link">&para;</a></h3>
<h4 id="z-scoremin-max-scaling">特征缩放：标准化（Z-score）、归一化（Min-Max Scaling）等，消除量纲影响<a class="headerlink" href="#z-scoremin-max-scaling" title="Permanent link">&para;</a></h4>
<p>不同特征往往具有 <strong>不同的量纲（单位）和取值范围</strong>：</p>
<table>
<thead>
<tr>
<th>特征</th>
<th>含义</th>
<th>取值范围</th>
</tr>
</thead>
<tbody>
<tr>
<td>身高</td>
<td>单位：cm</td>
<td>150 ~ 190</td>
</tr>
<tr>
<td>体重</td>
<td>单位：kg</td>
<td>40 ~ 90</td>
</tr>
<tr>
<td>收入</td>
<td>单位：元</td>
<td>3,000 ~ 30,000</td>
</tr>
</tbody>
</table>
<p>👉 在这种情况下：</p>
<ul>
<li><strong>距离度量类算法</strong>（如 KNN、K-Means）会被数值大的特征主导。</li>
<li><strong>梯度下降算法</strong>（如线性回归、神经网络）会因不同特征尺度不同导致收敛缓慢或震荡。</li>
</ul>
<p>✅ 通过特征缩放，使所有特征处于<strong>相似的数值范围</strong>，从而：</p>
<ul>
<li>提高模型收敛速度</li>
<li>避免特征“主导效应”</li>
<li>提高训练稳定性</li>
</ul>
<hr />
<h5 id="_18">🧮 常见特征缩放方法<a class="headerlink" href="#_18" title="Permanent link">&para;</a></h5>
<h6 id="1-standardization-z-score-normalization">1️⃣ 标准化（Standardization / Z-score Normalization）<a class="headerlink" href="#1-standardization-z-score-normalization" title="Permanent link">&para;</a></h6>
<p><strong>公式：</strong>
$$
x' = \frac{x - \mu}{\sigma}
$$</p>
<ul>
<li><span class="arithmatex">\(\mu\)</span>：均值（mean）</li>
<li><span class="arithmatex">\(\sigma\)</span>：标准差（standard deviation）</li>
</ul>
<p>👉 缩放后数据服从 <strong>均值为0、标准差为1</strong> 的分布。</p>
<p><strong>适用场景：</strong></p>
<ul>
<li>数据近似符合正态分布</li>
<li>线性模型（Logistic Regression, SVM, PCA）</li>
<li>神经网络输入层</li>
</ul>
<p><strong>Python 实现：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;身高&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">160</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">180</span><span class="p">],</span>
    <span class="s1">&#39;体重&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">80</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
</code></pre></div>
<p>输出：</p>
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
</tr>
</thead>
<tbody>
<tr>
<td>-1.22</td>
<td>-1.22</td>
</tr>
<tr>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>1.22</td>
<td>1.22</td>
</tr>
</tbody>
</table>
<hr />
<h6 id="2-min-max-scaling">2️⃣ 归一化（Min-Max Scaling）<a class="headerlink" href="#2-min-max-scaling" title="Permanent link">&para;</a></h6>
<p><strong>公式：</strong>
$$
x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
$$</p>
<ul>
<li>将数据缩放至 <code>[0, 1]</code> 或自定义区间 <code>[a, b]</code>。</li>
</ul>
<p><strong>适用场景：</strong></p>
<ul>
<li>数据没有明显的正态分布</li>
<li>神经网络输入层（尤其是 Sigmoid / Tanh）</li>
<li>需要固定区间的算法</li>
</ul>
<p><strong>Python 实现：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
</code></pre></div>
<p>输出：</p>
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr>
<td>1.0</td>
<td>1.0</td>
</tr>
</tbody>
</table>
<hr />
<h6 id="3-robust-scaling">3️⃣ 稳健缩放（Robust Scaling）<a class="headerlink" href="#3-robust-scaling" title="Permanent link">&para;</a></h6>
<p><strong>公式：</strong>
$$
x' = \frac{x - \text{Median}(x)}{IQR}
$$
其中 (IQR = Q3 - Q1)（四分位距）。</p>
<p><strong>适用场景：</strong></p>
<ul>
<li>数据中存在异常值（outliers）</li>
<li>对异常值不敏感的模型</li>
</ul>
<p><strong>Python 实现：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">RobustScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">RobustScaler</span><span class="p">()</span>
<span class="n">scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
</code></pre></div>
<hr />
<h6 id="4-l2-normalization">4️⃣ 单位向量化（L2 Normalization）<a class="headerlink" href="#4-l2-normalization" title="Permanent link">&para;</a></h6>
<p><strong>公式：</strong>
$$
x' = \frac{x}{|x|}
$$
将每个样本缩放为单位长度（即向量长度为1）。</p>
<p><strong>适用场景：</strong></p>
<ul>
<li>文本向量（TF-IDF、词嵌入）</li>
<li>余弦相似度度量任务</li>
</ul>
<p><strong>Python 实现：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Normalizer</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">)</span>
<span class="n">scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
</code></pre></div>
<hr />
<table>
<thead>
<tr>
<th>方法</th>
<th>公式</th>
<th>结果范围</th>
<th>是否抗异常值</th>
<th>典型应用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>标准化</strong></td>
<td>((x - μ)/σ)</td>
<td>无界（均值0，方差1）</td>
<td>❌</td>
<td>SVM, PCA, LR</td>
</tr>
<tr>
<td><strong>归一化</strong></td>
<td>((x - min)/(max - min))</td>
<td>[0, 1]</td>
<td>❌</td>
<td>神经网络</td>
</tr>
<tr>
<td><strong>稳健缩放</strong></td>
<td>((x - Median)/IQR)</td>
<td>无界</td>
<td>✅</td>
<td>含异常值数据</td>
</tr>
<tr>
<td><strong>单位向量化</strong></td>
<td>(x/‖x‖)</td>
<td>向量长度=1</td>
<td>✅</td>
<td>文本相似度</td>
</tr>
</tbody>
</table>
<h4 id="one-hot-encoding">编码分类变量：独热编码（One-Hot Encoding）、标签编码等，将类别数据转为数值数据<a class="headerlink" href="#one-hot-encoding" title="Permanent link">&para;</a></h4>
<p>机器学习算法（尤其是线性模型、神经网络）只能处理<strong>数值型特征</strong>，
而现实数据常包含大量<strong>类别特征（categorical features）</strong>，例如：</p>
<table>
<thead>
<tr>
<th>性别</th>
<th>城市</th>
<th>教育水平</th>
</tr>
</thead>
<tbody>
<tr>
<td>男</td>
<td>北京</td>
<td>本科</td>
</tr>
<tr>
<td>女</td>
<td>上海</td>
<td>硕士</td>
</tr>
<tr>
<td>女</td>
<td>广州</td>
<td>博士</td>
</tr>
</tbody>
</table>
<p>👉 模型无法直接理解“北京”“硕士”，
必须把这些文字转为数值，且要<strong>避免引入人为的大小关系</strong>。</p>
<hr />
<h5 id="_19">🧮 常见编码方法<a class="headerlink" href="#_19" title="Permanent link">&para;</a></h5>
<h6 id="1-label-encoding">1️⃣ 标签编码（Label Encoding）<a class="headerlink" href="#1-label-encoding" title="Permanent link">&para;</a></h6>
<p>将每个类别映射为一个整数标签。</p>
<table>
<thead>
<tr>
<th>城市</th>
<th>编码</th>
</tr>
</thead>
<tbody>
<tr>
<td>北京</td>
<td>0</td>
</tr>
<tr>
<td>上海</td>
<td>1</td>
</tr>
<tr>
<td>广州</td>
<td>2</td>
</tr>
</tbody>
</table>
<p><strong>实现：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;城市&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;北京&#39;</span><span class="p">,</span> <span class="s1">&#39;上海&#39;</span><span class="p">,</span> <span class="s1">&#39;广州&#39;</span><span class="p">,</span> <span class="s1">&#39;北京&#39;</span><span class="p">]})</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;城市_编码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;城市&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>
<p>输出：</p>
<table>
<thead>
<tr>
<th>城市</th>
<th>城市_编码</th>
</tr>
</thead>
<tbody>
<tr>
<td>北京</td>
<td>0</td>
</tr>
<tr>
<td>上海</td>
<td>2</td>
</tr>
<tr>
<td>广州</td>
<td>1</td>
</tr>
<tr>
<td>北京</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>✅ 优点：</p>
<ul>
<li>简单高效</li>
<li>不增加维度</li>
</ul>
<p>⚠️ 缺点：</p>
<ul>
<li><strong>引入了“大小”关系</strong>（0 &lt; 1 &lt; 2），
  对线性模型或距离模型（如 KNN、SVM）会造成误导。</li>
</ul>
<p><strong>适用场景：</strong></p>
<ul>
<li><strong>树模型（如决策树、随机森林、XGBoost）</strong>
  树模型只关注是否相等，不受数值大小影响。</li>
</ul>
<hr />
<h6 id="2-one-hot-encoding">2️⃣ 独热编码（One-Hot Encoding）<a class="headerlink" href="#2-one-hot-encoding" title="Permanent link">&para;</a></h6>
<p>将每个类别转换为一个“0/1 向量”，
每个位置代表一个类别是否存在。</p>
<table>
<thead>
<tr>
<th>城市</th>
<th>北京</th>
<th>上海</th>
<th>广州</th>
</tr>
</thead>
<tbody>
<tr>
<td>北京</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>上海</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>广州</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><strong>实现：</strong></p>
<h6 id="1pandas-get_dummies">✅ 方法 1：Pandas 自带 <code>get_dummies()</code><a class="headerlink" href="#1pandas-get_dummies" title="Permanent link">&para;</a></h6>
<div class="highlight"><pre><span></span><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;城市&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;北京&#39;</span><span class="p">,</span> <span class="s1">&#39;上海&#39;</span><span class="p">,</span> <span class="s1">&#39;广州&#39;</span><span class="p">,</span> <span class="s1">&#39;北京&#39;</span><span class="p">]})</span>
<span class="n">df_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;城市&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_encoded</span><span class="p">)</span>
</code></pre></div>
<p>输出：</p>
<table>
<thead>
<tr>
<th>城市_北京</th>
<th>城市_上海</th>
<th>城市_广州</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<h6 id="2scikit-learn-onehotencoder">✅ 方法 2：Scikit-Learn <code>OneHotEncoder</code><a class="headerlink" href="#2scikit-learn-onehotencoder" title="Permanent link">&para;</a></h6>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;城市&#39;</span><span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">encoder</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">([</span><span class="s1">&#39;城市&#39;</span><span class="p">])))</span>
</code></pre></div>
<hr />
<p>✅ <strong>优点：</strong></p>
<ul>
<li>不引入类别顺序</li>
<li>适用于几乎所有机器学习算法</li>
</ul>
<p>⚠️ <strong>缺点：</strong></p>
<ul>
<li>维度爆炸（类别多时会产生大量特征）</li>
<li>稀疏矩阵占用内存</li>
</ul>
<p><strong>适用场景：</strong></p>
<ul>
<li>线性模型（如 Logistic Regression）</li>
<li>神经网络</li>
<li>KNN、SVM、KMeans</li>
</ul>
<hr />
<h6 id="3-binary-encoding">3️⃣ 二值编码（Binary Encoding）<a class="headerlink" href="#3-binary-encoding" title="Permanent link">&para;</a></h6>
<p>每个类别先映射为整数，再转为二进制位。</p>
<p>例如有 6 个类别：</p>
<table>
<thead>
<tr>
<th>类别</th>
<th>整数</th>
<th>二进制</th>
<th>分列</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1</td>
<td>001</td>
<td>0,0,1</td>
</tr>
<tr>
<td>B</td>
<td>2</td>
<td>010</td>
<td>0,1,0</td>
</tr>
<tr>
<td>C</td>
<td>3</td>
<td>011</td>
<td>0,1,1</td>
</tr>
</tbody>
</table>
<p>✅ 优点：</p>
<ul>
<li>压缩维度（比独热编码小）</li>
<li>不引入序关系
  ⚠️ 缺点：</li>
<li>不直观</li>
</ul>
<p>实现依赖库：<code>category_encoders</code></p>
<div class="highlight"><pre><span></span><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">category_encoders</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">category_encoders</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ce</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">ce</span><span class="o">.</span><span class="n">BinaryEncoder</span><span class="p">(</span><span class="n">cols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;城市&#39;</span><span class="p">])</span>
<span class="n">df_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_encoded</span><span class="p">)</span>
</code></pre></div>
<hr />
<h6 id="4-frequency-encoding">4️⃣ 频数编码（Frequency Encoding）<a class="headerlink" href="#4-frequency-encoding" title="Permanent link">&para;</a></h6>
<p>用每个类别出现的<strong>频率或次数</strong>替换类别值。</p>
<table>
<thead>
<tr>
<th>城市</th>
<th>频数编码</th>
</tr>
</thead>
<tbody>
<tr>
<td>北京</td>
<td>2</td>
</tr>
<tr>
<td>上海</td>
<td>1</td>
</tr>
<tr>
<td>广州</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><strong>实现：</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">freq</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;城市&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;城市_频数编码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;城市&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span>
</code></pre></div>
<p>✅ 优点：</p>
<ul>
<li>简单，不增加维度</li>
<li>可捕捉类别分布信息</li>
</ul>
<p>⚠️ 缺点：</p>
<ul>
<li>仍可能隐含“大小”关系</li>
<li>不适合距离度量模型</li>
</ul>
<hr />
<h6 id="5-target-encoding">5️⃣ 目标编码（Target Encoding）<a class="headerlink" href="#5-target-encoding" title="Permanent link">&para;</a></h6>
<p>用每个类别对应目标变量 ( y ) 的<strong>平均值</strong>编码。
常用于分类问题（尤其是高基数类别）。</p>
<table>
<thead>
<tr>
<th>城市</th>
<th>平均购买率</th>
</tr>
</thead>
<tbody>
<tr>
<td>北京</td>
<td>0.8</td>
</tr>
<tr>
<td>上海</td>
<td>0.3</td>
</tr>
<tr>
<td>广州</td>
<td>0.6</td>
</tr>
</tbody>
</table>
<p><strong>实现：</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">target_mean</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;城市&#39;</span><span class="p">)[</span><span class="s1">&#39;是否购买&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;城市_目标编码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;城市&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">target_mean</span><span class="p">)</span>
</code></pre></div>
<p>✅ 优点：</p>
<ul>
<li>对高基数类别有很强表现力
  ⚠️ 缺点：</li>
<li>容易过拟合（尤其在样本少时）
  → 应在交叉验证中谨慎使用。</li>
</ul>
<hr />
<table>
<thead>
<tr>
<th>编码方式</th>
<th>是否保序</th>
<th>是否扩维</th>
<th>是否抗高基数</th>
<th>典型模型</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>标签编码</strong></td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>树模型</td>
<td>简单快速</td>
</tr>
<tr>
<td><strong>独热编码</strong></td>
<td>❌</td>
<td>✅</td>
<td>❌</td>
<td>线性、NN</td>
<td>无序类别推荐</td>
</tr>
<tr>
<td><strong>二值编码</strong></td>
<td>❌</td>
<td>✅（少）</td>
<td>✅</td>
<td>通用</td>
<td>平衡维度与无序性</td>
</tr>
<tr>
<td><strong>频数编码</strong></td>
<td>❌</td>
<td>❌</td>
<td>✅</td>
<td>树模型</td>
<td>捕捉全局统计</td>
</tr>
<tr>
<td><strong>目标编码</strong></td>
<td>❌</td>
<td>❌</td>
<td>✅</td>
<td>高基数分类</td>
<td>防止过拟合需正则</td>
</tr>
</tbody>
</table>
<h4 id="_20">数据类型转换：如将文本、时间等转为数值型<a class="headerlink" href="#_20" title="Permanent link">&para;</a></h4>
<p>机器学习算法大多数只能处理数值（float、int）类型数据，例如：</p>
<ul>
<li>回归模型、SVM、KNN、神经网络等；</li>
<li>决策树类模型（如 RandomForest）虽可处理部分类别数据，但通常仍建议数值化。</li>
</ul>
<p>👉 <strong>目标：</strong>
将文本、日期、布尔、类别等字段，转换为模型可理解的数值特征。</p>
<hr />
<h5 id="_21">📚 常见的数据类型及转换方式<a class="headerlink" href="#_21" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>转换方式</th>
<th>举例说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>文本型（string/object）</strong></td>
<td>编码（如LabelEncoder、OneHotEncoder）</td>
<td>“城市”列：<code>["北京","上海","广州"] → [0,1,2]</code>（LabelEncoder）或 <code>[1,0,0],[0,1,0],[0,0,1]</code>（OneHot）</td>
</tr>
<tr>
<td><strong>类别型（categorical）</strong></td>
<td>编码（与文本型相同）</td>
<td>“性别”列：<code>["男","女"] → [0,1]</code></td>
</tr>
<tr>
<td><strong>时间型（datetime）</strong></td>
<td>提取时间特征或转换为时间戳</td>
<td>“2024-05-10” → 提取出<code>年=2024, 月=5, 日=10, 星期=5</code>，或转换为 <code>timestamp=1715299200</code></td>
</tr>
<tr>
<td><strong>布尔型（bool）</strong></td>
<td>转换为0和1</td>
<td><code>True → 1, False → 0</code></td>
</tr>
<tr>
<td><strong>混合型（数值+文本）</strong></td>
<td>先清洗再转换</td>
<td><code>"12kg" → 12</code> 或 <code>"否"→0</code>、<code>"是"→1</code></td>
</tr>
<tr>
<td><strong>文本描述型（自然语言）</strong></td>
<td>文本向量化（TF-IDF、Word2Vec、BERT Embedding）</td>
<td><code>"我喜欢机器学习"</code> → 向量 <code>[0.25, 0.11, 0.83, ...]</code></td>
</tr>
</tbody>
</table>
<h5 id="_22">🧠 时间类型转换详解<a class="headerlink" href="#_22" title="Permanent link">&para;</a></h5>
<p>时间数据是最常见的“非数值型”数据之一。
通常有三种处理方式：</p>
<h6 id="1_4">✅ 方法1：提取时间特征<a class="headerlink" href="#1_4" title="Permanent link">&para;</a></h6>
<p>适合有周期规律的数据（如销量、温度、交通流量等）</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;date&quot;</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">([</span><span class="s2">&quot;2023-05-01&quot;</span><span class="p">,</span> <span class="s2">&quot;2023-06-15&quot;</span><span class="p">,</span> <span class="s2">&quot;2023-07-20&quot;</span><span class="p">])</span>
<span class="p">})</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;year&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;date&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;month&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;date&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">month</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;day&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;date&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">day</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;weekday&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;date&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">weekday</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>
<p>输出：</p>
<div class="highlight"><pre><span></span><code>        date  year  month  day  weekday
0 2023-05-01  2023      5    1        0
1 2023-06-15  2023      6   15        3
2 2023-07-20  2023      7   20        3
</code></pre></div>
<hr />
<h6 id="2_3">✅ 方法2：转换为时间戳（数值型）<a class="headerlink" href="#2_3" title="Permanent link">&para;</a></h6>
<div class="highlight"><pre><span></span><code><span class="n">df</span><span class="p">[</span><span class="s2">&quot;timestamp&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;date&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span> <span class="o">//</span> <span class="mf">1e9</span>  <span class="c1"># 秒级时间戳</span>
</code></pre></div>
<p>例如：</p>
<div class="highlight"><pre><span></span><code>2023-05-01 → 1682899200
</code></pre></div>
<hr />
<h6 id="3sincos">✅ 方法3：周期性特征编码（sin、cos）<a class="headerlink" href="#3sincos" title="Permanent link">&para;</a></h6>
<p>周期性时间特征（如“月份”、“小时”）可用正余弦函数编码，以保留连续性。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">df</span><span class="p">[</span><span class="s2">&quot;month_sin&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;month&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;month_cos&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;month&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="mi">12</span><span class="p">)</span>
</code></pre></div>
<p>👉 优点：模型可以学习到“12月和1月”相邻，而不是距离为11。</p>
<hr />
<h5 id="_23">🧮 文本类型转换详解<a class="headerlink" href="#_23" title="Permanent link">&para;</a></h5>
<h6 id="1label-encoding">✅ 方法1：Label Encoding（标签编码）<a class="headerlink" href="#1label-encoding" title="Permanent link">&para;</a></h6>
<p>适合有<strong>大小或顺序关系</strong>的类别（如：低、中、高）</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;label_encoded&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s2">&quot;北京&quot;</span><span class="p">,</span><span class="s2">&quot;上海&quot;</span><span class="p">,</span><span class="s2">&quot;广州&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>
<p>输出：</p>
<div class="highlight"><pre><span></span><code>原始值：[&quot;北京&quot;,&quot;上海&quot;,&quot;广州&quot;]
编码后：[0, 2, 1]
</code></pre></div>
<hr />
<h6 id="2one-hot-encoding">✅ 方法2：One-Hot Encoding（独热编码）<a class="headerlink" href="#2one-hot-encoding" title="Permanent link">&para;</a></h6>
<p>适合无序类别</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;北京&quot;</span><span class="p">,</span> <span class="s2">&quot;上海&quot;</span><span class="p">,</span> <span class="s2">&quot;广州&quot;</span><span class="p">]})</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;city&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>
<p>输出：</p>
<div class="highlight"><pre><span></span><code>   city_北京  city_上海  city_广州
0        1        0        0
1        0        1        0
2        0        0        1
</code></pre></div>
<hr />
<h6 id="3tf-idf-word2vec-bert">✅ 方法3：文本向量化（TF-IDF / Word2Vec / BERT）<a class="headerlink" href="#3tf-idf-word2vec-bert" title="Permanent link">&para;</a></h6>
<p>用于处理自然语言文本。</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;我喜欢机器学习&quot;</span><span class="p">,</span> <span class="s2">&quot;机器学习很好玩&quot;</span><span class="p">]</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</code></pre></div>
<hr />
<table>
<thead>
<tr>
<th>步骤</th>
<th>转换对象</th>
<th>工具/方法</th>
<th>转换后类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>布尔型</td>
<td><code>.astype(int)</code></td>
<td>int</td>
</tr>
<tr>
<td>2</td>
<td>文本/类别型</td>
<td><code>LabelEncoder</code> / <code>OneHotEncoder</code> / <code>get_dummies()</code></td>
<td>数值</td>
</tr>
<tr>
<td>3</td>
<td>时间型</td>
<td><code>pd.to_datetime()</code> + <code>.dt</code>提取</td>
<td>数值</td>
</tr>
<tr>
<td>4</td>
<td>混合字符串</td>
<td><code>str.replace()</code> + <code>.astype()</code></td>
<td>数值</td>
</tr>
<tr>
<td>5</td>
<td>自然语言文本</td>
<td>TF-IDF / Word2Vec / Embedding</td>
<td>向量</td>
</tr>
</tbody>
</table>
<h3 id="23">2.3 数据分割<a class="headerlink" href="#23" title="Permanent link">&para;</a></h3>
<h4 id="_24">划分训练集、验证集和测试集，合理分配数据，避免过拟合<a class="headerlink" href="#_24" title="Permanent link">&para;</a></h4>
<p>在机器学习中，我们希望模型<strong>不仅在训练数据上表现好</strong>，还要<strong>在未知数据上表现优异</strong>。
如果只用同一份数据来训练和评估模型，就会导致模型“背题”——即 <strong>过拟合（Overfitting）</strong>。</p>
<p>👉 <strong>解决方法：</strong>
将原始数据划分为不同的数据集，用于不同目的：</p>
<table>
<thead>
<tr>
<th>数据集</th>
<th>用途</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>训练集（Training Set）</strong></td>
<td>训练模型</td>
<td>学习数据特征和规律</td>
</tr>
<tr>
<td><strong>验证集（Validation Set）</strong></td>
<td>调参和模型选择</td>
<td>判断模型在未见数据上的表现，防止过拟合</td>
</tr>
<tr>
<td><strong>测试集（Test Set）</strong></td>
<td>最终评估</td>
<td>模拟真实环境下的模型表现</td>
</tr>
</tbody>
</table>
<hr />
<h5 id="_25">📚 常见划分比例<a class="headerlink" href="#_25" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>数据集</th>
<th>常见比例</th>
</tr>
</thead>
<tbody>
<tr>
<td>训练集</td>
<td>60%～80%</td>
</tr>
<tr>
<td>验证集</td>
<td>10%～20%</td>
</tr>
<tr>
<td>测试集</td>
<td>10%～20%</td>
</tr>
</tbody>
</table>
<p>例如：</p>
<div class="highlight"><pre><span></span><code>80% 训练集 + 10% 验证集 + 10% 测试集
</code></pre></div>
<p>✅ 经验法则：</p>
<ul>
<li>数据量<strong>较大</strong> → 可以 70% / 15% / 15%</li>
<li>数据量<strong>较小</strong> → 采用 <strong>交叉验证（K-Fold Cross Validation）</strong> 来替代验证集</li>
</ul>
<hr />
<h5 id="python">🧠 数据划分的基本方法（Python 实例）<a class="headerlink" href="#python" title="Permanent link">&para;</a></h5>
<h6 id="1-sklearn-train_test_split">✅ 方法1：使用 sklearn 的 <code>train_test_split</code><a class="headerlink" href="#1-sklearn-train_test_split" title="Permanent link">&para;</a></h6>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># 构造示例数据</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;feature1&quot;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span>
    <span class="s2">&quot;feature2&quot;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">21</span><span class="p">),</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># 先划分训练集 + 临时集</span>
<span class="n">train_set</span><span class="p">,</span> <span class="n">temp_set</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 再划分验证集和测试集</span>
<span class="n">val_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">temp_set</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;训练集: </span><span class="si">{</span><span class="n">train_set</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;验证集: </span><span class="si">{</span><span class="n">val_set</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;测试集: </span><span class="si">{</span><span class="n">test_set</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>输出：</p>
<div class="highlight"><pre><span></span><code>训练集: (7, 3)
验证集: (1, 3)
测试集: (2, 3)
</code></pre></div>
<hr />
<h6 id="2stratified-split">✅ 方法2：分层抽样（Stratified Split）<a class="headerlink" href="#2stratified-split" title="Permanent link">&para;</a></h6>
<p>用于<strong>分类任务</strong>，保证每个数据集中类别比例一致。</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s2">&quot;feature1&quot;</span><span class="p">,</span> <span class="s2">&quot;feature2&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_temp</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_temp</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_temp</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
</code></pre></div>
<p>👉 优点：
避免划分后某一类样本过少或缺失。</p>
<hr />
<h5 id="cross-validation">🧪 交叉验证（Cross Validation）<a class="headerlink" href="#cross-validation" title="Permanent link">&para;</a></h5>
<p>当数据量较小时，<strong>单次划分不够稳定</strong>，这时用交叉验证。
常用方式是 <strong>K 折交叉验证（K-Fold CV）</strong>：</p>
<h6 id="_26">📘 原理：<a class="headerlink" href="#_26" title="Permanent link">&para;</a></h6>
<ul>
<li>将数据分成 K 份；</li>
<li>每次取 1 份作为验证集，其余 K−1 份作为训练集；</li>
<li>重复 K 次；</li>
<li>最后对 K 次验证结果取平均。</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kf</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;每折得分:&quot;</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;平均准确率:&quot;</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</code></pre></div>
<p>输出：</p>
<div class="highlight"><pre><span></span><code>每折得分: [0.96 0.93 0.96 0.90 0.96]
平均准确率: 0.942
</code></pre></div>
<p>✅ 优点：</p>
<ul>
<li>更稳定的模型评估；</li>
<li>数据充分利用；</li>
<li>特别适合样本较少的情况。</li>
</ul>
<hr />
<table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决办法</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>数据泄漏（Data Leakage）</strong></td>
<td>测试数据在训练阶段被“偷看”</td>
<td>划分前要先分割数据，再做标准化、特征工程</td>
</tr>
<tr>
<td><strong>时间序列数据不能随机划分</strong></td>
<td>时间有顺序</td>
<td>应使用时间顺序划分，如前 80% 训练，后 20% 测试</td>
</tr>
<tr>
<td><strong>类别分布不均衡</strong></td>
<td>某类样本比例太低</td>
<td>使用分层抽样（stratify）保持比例</td>
</tr>
<tr>
<td><strong>随机性问题</strong></td>
<td>每次划分结果不同</td>
<td>设置 <code>random_state</code> 保证结果可复现</td>
</tr>
</tbody>
</table>
<h5 id="_27">完整示例<a class="headerlink" href="#_27" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># 1️⃣ 加载数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data.csv&quot;</span><span class="p">)</span>

<span class="c1"># 2️⃣ 划分特征与标签</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span>

<span class="c1"># 3️⃣ 划分训练、验证、测试集</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_temp</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">y_temp</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_temp</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 4️⃣ 仅用训练集拟合标准化器，防止数据泄漏</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># 5️⃣ 对全部集做变换</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="24">2.4 其他处理<a class="headerlink" href="#24" title="Permanent link">&para;</a></h3>
<h4 id="smote">处理不平衡数据：过采样、欠采样、SMOTE等<a class="headerlink" href="#smote" title="Permanent link">&para;</a></h4>
<p>在分类问题中，如果各类别样本数量差异很大，就叫做<strong>类别不平衡</strong>。
例如：</p>
<table>
<thead>
<tr>
<th>类别</th>
<th>样本数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>正样本（1）</td>
<td>100</td>
</tr>
<tr>
<td>负样本（0）</td>
<td>5000</td>
</tr>
</tbody>
</table>
<p>此时：</p>
<ul>
<li>模型若只预测“0”，准确率也能达 98%；</li>
<li>但模型<strong>几乎没学到少数类（1）的特征</strong>。</li>
</ul>
<p>👉 <strong>结果</strong>：高准确率但低召回率，模型“看似聪明，实则无用”。</p>
<hr />
<h5 id="_28">🧠 为什么要处理不平衡数据？<a class="headerlink" href="#_28" title="Permanent link">&para;</a></h5>
<p>若不处理，模型会：</p>
<ul>
<li>倾向于预测多数类；</li>
<li>忽略稀有事件（如欺诈检测、疾病诊断、异常检测）；</li>
<li>性能指标（准确率）失真。</li>
</ul>
<p>因此，我们需要 <strong>重新平衡数据分布</strong>，让模型公平学习每一类。</p>
<hr />
<h5 id="_29">⚙️ 常见的处理方法<a class="headerlink" href="#_29" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>方法类别</th>
<th>代表方法</th>
<th>思想</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>数据层面</strong></td>
<td>欠采样、过采样、SMOTE</td>
<td>改变数据分布</td>
</tr>
<tr>
<td><strong>算法层面</strong></td>
<td>加权模型、代价敏感学习</td>
<td>修改训练权重</td>
</tr>
<tr>
<td><strong>评估层面</strong></td>
<td>使用 F1-score、AUC 等指标</td>
<td>改变评估方式</td>
</tr>
</tbody>
</table>
<h5 id="_30">🧩 数据层面方法详解<a class="headerlink" href="#_30" title="Permanent link">&para;</a></h5>
<h6 id="1-under-sampling">1️⃣ 欠采样（Under-Sampling）<a class="headerlink" href="#1-under-sampling" title="Permanent link">&para;</a></h6>
<p>👉 <strong>思想</strong>：
从多数类中随机删除部分样本，使各类样本数量接近。</p>
<p><strong>✅ 优点：</strong></p>
<ul>
<li>简单直观</li>
<li>降低计算量</li>
</ul>
<p><strong>⚠️ 缺点：</strong></p>
<ul>
<li>丢失多数类信息，可能影响模型表现</li>
</ul>
<p><strong>📘 示例：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">imblearn.under_sampling</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomUnderSampler</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;x1&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">],</span> <span class="s1">&#39;x2&#39;</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">]})</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="n">rus</span> <span class="o">=</span> <span class="n">RandomUnderSampler</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_res</span><span class="p">,</span> <span class="n">y_res</span> <span class="o">=</span> <span class="n">rus</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;采样前:&quot;</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;采样后:&quot;</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_res</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</code></pre></div>
<p>输出：</p>
<div class="highlight"><pre><span></span><code>采样前:
0    4
1    4
采样后:
0    4
1    4
</code></pre></div>
<hr />
<h6 id="2-over-sampling">2️⃣ 过采样（Over-Sampling）<a class="headerlink" href="#2-over-sampling" title="Permanent link">&para;</a></h6>
<p>👉 <strong>思想</strong>：
通过复制或合成新的少数类样本，使其数量与多数类接近。</p>
<p><strong>✅ 优点：</strong></p>
<ul>
<li>不丢失信息</li>
<li>有助于模型学习少数类特征</li>
</ul>
<p><strong>⚠️ 缺点：</strong></p>
<ul>
<li>容易过拟合（尤其是简单复制样本）</li>
</ul>
<p><strong>📘 示例：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">imblearn.over_sampling</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomOverSampler</span>

<span class="n">ros</span> <span class="o">=</span> <span class="n">RandomOverSampler</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_res</span><span class="p">,</span> <span class="n">y_res</span> <span class="o">=</span> <span class="n">ros</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;采样后:&quot;</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_res</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</code></pre></div>
<hr />
<h6 id="3-smotesynthetic-minority-over-sampling-technique">3️⃣ SMOTE（Synthetic Minority Over-sampling Technique）<a class="headerlink" href="#3-smotesynthetic-minority-over-sampling-technique" title="Permanent link">&para;</a></h6>
<p>👉 <strong>思想：</strong>
通过<strong>插值算法</strong>生成新的少数类样本，而不是简单复制。</p>
<ul>
<li>对于少数类样本 <span class="arithmatex">\(x_i\)</span>，在其 K 个近邻中随机选取一个样本 <span class="arithmatex">\(x_j\)</span></li>
<li>按比例生成新样本：
$$
  x_{new} = x_i + \lambda (x_j - x_i), \quad \lambda \in [0,1]
$$</li>
</ul>
<p><strong>✅ 优点：</strong></p>
<ul>
<li>比随机过采样更合理</li>
<li>减少过拟合</li>
</ul>
<p><strong>⚠️ 缺点：</strong></p>
<ul>
<li>可能生成噪声点（边界模糊）</li>
</ul>
<p><strong>📘 示例：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">imblearn.over_sampling</span><span class="w"> </span><span class="kn">import</span> <span class="n">SMOTE</span>

<span class="n">sm</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_res</span><span class="p">,</span> <span class="n">y_res</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;采样后:&quot;</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_res</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</code></pre></div>
<hr />
<h5 id="_31">🧮 算法层面方法<a class="headerlink" href="#_31" title="Permanent link">&para;</a></h5>
<p>有时不改数据，而是在<strong>模型训练时调整权重</strong>。</p>
<h6 id="1-class-weight">✅ 1. 类别权重（Class Weight）<a class="headerlink" href="#1-class-weight" title="Permanent link">&para;</a></h6>
<p>例如在 Logistic Regression、SVM、RandomForest 中：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_res</span><span class="p">,</span> <span class="n">y_res</span><span class="p">)</span>
</code></pre></div>
<p>作用：</p>
<ul>
<li>自动根据类别数量分配权重：</li>
</ul>
<div class="arithmatex">\[
  w_i = \frac{N}{2 \times N_i}
\]</div>
<p>即类别样本越少，权重越大。</p>
<hr />
<h6 id="2-cost-sensitive-learning">✅ 2. 代价敏感学习（Cost-sensitive Learning）<a class="headerlink" href="#2-cost-sensitive-learning" title="Permanent link">&para;</a></h6>
<p>通过设置 <strong>误分类代价矩阵</strong>，让模型“更怕”少数类预测错误。</p>
<p>例如在决策树中：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">5</span><span class="p">})</span>
</code></pre></div>
<hr />
<h5 id="_32">📊 评估层面方法<a class="headerlink" href="#_32" title="Permanent link">&para;</a></h5>
<p>当类别不平衡时，<strong>准确率（Accuracy）</strong> 不能真实反映性能。
推荐使用以下指标：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>含义</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>精确率（Precision）</strong></td>
<td>预测为正的样本中，实际为正的比例</td>
<td>少数类错误代价高时（如诈骗检测）</td>
</tr>
<tr>
<td><strong>召回率（Recall）</strong></td>
<td>实际为正的样本中，被模型找出的比例</td>
<td>少数类漏检代价高时（如癌症检测）</td>
</tr>
<tr>
<td><strong>F1-score</strong></td>
<td>精确率与召回率的调和平均</td>
<td>综合评估</td>
</tr>
<tr>
<td><strong>ROC-AUC</strong></td>
<td>模型区分类别能力</td>
<td>综合性能指标</td>
</tr>
</tbody>
</table>
<h4 id="_33">数据增强：在图像、文本等领域扩充训练样本<a class="headerlink" href="#_33" title="Permanent link">&para;</a></h4>
<p><strong>数据增强（Data Augmentation）</strong>
是指通过对已有数据进行<strong>变换、扰动或生成新样本</strong>，来<strong>扩充训练数据集</strong>，
以提升模型的<strong>泛化能力（Generalization）</strong>、防止<strong>过拟合（Overfitting）</strong>。</p>
<h5 id="_34">🧠 为什么要做数据增强？<a class="headerlink" href="#_34" title="Permanent link">&para;</a></h5>
<p>在实际中，数据往往：</p>
<ul>
<li>样本量少；</li>
<li>分布不均；</li>
<li>噪声大；</li>
<li>难以采集。</li>
</ul>
<p>👉 数据增强可以：</p>
<ul>
<li><strong>增加样本多样性</strong>；</li>
<li><strong>让模型学习“本质特征”</strong>；</li>
<li><strong>减少过拟合、提升鲁棒性</strong>；</li>
<li><strong>节约标注成本</strong>。</li>
</ul>
<h5 id="_35">📊 数据增强的类型总览<a class="headerlink" href="#_35" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>类型</th>
<th>适用领域</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像增强</td>
<td>CV（计算机视觉）</td>
<td>翻转、旋转、裁剪、亮度调整、噪声添加</td>
</tr>
<tr>
<td>文本增强</td>
<td>NLP（自然语言处理）</td>
<td>同义词替换、随机插入、回译、混合生成</td>
</tr>
<tr>
<td>音频增强</td>
<td>语音识别、音频分类</td>
<td>加噪、变速、变调、时间裁剪</td>
</tr>
<tr>
<td>数值特征增强</td>
<td>结构化数据</td>
<td>随机扰动、SMOTE、噪声注入</td>
</tr>
</tbody>
</table>
<h5 id="_36">🖼️ 图像数据增强详解<a class="headerlink" href="#_36" title="Permanent link">&para;</a></h5>
<p>图像增强（Image Augmentation）是最常用的方式之一。</p>
<h6 id="1_5">✅ 1️⃣ 基本变换<a class="headerlink" href="#1_5" title="Permanent link">&para;</a></h6>
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>翻转（Flip）</td>
<td>水平或垂直翻转</td>
</tr>
<tr>
<td>旋转（Rotation）</td>
<td>随机角度旋转</td>
</tr>
<tr>
<td>平移（Shift）</td>
<td>图像在平面内移动</td>
</tr>
<tr>
<td>缩放（Zoom）</td>
<td>改变大小</td>
</tr>
<tr>
<td>裁剪（Crop）</td>
<td>随机或中心裁剪</td>
</tr>
<tr>
<td>颜色扰动</td>
<td>改变亮度、对比度、饱和度</td>
</tr>
<tr>
<td>加噪声</td>
<td>模拟噪声环境</td>
</tr>
</tbody>
</table>
<p><strong>📘 示例（使用 PyTorch）</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># 定义数据增强管道</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mi">15</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ColorJitter</span><span class="p">(</span><span class="n">brightness</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">contrast</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">])</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;cat.jpg&quot;</span><span class="p">)</span>
<span class="n">aug_img</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</code></pre></div>
<p>✅ 这些操作能在训练时动态应用，每个 epoch 生成不同版本的图片。</p>
<hr />
<h6 id="2_4">✅ 2️⃣ 高级图像增强<a class="headerlink" href="#2_4" title="Permanent link">&para;</a></h6>
<table>
<thead>
<tr>
<th>技术</th>
<th>原理</th>
<th>应用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Cutout</strong></td>
<td>在图片上随机遮盖一部分区域</td>
<td>增强模型抗遮挡性</td>
</tr>
<tr>
<td><strong>Mixup</strong></td>
<td>将两张图线性混合</td>
<td>改善边界泛化能力</td>
</tr>
<tr>
<td><strong>CutMix</strong></td>
<td>将一张图的一部分贴到另一张图</td>
<td>提升鲁棒性</td>
</tr>
<tr>
<td><strong>AutoAugment / RandAugment</strong></td>
<td>自动搜索最佳增强策略</td>
<td>提升模型性能</td>
</tr>
</tbody>
</table>
<hr />
<h5 id="nlp">✍️ 文本数据增强详解（NLP）<a class="headerlink" href="#nlp" title="Permanent link">&para;</a></h5>
<p>文本增强相对更复杂，因为语言结构要保持语义合理。</p>
<h6 id="1_6">✅ 1️⃣ 基本策略<a class="headerlink" href="#1_6" title="Permanent link">&para;</a></h6>
<table>
<thead>
<tr>
<th>方法</th>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>同义词替换</strong></td>
<td>“我很高兴” → “我非常开心”</td>
<td>替换部分词汇</td>
</tr>
<tr>
<td><strong>随机插入</strong></td>
<td>“我去吃饭” → “我马上去吃饭”</td>
<td>随机插入相近词</td>
</tr>
<tr>
<td><strong>随机删除</strong></td>
<td>“我今天去上学” → “我去上学”</td>
<td>删除不影响语义的词</td>
</tr>
<tr>
<td><strong>随机交换</strong></td>
<td>“他去了北京” → “北京他去了”</td>
<td>打乱局部顺序</td>
</tr>
<tr>
<td><strong>回译（Back Translation）</strong></td>
<td>中文 → 英文 → 中文</td>
<td>利用翻译模型生成新句式</td>
</tr>
<tr>
<td><strong>EDA（Easy Data Augmentation）</strong></td>
<td>综合以上操作</td>
<td>简单实用的增强方法</td>
</tr>
</tbody>
</table>
<p><strong>📘 示例（使用 nlpaug）</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">nlpaug.augmenter.word</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">naw</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;机器学习可以让计算机自己学习规律。&quot;</span>

<span class="c1"># 同义词替换增强</span>
<span class="n">aug</span> <span class="o">=</span> <span class="n">naw</span><span class="o">.</span><span class="n">SynonymAug</span><span class="p">(</span><span class="n">aug_src</span><span class="o">=</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
<span class="n">aug_text</span> <span class="o">=</span> <span class="n">aug</span><span class="o">.</span><span class="n">augment</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;原句：&quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;增强后：&quot;</span><span class="p">,</span> <span class="n">aug_text</span><span class="p">)</span>
</code></pre></div>
<p>输出：</p>
<div class="highlight"><pre><span></span><code>原句： 机器学习可以让计算机自己学习规律。
增强后： 机器学习能使计算机自己学习规律。
</code></pre></div>
<hr />
<h2 id="_37">三、线性模型<a class="headerlink" href="#_37" title="Permanent link">&para;</a></h2>
<h3 id="31">3.1 线性回归<a class="headerlink" href="#31" title="Permanent link">&para;</a></h3>
<p><strong>线性回归（Linear Regression）</strong> 是机器学习中最基础的监督学习模型之一，用于 <strong>预测一个连续数值</strong>。</p>
<p>其核心思想是：</p>
<blockquote>
<p>寻找一个最优的线性函数，使预测值 <span class="arithmatex">\(\hat{y}\)</span> 与真实值 <span class="arithmatex">\(y\)</span> 之间的误差最小。</p>
</blockquote>
<hr />
<h4 id="1_7">1️⃣ 原理<a class="headerlink" href="#1_7" title="Permanent link">&para;</a></h4>
<h5 id="_38">一元线性回归<a class="headerlink" href="#_38" title="Permanent link">&para;</a></h5>
<p>假设输入特征只有一个 ( x )，输出为 ( y )：</p>
<div class="arithmatex">\[
\hat{y} = w x + b
\]</div>
<p>其中：</p>
<ul>
<li>$ \hat{y} $：预测值</li>
<li>$ w $：权重（斜率）</li>
<li>$b $：偏置（截距）</li>
</ul>
<p>目标是：找到最佳的 <span class="arithmatex">\(w\)</span>、<span class="arithmatex">\(b\)</span>，使得预测结果最接近真实值。</p>
<hr />
<h5 id="_39">多元线性回归<a class="headerlink" href="#_39" title="Permanent link">&para;</a></h5>
<p>当特征有多个 $x_1, x_2, ..., x_n $ 时：</p>
<div class="arithmatex">\[
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
\]</div>
<p>或者用向量形式表示为：</p>
<div class="arithmatex">\[
\hat{y} = \mathbf{w}^\top \mathbf{x} + b
\]</div>
<h5 id="_40">岭回归<a class="headerlink" href="#_40" title="Permanent link">&para;</a></h5>
<p><strong>岭回归</strong>是 <strong>线性回归的一种改进</strong>，主要用于解决 <strong>多重共线性（特征高度相关）</strong> 或 <strong>过拟合</strong> 问题。</p>
<p>核心思想：</p>
<blockquote>
<p>在最小化预测误差的基础上，增加对权重的 <strong>L2 正则化约束</strong>，使模型权重不至于过大。</p>
</blockquote>
<p>线性回归的损失函数（均方误差）为：</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2
\]</div>
<p>岭回归在此基础上增加 <strong>L2 正则化项</strong>：</p>
<div class="arithmatex">\[
J_{\text{ridge}}(\mathbf{w}) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n} w_j^2
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(\lambda \ge 0\)</span> 是 <strong>正则化系数</strong></li>
<li><span class="arithmatex">\(w_j\)</span> 是模型的权重</li>
<li>当 <span class="arithmatex">\(\lambda = 0\)</span> 时，退化为普通线性回归</li>
<li>当 <span class="arithmatex">\(\lambda\)</span> 增大时，模型权重被压缩，减少过拟合</li>
</ul>
<h6 id="_41">解析解（正规方程）<a class="headerlink" href="#_41" title="Permanent link">&para;</a></h6>
<p>加入 L2 正则化后的解析解为：</p>
<div class="arithmatex">\[
\mathbf{w} = (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{y}
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(\mathbf{I}\)</span> 是 <span class="arithmatex">\(n \times n\)</span> 单位矩阵</li>
<li>正则化项保证矩阵 <strong>可逆</strong>，解决多重共线性问题</li>
</ul>
<blockquote>
<p>注意：偏置 <span class="arithmatex">\(b\)</span> 通常不正则化。</p>
</blockquote>
<h6 id="_42">梯度下降法<a class="headerlink" href="#_42" title="Permanent link">&para;</a></h6>
<p>岭回归梯度更新公式：</p>
<div class="arithmatex">\[
\frac{\partial J_{\text{ridge}}}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) x_j^{(i)} + \lambda w_j
\]</div>
<p>更新规则：</p>
<div class="arithmatex">\[
w_j := w_j - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) x_j^{(i)} + \lambda w_j \right)
\]</div>
<p>其中 <span class="arithmatex">\(\alpha\)</span> 为学习率。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>

<span class="c1"># 构造示例数据</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># 划分训练集和测试集</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 训练岭回归模型</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># alpha = λ</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 模型评估</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;系数 w:&quot;</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;截距 b:&quot;</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;R²:&quot;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h5 id="lasso">Lasso回归<a class="headerlink" href="#lasso" title="Permanent link">&para;</a></h5>
<p><strong>Lasso 回归（Least Absolute Shrinkage and Selection Operator）</strong> 是在线性回归的基础上加入 <strong>L1 正则化项</strong> 的模型。
它不仅能防止过拟合，还能实现 <strong>特征选择（Feature Selection）</strong>，因为它会使部分特征系数变为 <strong>0</strong>。</p>
<h6 id="_43">⚙️ 模型原理<a class="headerlink" href="#_43" title="Permanent link">&para;</a></h6>
<p>在普通线性回归中，我们希望最小化残差平方和：</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \frac{1}{2m} \sum_{i=1}^{m} (y_i - \hat{y}*i)^2 = \frac{1}{2m} \sum*{i=1}^{m} (y_i - \mathbf{w}^T \mathbf{x}_i)^2
\]</div>
<p>而在 <strong>Lasso 回归</strong> 中，我们在损失函数中加入一个 <strong>L1 正则化项</strong>：</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \frac{1}{2m} \sum_{i=1}^{m} (y_i - \mathbf{w}^T \mathbf{x}*i)^2 + \lambda \sum*{j=1}^{n} |w_j|
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(\lambda\)</span> 为正则化强度（超参数），控制模型复杂度；</li>
<li><span class="arithmatex">\(|w_j|\)</span> 是权重的绝对值；</li>
<li>L1 正则化鼓励权重稀疏（部分权重为 0）。</li>
</ul>
<h6 id="_44">🧮 数学推导<a class="headerlink" href="#_44" title="Permanent link">&para;</a></h6>
<p><strong>1. 损失函数</strong></p>
<p>我们最小化如下目标：
$$
\min_{\mathbf{w}} \frac{1}{2m} |\mathbf{y} - X\mathbf{w}|_2^2 + \lambda |\mathbf{w}|_1
$$</p>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(|\mathbf{w}|_1 = \sum_j |w_j|\)</span></li>
<li><span class="arithmatex">\(|\cdot|_2^2\)</span> 表示平方和。</li>
</ul>
<hr />
<p><strong>2. L1 的不可导性</strong></p>
<p>由于 <span class="arithmatex">\(|w_j|\)</span> 在 <span class="arithmatex">\(w_j=0\)</span> 处不可导，梯度下降不能直接使用。
因此，Lasso 回归的求解通常采用 <strong>坐标下降法（Coordinate Descent）</strong> 或 <strong>次梯度法（Subgradient Method）</strong>。</p>
<hr />
<p><strong>3. 坐标下降法（Coordinate Descent）</strong></p>
<p>在每个特征维度上单独优化：</p>
<div class="arithmatex">\[
w_j \leftarrow S\left( \frac{1}{m} \sum_{i=1}^{m} x_{ij}(y_i - \hat{y}_{i, -j}), \frac{\lambda}{m} \right)
\]</div>
<p>其中 <span class="arithmatex">\(S\)</span> 是 <strong>软阈值函数（Soft Thresholding Function）</strong>：</p>
<div class="arithmatex">\[
S(z, \gamma) =
\begin{cases}
z - \gamma, &amp; \text{if } z &gt; \gamma \
0, &amp; \text{if } |z| \le \gamma \
z + \gamma, &amp; \text{if } z &lt; -\gamma
\end{cases}
\]</div>
<p>该函数能自动把小于阈值的权重压缩为 0，从而实现 <strong>特征选择</strong>。</p>
<hr />
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lasso</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># 加载数据</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 划分数据</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 创建并训练 Lasso 回归模型</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 评估</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;R²:&quot;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;系数:&quot;</span><span class="p">,</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</code></pre></div>
<h6 id="_45">⚠️ 注意事项<a class="headerlink" href="#_45" title="Permanent link">&para;</a></h6>
<ol>
<li>
<p><strong>特征标准化</strong></p>
<ul>
<li>在使用 Lasso 前必须对特征进行 <strong>标准化</strong>（StandardScaler），否则不同量纲的特征会受到不同惩罚。</li>
</ul>
</li>
<li>
<p><strong>超参数 λ 的选择</strong></p>
<ul>
<li><span class="arithmatex">\(\lambda\)</span> 过大 → 系数全部趋近于 0；</li>
<li><span class="arithmatex">\(\lambda\)</span> 过小 → 退化为普通线性回归；</li>
<li>可通过交叉验证（如 <code>LassoCV</code>）自动选择最佳参数。</li>
</ul>
</li>
<li>
<p><strong>多重共线性问题</strong></p>
<ul>
<li>Lasso 能自动“剔除”冗余特征，因此在多重共线性问题中表现较好。</li>
</ul>
</li>
</ol>
<hr />
<h5 id="_46">弹性网络回归<a class="headerlink" href="#_46" title="Permanent link">&para;</a></h5>
<p><strong>弹性网络（Elastic Net）</strong> 是一种在 <strong>Lasso</strong> 的基础上加入 <strong>L2 正则化</strong> 的线性模型。
它在保证特征选择能力的同时，缓解了 Lasso 在特征高度相关时的不稳定性问题。</p>
<h6 id="_47">⚙️ 模型原理<a class="headerlink" href="#_47" title="Permanent link">&para;</a></h6>
<p>普通线性回归的目标函数为：</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \frac{1}{2m} \sum_{i=1}^{m} (y_i - \mathbf{w}^T \mathbf{x}_i)^2
\]</div>
<p>而弹性网络在此基础上加入了 <strong>L1 + L2 正则项</strong>：</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \frac{1}{2m} \sum_{i=1}^{m} (y_i - \mathbf{w}^T \mathbf{x}*i)^2 +
\lambda_1 \sum*{j=1}^{n} |w_j| +
\frac{\lambda_2}{2} \sum_{j=1}^{n} w_j^2
\]</div>
<p>为了更方便调节，通常将它改写为带有混合系数的形式：</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \frac{1}{2m} |\mathbf{y} - X\mathbf{w}|_2^2 +
\lambda \left( \alpha |\mathbf{w}|_1 + \frac{1 - \alpha}{2} |\mathbf{w}|_2^2 \right)
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(\lambda\)</span> 控制整体正则化强度；</li>
<li>
<p><span class="arithmatex">\(\alpha \in [0,1]\)</span> 控制 L1 和 L2 的比例：</p>
</li>
<li>
<p><span class="arithmatex">\(\alpha = 1\)</span> → Lasso 回归；</p>
</li>
<li><span class="arithmatex">\(\alpha = 0\)</span> → 岭回归；</li>
<li><span class="arithmatex">\(0 &lt; \alpha &lt; 1\)</span> → 弹性网络。</li>
</ul>
<hr />
<h6 id="_48">🧮 数学推导思路<a class="headerlink" href="#_48" title="Permanent link">&para;</a></h6>
<p>弹性网络的目标是同时最小化：</p>
<ul>
<li>残差平方和；</li>
<li>权重的 <strong>绝对值之和</strong>（稀疏性）；</li>
<li>权重的 <strong>平方和</strong>（稳定性）。</li>
</ul>
<p>由于 L1 项不可导，因此使用 <strong>坐标下降法（Coordinate Descent）</strong> 求解。</p>
<p>其更新公式类似于 Lasso 的软阈值函数：</p>
<div class="arithmatex">\[
w_j \leftarrow \frac{S\left(\frac{1}{m}\sum_{i=1}^{m} x_{ij}(y_i - \hat{y}_{i,-j}), \lambda \alpha\right)}{1 + \lambda (1-\alpha)}
\]</div>
<p>其中 <span class="arithmatex">\(S(z, \gamma)\)</span> 为软阈值函数：</p>
<div class="arithmatex">\[
S(z, \gamma) =
\begin{cases}
z - \gamma, &amp; z &gt; \gamma \
0, &amp; |z| \le \gamma \
z + \gamma, &amp; z &lt; -\gamma
\end{cases}
\]</div>
<p>L2 项的存在使得分母中多了 <span class="arithmatex">\(1 + \lambda(1-\alpha)\)</span>，这会防止权重过度压缩，缓解 Lasso 的不稳定性。</p>
<hr />
<table>
<thead>
<tr>
<th>模型</th>
<th>正则化几何形状</th>
<th>特征选择能力</th>
<th>稳定性</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ridge</td>
<td>圆形（L2）</td>
<td>无</td>
<td>强</td>
</tr>
<tr>
<td>Lasso</td>
<td>菱形（L1）</td>
<td>强</td>
<td>弱（相关特征不稳定）</td>
</tr>
<tr>
<td>ElasticNet</td>
<td>圆角菱形（L1+L2）</td>
<td>中等</td>
<td>中等偏强</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>模型</th>
<th>正则项</th>
<th>稀疏性</th>
<th>多重共线性处理</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性回归</td>
<td>无</td>
<td>无</td>
<td>差</td>
<td>无约束数据</td>
</tr>
<tr>
<td>岭回归</td>
<td>L2</td>
<td>无</td>
<td>好</td>
<td>防止过拟合</td>
</tr>
<tr>
<td>Lasso 回归</td>
<td>L1</td>
<td>强</td>
<td>差</td>
<td>特征选择</td>
</tr>
<tr>
<td>弹性网络</td>
<td>L1 + L2</td>
<td>中等</td>
<td>好</td>
<td>高维稀疏数据</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">ElasticNet</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># 加载数据</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 标准化</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 划分训练集与测试集</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 训练模型</span>
<span class="n">elastic</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># α控制L1与L2比例</span>
<span class="n">elastic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">elastic</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 评估</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;R²:&quot;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;系数:&quot;</span><span class="p">,</span> <span class="n">elastic</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</code></pre></div>
<h4 id="2_5">2️⃣ 损失函数<a class="headerlink" href="#2_5" title="Permanent link">&para;</a></h4>
<p>线性回归常用的损失函数是 <strong>均方误差（MSE）</strong>：</p>
<div class="arithmatex">\[
\text{MSE} = \frac{1}{m}\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2
\]</div>
<p>其他常见的有：</p>
<ul>
<li><strong>MAE</strong>（平均绝对误差）：
$$
  \text{MAE} = \frac{1}{m}\sum | \hat{y} - y |
$$</li>
<li><strong>RMSE</strong>（均方根误差）：
$$
  \text{RMSE} = \sqrt{\text{MSE}}
$$</li>
</ul>
<h4 id="3_2">3️⃣ 数学推导过程<a class="headerlink" href="#3_2" title="Permanent link">&para;</a></h4>
<p>我们来推导出最优参数 <span class="arithmatex">\(\mathbf{w}\)</span> 的解析解（正规方程法）。</p>
<h5 id="1_8">1️⃣ 向量化表示<a class="headerlink" href="#1_8" title="Permanent link">&para;</a></h5>
<p>设：</p>
<ul>
<li><span class="arithmatex">\(\mathbf{X}\)</span> 为特征矩阵（维度：<span class="arithmatex">\(m \times n\)</span>）</li>
<li><span class="arithmatex">\(\mathbf{y}\)</span> 为真实标签（维度：<span class="arithmatex">\(m \times 1\)</span>）</li>
<li><span class="arithmatex">\(\mathbf{w}\)</span> 为权重向量（维度：<span class="arithmatex">\(n \times 1\)</span>）</li>
</ul>
<p>模型可写为：</p>
<div class="arithmatex">\[
\hat{\mathbf{y}} = \mathbf{X} \mathbf{w}
\]</div>
<p>损失函数为：</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \frac{1}{2m} (\mathbf{Xw} - \mathbf{y})^\top (\mathbf{Xw} - \mathbf{y})
\]</div>
<hr />
<h5 id="2_6">2️⃣ 对参数求导<a class="headerlink" href="#2_6" title="Permanent link">&para;</a></h5>
<p>我们对 <span class="arithmatex">\(\mathbf{w}\)</span> 求偏导：</p>
<div class="arithmatex">\[
\frac{\partial J}{\partial \mathbf{w}} = \frac{1}{m} \mathbf{X}^\top (\mathbf{Xw} - \mathbf{y})
\]</div>
<p>令导数为 0，得到最优解：</p>
<div class="arithmatex">\[
\mathbf{X}^\top \mathbf{Xw} = \mathbf{X}^\top \mathbf{y}
\]</div>
<hr />
<h5 id="3_3">3️⃣ 解出参数（正规方程）<a class="headerlink" href="#3_3" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
\]</div>
<p>这是 <strong>线性回归的解析解</strong>，前提是 <span class="arithmatex">\(\mathbf{X}^\top \mathbf{X}\)</span> 可逆。</p>
<hr />
<h5 id="4_4">4️⃣ 梯度下降法（数值解）<a class="headerlink" href="#4_4" title="Permanent link">&para;</a></h5>
<p>若特征较多或矩阵不可逆，可使用梯度下降法迭代求解：</p>
<p>更新规则：</p>
<div class="arithmatex">\[
\begin{aligned}
w_j &amp;:= w_j - \alpha \frac{\partial J}{\partial w_j} \
b &amp;:= b - \alpha \frac{\partial J}{\partial b}
\end{aligned}
\]</div>
<p>其中学习率 <span class="arithmatex">\(\alpha\)</span> 控制每次更新的步长。</p>
<p>导数展开为：</p>
<div class="arithmatex">\[
\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}
\]</div>
<div class="arithmatex">\[
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})
\]</div>
<h4 id="4_5">4️⃣ 评估指标<a class="headerlink" href="#4_5" title="Permanent link">&para;</a></h4>
<p>线性回归常用以下指标评估模型性能：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>公式</th>
<th>含义</th>
<th>理想值</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MSE</strong>（均方误差）</td>
<td><span class="arithmatex">\(\frac{1}{m}\sum (\hat{y} - y)^2\)</span></td>
<td>衡量误差平方的平均值</td>
<td>越小越好</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>RMSE</strong>（均方根误差）</td>
<td><span class="arithmatex">\(\sqrt{\frac{1}{m}\sum (\hat{y} - y)^2}\)</span></td>
<td>与原量纲一致</td>
<td>越小越好</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>MAE</strong>（平均绝对误差）</td>
<td>$ \frac{1}{m}\sum \lvert  \hat{y} - y \rvert$</td>
<td>对异常值更鲁棒</td>
<td>越小越好</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>R²</strong>（决定系数）</td>
<td><span class="arithmatex">\(R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}\)</span></td>
<td>反映拟合程度</td>
<td>越接近 1 越好</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="arithmatex">\[
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\]</div>
<h4 id="5_1">5️⃣ 实现代码<a class="headerlink" href="#5_1" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># 生成示例数据</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 数据标准化</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 划分训练集和测试集</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 创建并训练模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 评估模型</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;均方误差(MSE): </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R²分数: </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;系数: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;截距: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 可视化结果</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;实际值&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;预测值&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;特征&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;目标&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;线性回归结果&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h4 id="6_1">6️⃣ 模型优化（参数调优）<a class="headerlink" href="#6_1" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># 参数网格</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="s1">&#39;l1_ratio&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># 网格搜索</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">ElasticNet</span><span class="p">(),</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;最佳参数: </span><span class="si">{</span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;最佳分数: </span><span class="si">{</span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h4 id="7_1">7️⃣ 注意事项<a class="headerlink" href="#7_1" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>问题</th>
<th>说明</th>
<th>解决方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>多重共线性</strong></td>
<td>特征高度相关导致矩阵不可逆</td>
<td>使用岭回归（L2正则化）</td>
</tr>
<tr>
<td><strong>异常值敏感</strong></td>
<td>极端样本会严重影响模型</td>
<td>去除或鲁棒回归</td>
</tr>
<tr>
<td><strong>线性假设</strong></td>
<td>模型假设输入与输出线性关系</td>
<td>可加入多项式特征</td>
</tr>
<tr>
<td><strong>异方差性</strong></td>
<td>残差方差不一致</td>
<td>对数变换或加权回归</td>
</tr>
<tr>
<td><strong>特征缩放</strong></td>
<td>梯度下降收敛速度慢</td>
<td>标准化或归一化</td>
</tr>
</tbody>
</table>
<h3 id="32">3.2 逻辑回归<a class="headerlink" href="#32" title="Permanent link">&para;</a></h3>
<p>逻辑回归是一种用于<strong>二分类问题</strong>的线性模型。
与线性回归不同的是，它的输出是一个<strong>概率值</strong>（范围 <span class="arithmatex">\([0,1]\)</span>），然后通过阈值（通常为 0.5）将样本分类为正类或负类。</p>
<h4 id="1_9">1️⃣ 原理<a class="headerlink" href="#1_9" title="Permanent link">&para;</a></h4>
<p>假设输入样本为 <span class="arithmatex">\(x = (x_1, x_2, \dots, x_n)\)</span>，模型参数为 <span class="arithmatex">\(\theta = (\theta_0, \theta_1, \dots, \theta_n)\)</span>，逻辑回归假设：</p>
<div class="arithmatex">\[
z = \theta^T x = \theta_0 + \theta_1 x_1 + \dots + \theta_n x_n
\]</div>
<p>然后使用 <strong>Sigmoid 函数</strong> 将 <span class="arithmatex">\(z\)</span> 映射到 <span class="arithmatex">\([0,1]\)</span>：</p>
<div class="arithmatex">\[
h_\theta(x) = \sigma(z) = \frac{1}{1 + e^{-z}}
\]</div>
<p>此时 <span class="arithmatex">\(h_\theta(x)\)</span> 表示样本属于正类（<span class="arithmatex">\(y=1\)</span>）的概率。</p>
<h5 id="_49">分类规则<a class="headerlink" href="#_49" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\hat{y} =
\begin{cases}
1, &amp; \text{if } h_\theta(x) \ge 0.5 \\
0, &amp; \text{if } h_\theta(x) &lt; 0.5
\end{cases}
\]</div>
<h4 id="2_7">2️⃣ 损失函数<a class="headerlink" href="#2_7" title="Permanent link">&para;</a></h4>
<p>逻辑回归使用的损失函数是<strong>对数损失（Log Loss）</strong> 或 <strong>交叉熵损失（Cross Entropy Loss）</strong>。</p>
<h5 id="21_1">2.1 单样本损失函数<a class="headerlink" href="#21_1" title="Permanent link">&para;</a></h5>
<p>对于一个样本 <span class="arithmatex">\((x^{(i)}, y^{(i)})\)</span>：</p>
<div class="arithmatex">\[
L(\theta) = -\left[y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))\right]
\]</div>
<h5 id="22_1">2.2 总损失函数<a class="headerlink" href="#22_1" title="Permanent link">&para;</a></h5>
<p>对所有样本取平均：</p>
<div class="arithmatex">\[
J(\theta) = -\frac{1}{m} \sum_{i=1}^m \left[y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))\right]
\]</div>
<h4 id="3_4">3️⃣ 数学推导过程<a class="headerlink" href="#3_4" title="Permanent link">&para;</a></h4>
<p>目标：通过最小化 <span class="arithmatex">\(J(\theta)\)</span> 求解最优参数 <span class="arithmatex">\(\theta\)</span>。</p>
<h5 id="31-gradient">3.1 梯度（Gradient）<a class="headerlink" href="#31-gradient" title="Permanent link">&para;</a></h5>
<p>对参数 <span class="arithmatex">\(\theta_j\)</span> 求偏导：</p>
<div class="arithmatex">\[
\frac{\partial J(\theta)}{\partial \theta_j}
= \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
\]</div>
<h5 id="32_1">3.2 梯度下降法更新规则<a class="headerlink" href="#32_1" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}
\]</div>
<p>其中 <span class="arithmatex">\(\alpha\)</span> 为学习率（learning rate）。</p>
<h4 id="4_6">4️⃣ 评估指标<a class="headerlink" href="#4_6" title="Permanent link">&para;</a></h4>
<p>常见的二分类评估指标有：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>含义</th>
<th>公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>准确率（Accuracy）</td>
<td>分类正确的比例</td>
<td><span class="arithmatex">\(\displaystyle \text{Acc} = \frac{TP + TN}{TP + TN + FP + FN}\)</span></td>
</tr>
<tr>
<td>精确率（Precision）</td>
<td>预测为正中实际为正的比例</td>
<td><span class="arithmatex">\(\displaystyle \text{P} = \frac{TP}{TP + FP}\)</span></td>
</tr>
<tr>
<td>召回率（Recall）</td>
<td>实际为正中被预测为正的比例</td>
<td><span class="arithmatex">\(\displaystyle \text{R} = \frac{TP}{TP + FN}\)</span></td>
</tr>
<tr>
<td>F1-score</td>
<td>精确率与召回率的调和平均</td>
<td><span class="arithmatex">\(\displaystyle F1 = \frac{2PR}{P + R}\)</span></td>
</tr>
<tr>
<td>AUC（ROC 曲线下的面积）</td>
<td>衡量整体分类能力</td>
<td>数值越接近 1 越好</td>
</tr>
</tbody>
</table>
<h4 id="5_2">5️⃣ 实现代码<a class="headerlink" href="#5_2" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_breast_cancer</span>

<span class="c1"># 1. 加载数据</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># 2. 划分数据集</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 3. 训练逻辑回归模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 4. 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># 5. 评估</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;准确率:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;精确率:&quot;</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;召回率:&quot;</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1 值:&quot;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC 值:&quot;</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_prob</span><span class="p">))</span>
</code></pre></div>
<h4 id="6_2">6️⃣ 模型优化<a class="headerlink" href="#6_2" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优化方向</th>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>特征缩放</strong></td>
<td>标准化 <code>StandardScaler</code></td>
<td>有助于梯度下降更快收敛</td>
</tr>
<tr>
<td><strong>正则化</strong></td>
<td><span class="arithmatex">\(L_1\)</span>（Lasso）或 <span class="arithmatex">\(L_2\)</span>（Ridge）</td>
<td>防止过拟合，sklearn 默认使用 <span class="arithmatex">\(L_2\)</span></td>
</tr>
<tr>
<td><strong>调整超参数</strong></td>
<td><code>C</code>（正则化强度）</td>
<td>越小代表正则化越强</td>
</tr>
<tr>
<td><strong>特征选择</strong></td>
<td>去除共线性特征</td>
<td>提高模型稳定性</td>
</tr>
<tr>
<td><strong>类别不平衡处理</strong></td>
<td><code>class_weight='balanced'</code> 或上采样</td>
<td>解决类别样本分布不均的问题</td>
</tr>
</tbody>
</table>
<h4 id="7_2">7️⃣ 注意事项<a class="headerlink" href="#7_2" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>输入特征需数值化</strong>（One-Hot 编码等方式处理分类特征）</li>
<li><strong>特征缩放很重要</strong>（尤其是使用梯度下降训练时）</li>
<li><strong>不要在强非线性问题上直接使用逻辑回归</strong>（它本质是线性分类器）</li>
<li><strong>检查多重共线性</strong>（可用 VIF 检查）</li>
<li><strong>合理选择正则化项</strong>，防止过拟合或欠拟合。</li>
</ol>
<h4 id="8_1">8️⃣ 优缺点<a class="headerlink" href="#8_1" title="Permanent link">&para;</a></h4>
<p><strong>优点：</strong></p>
<ul>
<li>简单、高效，可解释性强；</li>
<li>输出概率，有良好的理论基础；</li>
<li>可用于在线学习（增量更新）。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>仅能处理线性可分问题；</li>
<li>对异常值敏感；</li>
<li>特征工程影响大。</li>
</ul>
<h3 id="32_2">3.2 面试题<a class="headerlink" href="#32_2" title="Permanent link">&para;</a></h3>
<h4 id="_50">什么是梯度下降？<a class="headerlink" href="#_50" title="Permanent link">&para;</a></h4>
<p>梯度下降是一种<strong>优化算法</strong>，用于最小化一个目标函数（如损失函数）。
它的核心思想是：</p>
<blockquote>
<p><strong>沿着函数梯度的反方向</strong>不断调整参数，直到找到函数的最小值。</p>
</blockquote>
<p>你可以把它想象成一个人站在山顶（损失函数的高点），
每次朝着“最陡的下坡方向”走一步，直到走到山谷底（损失最小点）。</p>
<p><strong>数学定义</strong></p>
<p>假设我们有一个需要最小化的函数：</p>
<div class="arithmatex">\[
J(\theta)
\]</div>
<p>其中 <span class="arithmatex">\(\theta\)</span> 表示参数向量（例如模型权重）。
目标是找到：</p>
<div class="arithmatex">\[
\theta^* = \arg\min_\theta J(\theta)
\]</div>
<hr />
<p><strong>梯度（Gradient）</strong></p>
<p>梯度是函数在某一点的<strong>方向导数向量</strong>，表示函数增长最快的方向。
梯度定义为：</p>
<div class="arithmatex">\[
\nabla_\theta J(\theta) =
\begin{bmatrix}
\frac{\partial J}{\partial \theta_1} \
\frac{\partial J}{\partial \theta_2} \
\vdots \
\frac{\partial J}{\partial \theta_n}
\end{bmatrix}
\]</div>
<p>若我们想让函数 <span class="arithmatex">\(J(\theta)\)</span> 变小，就应当<strong>沿着梯度的反方向</strong>移动。</p>
<hr />
<p><strong>梯度下降的更新公式</strong></p>
<p>每次迭代时，更新参数：</p>
<div class="arithmatex">\[
\theta := \theta - \alpha \nabla_\theta J(\theta)
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(\alpha\)</span>：学习率（learning rate），控制每次更新步长；</li>
<li><span class="arithmatex">\(\nabla_\theta J(\theta)\)</span>：梯度；</li>
<li>负号表示<strong>沿梯度反方向</strong>更新。</li>
</ul>
<hr />
<p><strong>梯度下降的几何直观</strong></p>
<ul>
<li><strong>梯度方向</strong>：函数上升最快的方向；</li>
<li><strong>负梯度方向</strong>：函数下降最快的方向；</li>
<li><strong>学习率 <span class="arithmatex">\(\alpha\)</span> 太大</strong> → 可能跨过谷底、发散；</li>
<li><strong>学习率 <span class="arithmatex">\(\alpha\)</span> 太小</strong> → 收敛速度慢；</li>
<li><strong>恰当的 <span class="arithmatex">\(\alpha\)</span></strong> → 稳定而快速地收敛到最小值。</li>
</ul>
<p><strong>梯度下降的三种主要形式</strong></p>
<table>
<thead>
<tr>
<th>类型</th>
<th>计算方式</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>批量梯度下降（BGD）</strong></td>
<td>使用全部样本计算梯度：  <span class="arithmatex">\(\nabla_\theta J(\theta) = \frac{1}{m} \sum_{i=1}^m \nabla_\theta L^{(i)}(\theta)\)</span></td>
<td>收敛稳定，但计算量大</td>
</tr>
<tr>
<td><strong>随机梯度下降（SGD）</strong></td>
<td>每次使用一个样本更新：  <span class="arithmatex">\(\theta := \theta - \alpha \nabla_\theta L^{(i)}(\theta)\)</span></td>
<td>更新快，但波动大</td>
</tr>
<tr>
<td><strong>小批量梯度下降（Mini-batch GD）</strong></td>
<td>每次使用一小部分样本（如 32、64 个）计算梯度</td>
<td>综合两者优点，是深度学习的标准做法</td>
</tr>
</tbody>
</table>
<p><strong>梯度下降的收敛过程</strong></p>
<p>假设目标函数是：</p>
<div class="arithmatex">\[
J(\theta) = \theta^2
\]</div>
<p>则：</p>
<div class="arithmatex">\[
\frac{dJ}{d\theta} = 2\theta
\]</div>
<p>更新公式为：</p>
<div class="arithmatex">\[
\theta := \theta - \alpha \cdot 2\theta
\]</div>
<p>如果设初始 <span class="arithmatex">\(\theta_0 = 5, \alpha = 0.1\)</span>：</p>
<table>
<thead>
<tr>
<th>迭代次数</th>
<th><span class="arithmatex">\(\theta\)</span></th>
<th><span class="arithmatex">\(J(\theta)\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>5.0</td>
<td>25.0</td>
</tr>
<tr>
<td>1</td>
<td>4.0</td>
<td>16.0</td>
</tr>
<tr>
<td>2</td>
<td>3.2</td>
<td>10.24</td>
</tr>
<tr>
<td>3</td>
<td>2.56</td>
<td>6.55</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
<p>可以看到，<span class="arithmatex">\(\theta\)</span> 一步步接近最优点 <span class="arithmatex">\(0\)</span>，<span class="arithmatex">\(J(\theta)\)</span> 逐渐下降。</p>
<p><strong>梯度下降的优缺点</strong></p>
<p><strong>优点：</strong></p>
<ul>
<li>简单易实现；</li>
<li>通用性强，适用于各种可微函数；</li>
<li>可用于大规模机器学习任务。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>对学习率 <span class="arithmatex">\(\alpha\)</span> 敏感；</li>
<li>可能陷入局部最小值；</li>
<li>收敛速度取决于函数形状（鞍点、平缓区域等）。</li>
</ul>
<p><strong>改进算法（扩展）</strong></p>
<p>在现代深度学习中，梯度下降衍生出许多改进算法，例如：</p>
<table>
<thead>
<tr>
<th>优化算法</th>
<th>关键思想</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Momentum</strong></td>
<td>模拟惯性，加快收敛</td>
</tr>
<tr>
<td><strong>AdaGrad</strong></td>
<td>自适应学习率</td>
</tr>
<tr>
<td><strong>RMSProp</strong></td>
<td>避免学习率过小问题</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>结合 Momentum 和 RMSProp（最常用）</td>
</tr>
</tbody>
</table>
<h4 id="momentumadagradrmspropadam">梯度下降改进算法：Momentum、AdaGrad、RMSProp、Adam<a class="headerlink" href="#momentumadagradrmspropadam" title="Permanent link">&para;</a></h4>
<h5 id="momentum">Momentum<a class="headerlink" href="#momentum" title="Permanent link">&para;</a></h5>
<p>加入“惯性”的概念，让参数更新不仅依赖当前梯度，还考虑过去梯度的累积趋势。
就像一个小球在山谷中滚动时，会有<strong>惯性</strong>，不会立即停下。</p>
<p><strong>数学公式</strong></p>
<p>定义速度项 <span class="arithmatex">\(v_t\)</span>：</p>
<div class="arithmatex">\[
v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta J(\theta_t)
\]</div>
<p>参数更新为：</p>
<div class="arithmatex">\[
\theta_{t+1} = \theta_t - \alpha v_t
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(\beta\)</span> 是动量系数（通常取 <span class="arithmatex">\(0.9\)</span>）；</li>
<li><span class="arithmatex">\(v_t\)</span> 类似动量的累积梯度。</li>
<li><span class="arithmatex">\(\alpha\)</span> 是学习率</li>
</ul>
<p><strong>特点</strong></p>
<p>✅ 加速收敛（尤其在峡谷形曲面中）
✅ 抑制震荡（方向更稳定）
⚠️ 如果 <span class="arithmatex">\(\beta\)</span> 太大，可能“冲过谷底”。</p>
<hr />
<h5 id="adagrad">AdaGrad（自适应学习率算法）<a class="headerlink" href="#adagrad" title="Permanent link">&para;</a></h5>
<p>不同参数的梯度大小差异可能很大。
AdaGrad 通过对<strong>每个参数单独调整学习率</strong>，使得更新更稳定。</p>
<p><strong>数学公式</strong></p>
<p>设每个参数都有自己的累积平方梯度 <span class="arithmatex">\(G_t\)</span>：</p>
<div class="arithmatex">\[
G_t = G_{t-1} + [\nabla_\theta J(\theta_t)]^2
\]</div>
<p>参数更新：</p>
<div class="arithmatex">\[
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{G_t + \varepsilon}} \nabla_\theta J(\theta_t)
\]</div>
<p>其中 <span class="arithmatex">\(\varepsilon\)</span> 是防止除零的小常数（如 <span class="arithmatex">\(10^{-8}\)</span>）。</p>
<p>** 特点**</p>
<p>✅ 对稀疏数据有效（如 NLP 中的词嵌入）
⚠️ 缺点：学习率不断衰减，可能过早停滞。</p>
<hr />
<h5 id="rmsprop">RMSProp（均方根传播）<a class="headerlink" href="#rmsprop" title="Permanent link">&para;</a></h5>
<p>RMSProp 是为了解决 AdaGrad <strong>学习率衰减过快</strong>的问题。
它不是累积所有历史梯度平方，而是<strong>使用指数加权移动平均</strong>（EWMA）。</p>
<p><strong>数学公式</strong></p>
<div class="arithmatex">\[
E[g^2]*t = \beta E[g^2]*{t-1} + (1 - \beta)(\nabla_\theta J(\theta_t))^2
\]</div>
<div class="arithmatex">\[
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{E[g^2]*t + \varepsilon}} \nabla*\theta J(\theta_t)
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(\beta\)</span> 一般取 <span class="arithmatex">\(0.9\)</span>；</li>
<li><span class="arithmatex">\(E[g^2]_t\)</span> 是梯度平方的指数加权平均。</li>
</ul>
<p><strong>特点</strong></p>
<p>✅ 自适应学习率，不会过早衰减
✅ 在非平稳问题（如 RNN）上表现良好
⚠️ 无动量项，收敛方向可能抖动</p>
<hr />
<h5 id="adamadaptive-moment-estimation">Adam（Adaptive Moment Estimation）<a class="headerlink" href="#adamadaptive-moment-estimation" title="Permanent link">&para;</a></h5>
<p>Adam = Momentum + RMSProp
既考虑梯度的<strong>动量（方向）</strong>，又考虑梯度的<strong>自适应学习率（尺度）</strong>。</p>
<p><strong>数学公式</strong></p>
<p>定义一阶动量（平均梯度）和二阶动量（平均平方梯度）：</p>
<div class="arithmatex">\[
\begin{aligned}
m_t &amp;= \beta_1 m_{t-1} + (1 - \beta_1)\nabla_\theta J(\theta_t) \
v_t &amp;= \beta_2 v_{t-1} + (1 - \beta_2)(\nabla_\theta J(\theta_t))^2
\end{aligned}
\]</div>
<p>为修正偏差，进行偏差校正：</p>
<div class="arithmatex">\[
\begin{aligned}
\hat{m}_t &amp;= \frac{m_t}{1 - \beta_1^t} \
\hat{v}_t &amp;= \frac{v_t}{1 - \beta_2^t}
\end{aligned}
\]</div>
<p>最终参数更新：</p>
<div class="arithmatex">\[
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \varepsilon} \hat{m}_t
\]</div>
<p><strong>参数取值（默认推荐）</strong></p>
<ul>
<li><span class="arithmatex">\(\beta_1 = 0.9\)</span></li>
<li><span class="arithmatex">\(\beta_2 = 0.999\)</span></li>
<li><span class="arithmatex">\(\varepsilon = 10^{-8}\)</span></li>
</ul>
<p><strong>特点</strong></p>
<p>✅ 收敛快、稳定、鲁棒
✅ 对超参数不太敏感
✅ 深度学习中最常用的优化算法（几乎默认选择）
⚠️ 有时会陷入局部极小值或鞍点</p>
<hr />
<table>
<thead>
<tr>
<th>算法</th>
<th>学习率类型</th>
<th>是否使用动量</th>
<th>是否需要偏差修正</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SGD</strong></td>
<td>固定</td>
<td>❌</td>
<td>❌</td>
<td>简单稳定</td>
<td>收敛慢，震荡</td>
</tr>
<tr>
<td><strong>Momentum</strong></td>
<td>固定</td>
<td>✅</td>
<td>❌</td>
<td>加速收敛</td>
<td>可能超调</td>
</tr>
<tr>
<td><strong>AdaGrad</strong></td>
<td>自适应</td>
<td>❌</td>
<td>❌</td>
<td>稀疏数据好</td>
<td>学习率过快衰减</td>
</tr>
<tr>
<td><strong>RMSProp</strong></td>
<td>自适应</td>
<td>❌</td>
<td>❌</td>
<td>稳定快速</td>
<td>无动量</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>自适应</td>
<td>✅</td>
<td>✅</td>
<td>快速稳定、通用</td>
<td>可能欠收敛</td>
</tr>
</tbody>
</table>
<h4 id="_51">梯度下降法找到的一定是下降最快的方向吗？<a class="headerlink" href="#_51" title="Permanent link">&para;</a></h4>
<p>梯度下降法找到的是当前点处函数值下降最快的方向，但仅限于基于一阶导数信息的“局部”最快下降方向。换句话说，它是在当前点附近用线性近似模型所决定的下降最快方向<dfn seq=source_group_web_1 type=source_group_web>9。</p>
<p><strong>理论说明</strong></p>
<ul>
<li>梯度方向是多元函数在某一点处函数值增长最快的方向，因此负梯度方向就是下降最快的方向。梯度下降法正是沿着负梯度方向更新参数，使目标函数快速减小。</li>
<li>但是，梯度下降法只利用了一阶导数信息，没有考虑更高阶的信息（如Hessian矩阵，即二阶导数）。因此，它对“局部”下降方向的判断仅限于当前点的切平面，无法保证全局最优或者在复杂地形中找到最快的下降路径。</li>
</ul>
<p><strong>与牛顿法的对比</strong></p>
<ul>
<li>牛顿法在每一步都考虑了二阶导数信息，能更好地拟合当前点附近的曲面，理论上收敛速度更快（二阶收敛），尤其在接近最优解时表现优于仅有一阶收敛速度的梯度下降法。</li>
<li>但由于牛顿法计算和存储开销大，实际大规模问题中往往还是选用梯度下降及其变种。</li>
</ul>
<p><strong>结论</strong></p>
<p>梯度下降法在当前点的负梯度方向是函数下降最快的方向，但仅基于一阶信息，对整体优化路径而言不一定能把握“全局最快”的下降方向，特别是在有高阶信息可利用时。</p>
<h5 id="mbgd">MBGD需要注意什么？<a class="headerlink" href="#mbgd" title="Permanent link">&para;</a></h5>
<h6 id="1-mini-batch-batch-size">1. <strong>Mini-batch 大小（batch size）选择</strong><a class="headerlink" href="#1-mini-batch-batch-size" title="Permanent link">&para;</a></h6>
<ul>
<li>
<p><strong>过大</strong>：</p>
<ul>
<li>梯度估计非常准确，但更新次数少 → 收敛慢。</li>
<li>需要更多显存。</li>
<li>噪声太小，可能陷入局部最小值。</li>
<li>
<p><strong>过小</strong>：</p>
</li>
<li>
<p>噪声大，收敛不稳定。</p>
</li>
<li>并行效率低。</li>
</ul>
</li>
</ul>
<p>📏 一般经验：</p>
<ul>
<li>小模型：<code>batch_size = 32 ~ 128</code></li>
<li>大模型（CNN/RNN）：<code>batch_size = 256 ~ 1024</code></li>
<li>极大模型（Transformer）：可能使用 <code>4096</code> 或更多（配合学习率调整）</li>
</ul>
<h6 id="2-learning-rate">2. <strong>学习率（Learning Rate, α）要配合调整</strong><a class="headerlink" href="#2-learning-rate" title="Permanent link">&para;</a></h6>
<p>学习率和 batch size <strong>高度相关</strong>。
常见经验公式：</p>
<div class="arithmatex">\[
\alpha_{\text{new}} = \alpha_{\text{base}} \times \frac{b_{\text{new}}}{b_{\text{base}}}
\]</div>
<p>即 batch size 翻倍，学习率也可以近似翻倍。
但要通过<strong>warmup</strong>（热身）等策略平滑过渡。</p>
<h6 id="3-shuffling">3. <strong>数据打乱（Shuffling）</strong><a class="headerlink" href="#3-shuffling" title="Permanent link">&para;</a></h6>
<p>每个 epoch 前必须 <strong>随机打乱数据</strong>，否则每个 batch 的数据可能分布有偏差，导致模型学习方向错误。
尤其在分类任务中，如果样本按类别顺序排列而未打乱，会严重影响训练效果。</p>
<h6 id="4_7">4. <strong>正则化与批归一化的关系</strong><a class="headerlink" href="#4_7" title="Permanent link">&para;</a></h6>
<ul>
<li>当 batch 较小时，<strong>BatchNorm 的均值与方差估计不稳定</strong>。
  → 可以使用 <strong>LayerNorm</strong> 或 <strong>GroupNorm</strong> 代替。</li>
<li>当 batch 较大时，BatchNorm 通常能提升收敛速度。</li>
</ul>
<h6 id="5_3">5. <strong>梯度噪声与收敛稳定性</strong><a class="headerlink" href="#5_3" title="Permanent link">&para;</a></h6>
<ul>
<li>MBGD 的梯度带有一定随机性，这种“噪声”其实有益处（防止陷入局部极小点）。</li>
<li>但噪声过大（batch 太小）时，会导致训练过程震荡。</li>
<li>可结合 <strong>Momentum</strong> 或 <strong>Adam</strong> 来平滑更新。</li>
</ul>
<h6 id="6_3">6. <strong>显存与计算效率的权衡</strong><a class="headerlink" href="#6_3" title="Permanent link">&para;</a></h6>
<ul>
<li>batch 越大，占用显存越多。</li>
<li>
<p>若显存不足，可采用：</p>
<ul>
<li><strong>Gradient Accumulation（梯度累积）</strong>；</li>
<li><strong>Mixed Precision Training（混合精度训练）</strong>；</li>
<li><strong>Gradient Checkpointing（梯度检查点）</strong>。</li>
</ul>
</li>
</ul>
<h6 id="7-mini-batch">7. <strong>Mini-batch 的构建要有代表性</strong><a class="headerlink" href="#7-mini-batch" title="Permanent link">&para;</a></h6>
<ul>
<li>尽量保证每个 batch 的数据分布与整体训练集相似；</li>
<li>对于类别不平衡问题，建议采用 <strong>分层采样（stratified sampling）</strong>；</li>
<li>对序列任务，要注意样本长度的分布。</li>
</ul>
<hr />
<h5 id="_52">什么是正态分布？为什么要重视它？<a class="headerlink" href="#_52" title="Permanent link">&para;</a></h5>
<p><img alt="正态分布.png" src="../../imgs/machine/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83.png" /></p>
<p>正态分布（也叫高斯分布）是一种连续概率分布，其概率密度函数（PDF）定义为：</p>
<div class="arithmatex">\[
f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(- \frac{(x - \mu)^2}{2\sigma^2}\right)
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(x\)</span>：随机变量；</li>
<li><span class="arithmatex">\(\mu\)</span>：均值（mean），分布中心；</li>
<li><span class="arithmatex">\(\sigma^2\)</span>：方差（variance），控制分布宽度；</li>
<li><span class="arithmatex">\(\sigma\)</span>：标准差（standard deviation）。</li>
</ul>
<p><strong>记号</strong>：<span class="arithmatex">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span> 表示随机变量 <span class="arithmatex">\(X\)</span> 服从均值为 <span class="arithmatex">\(\mu\)</span>、方差为 <span class="arithmatex">\(\sigma^2\)</span> 的正态分布。</p>
<p><strong>正态分布的特征</strong></p>
<ol>
<li>
<p><strong>对称性</strong></p>
<ul>
<li>关于均值 <span class="arithmatex">\(\mu\)</span> 对称，<span class="arithmatex">\(\mu\)</span> 是分布的峰值点。</li>
</ul>
</li>
<li>
<p><strong>单峰性</strong></p>
<ul>
<li>只有一个峰，呈钟形曲线。</li>
</ul>
</li>
<li>
<p><strong>68-95-99.7 规则</strong>（经验法则）</p>
<ul>
<li>约 68% 的数据落在 <span class="arithmatex">\([\mu - \sigma, \mu + \sigma]\)</span></li>
<li>约 95% 的数据落在 <span class="arithmatex">\([\mu - 2\sigma, \mu + 2\sigma]\)</span></li>
<li>约 99.7% 的数据落在 <span class="arithmatex">\([\mu - 3\sigma, \mu + 3\sigma]\)</span></li>
</ul>
</li>
<li>
<p><strong>渐近性</strong></p>
<ul>
<li>曲线两端无限接近 x 轴，但永不触及。</li>
</ul>
</li>
</ol>
<p><strong>正态分布的数学性质</strong></p>
<ol>
<li><strong>期望与方差</strong></li>
</ol>
<div class="arithmatex">\[
\mathbb{E}[X] = \mu, \quad \mathrm{Var}(X) = \sigma^2
\]</div>
<ol>
<li><strong>标准化</strong></li>
</ol>
<p>可以将任意正态分布转化为标准正态分布 <span class="arithmatex">\(Z \sim \mathcal{N}(0, 1)\)</span>：</p>
<div class="arithmatex">\[
Z = \frac{X - \mu}{\sigma}
\]</div>
<ol>
<li><strong>线性组合仍为正态分布</strong></li>
</ol>
<p>如果 <span class="arithmatex">\(X \sim \mathcal{N}(\mu_X, \sigma_X^2)\)</span>，<span class="arithmatex">\(Y \sim \mathcal{N}(\mu_Y, \sigma_Y^2)\)</span> 独立，则：</p>
<div class="arithmatex">\[
aX + bY \sim \mathcal{N}(a\mu_X + b\mu_Y, a^2\sigma_X^2 + b^2\sigma_Y^2)
\]</div>
<h6 id="_53">为什么要重视正态分布？<a class="headerlink" href="#_53" title="Permanent link">&para;</a></h6>
<ol>
<li>
<p><strong>中心极限定理（Central Limit Theorem, CLT）</strong></p>
<ul>
<li>无论原始数据分布如何，<strong>大量独立同分布的随机变量平均值趋向于正态分布</strong>。</li>
<li>这使得正态分布在统计推断中非常重要，例如置信区间、假设检验等。</li>
</ul>
</li>
<li>
<p><strong>数学分析方便</strong></p>
<ul>
<li>正态分布函数光滑、可微、闭合形式明确。</li>
<li>许多统计量（如均值、方差、线性回归估计量）在大样本下服从正态分布。</li>
</ul>
</li>
<li>
<p><strong>机器学习中的应用</strong></p>
<ul>
<li>假设误差服从正态分布是 <strong>线性回归</strong>、<strong>逻辑回归</strong>、<strong>高斯混合模型（GMM）</strong>等模型的核心假设。</li>
<li>噪声建模、贝叶斯推断、概率生成模型都依赖于正态分布。</li>
</ul>
</li>
<li>
<p><strong>异常检测与标准化</strong></p>
<ul>
<li>数据标准化（z-score）：</li>
</ul>
</li>
</ol>
<div class="arithmatex">\[
z = \frac{x - \mu}{\sigma}
\]</div>
<ul>
<li>通过正态分布的性质可以检测异常值（远离 <span class="arithmatex">\(\mu\)</span> 多倍标准差）。</li>
</ul>
<hr />
<h5 id="_54">如何检查变量是否遵循正态分布？<a class="headerlink" href="#_54" title="Permanent link">&para;</a></h5>
<h6 id="1_10">1. 图示法（可视化判断）<a class="headerlink" href="#1_10" title="Permanent link">&para;</a></h6>
<ul>
<li>直方图：观察数据分布是否呈现“中间高、两头低”的钟形曲线，且左右基本对称。</li>
<li>Q-Q图 / P-P图：若数据点大致落在一条对角线上，表明数据与正态分布吻合良好。</li>
<li>优点：直观、易于理解，适合初步判断，尤其在大样本时更稳健。</li>
</ul>
<h6 id="2_8">2. 描述性统计量法<a class="headerlink" href="#2_8" title="Permanent link">&para;</a></h6>
<ul>
<li>偏度（Skewness）与峰度（Kurtosis）：</li>
<li>偏度≈0 表示对称分布（正态）；&gt;0 为右偏，&lt;0 为左偏。</li>
<li>峰度≈0 表示峰态适中（正态）；&gt;0 为尖峰，&lt;0 为平峰。</li>
<li>一般认为：|偏度| 。</li>
</ul>
<h6 id="3_5">3. 统计检验法（假设检验）<a class="headerlink" href="#3_5" title="Permanent link">&para;</a></h6>
<ul>
<li>Shapiro-Wilk 检验（W 检验）：<ul>
<li>适用于小样本（n ≤ 50），检验效能较高。</li>
</ul>
</li>
<li>Kolmogorov-Smirnov 检验（K-S 检验）：<ul>
<li>适用于大样本（n &gt; 50 或 n &gt; 2000，依软件而定），通过比较样本分布与正态分布的累积函数差异进行判断。</li>
</ul>
</li>
<li>结果解读：<ul>
<li>原假设 H₀：数据服从正态分布。</li>
<li>若 p &gt; 0.05，不拒绝原假设，认为数据符合正态分布。</li>
</ul>
</li>
</ul>
<p><strong>总结流程图：</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">1.</span> 绘制直方图/QQ图 → 初步判断形状
       ↓
<span class="k">2.</span> 查看偏度/峰度 → 数值是否在合理范围
       ↓
<span class="k">3.</span> 进行SW或KS检验 → p &gt; 0.05？
       ↓
<span class="k">4.</span> 综合结论：是否满足正态性假设
</code></pre></div>
<h2 id="_55">四、模型验证<a class="headerlink" href="#_55" title="Permanent link">&para;</a></h2>
<h3 id="41">4.1 过拟合 &amp; 欠拟合<a class="headerlink" href="#41" title="Permanent link">&para;</a></h3>
<p><img alt="过拟合&amp;欠拟合.png" src="../../imgs/machine/%E8%BF%87%E6%8B%9F%E5%90%88%26%E6%AC%A0%E6%8B%9F%E5%90%88.png" /></p>
<h4 id="underfitting">欠拟合（Underfitting）<a class="headerlink" href="#underfitting" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>概念</strong>：模型过于简单，无法捕捉数据中的规律。</li>
<li>
<p><strong>表现</strong>：</p>
<ul>
<li>训练集误差高</li>
<li>测试集误差高</li>
</ul>
</li>
<li>
<p><strong>原因</strong>：</p>
<ul>
<li>模型容量不足（如线性模型拟合非线性数据）</li>
<li>特征不足</li>
<li>训练轮数太少、正则化过强</li>
</ul>
</li>
</ul>
<h4 id="overfitting">过拟合（Overfitting）<a class="headerlink" href="#overfitting" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>概念</strong>：模型过于复杂，过度拟合训练数据中的噪声。</li>
<li>
<p><strong>表现</strong>：</p>
<ul>
<li>训练集误差低</li>
<li>测试集误差高</li>
</ul>
</li>
<li>
<p><strong>原因</strong>：</p>
<ul>
<li>模型容量太大（如深度神经网络）</li>
<li>特征太多、噪声大</li>
<li>训练轮数太多、正则化太弱</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>情况</th>
<th>训练误差</th>
<th>测试误差</th>
</tr>
</thead>
<tbody>
<tr>
<td>欠拟合</td>
<td>高</td>
<td>高</td>
</tr>
<tr>
<td>合适拟合</td>
<td>低</td>
<td>低</td>
</tr>
<tr>
<td>过拟合</td>
<td>低</td>
<td>高</td>
</tr>
</tbody>
</table>
<p>训练误差和测试误差随训练轮数变化曲线示意：</p>
<ul>
<li>欠拟合：误差曲线高且几乎不下降</li>
<li>过拟合：训练误差不断下降，测试误差下降后回升</li>
</ul>
<h4 id="_56">欠拟合解决策略<a class="headerlink" href="#_56" title="Permanent link">&para;</a></h4>
<ul>
<li>提高模型复杂度（如用多项式回归、深层神经网络）</li>
<li>增加特征或工程特征</li>
<li>减小正则化参数（如 L1/L2）</li>
<li>训练更长时间</li>
</ul>
<h4 id="_57">过拟合解决策略<a class="headerlink" href="#_57" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>正则化</strong>：L1/L2、Dropout</li>
<li><strong>简化模型</strong>：减少参数、降低网络层数</li>
<li><strong>增加训练数据</strong></li>
<li><strong>数据增强</strong>（如图像翻转、噪声）</li>
<li><strong>提前停止</strong>（Early stopping）</li>
<li><strong>交叉验证</strong>：判断模型泛化性能</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="c1"># 生成非线性数据</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 欠拟合：1阶多项式</span>
<span class="n">model_under</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model_under</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_under</span> <span class="o">=</span> <span class="n">model_under</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 适合拟合：3阶多项式</span>
<span class="n">model_good</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model_good</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_good</span> <span class="o">=</span> <span class="n">model_good</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 过拟合：15阶多项式</span>
<span class="n">model_over</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">15</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model_over</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_over</span> <span class="o">=</span> <span class="n">model_over</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 可视化</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred_under</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Underfit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred_good</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Good fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred_over</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Overfit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h3 id="42">4.2 交叉验证<a class="headerlink" href="#42" title="Permanent link">&para;</a></h3>
<p><img alt="交叉验证.png" src="../../imgs/machine/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.png" /></p>
<p>在机器学习中，我们希望模型不仅在训练集上表现好，还能在未见过的数据上有良好的表现（即泛化能力）。
<strong>交叉验证</strong>是一种通过<strong>多次划分训练集和验证集</strong>来评估模型性能的技术。</p>
<ul>
<li><strong>目标</strong>：减少模型评估的偏差，使性能评估更稳定可靠。</li>
</ul>
<h4 id="_58">基本思想<a class="headerlink" href="#_58" title="Permanent link">&para;</a></h4>
<p>假设我们有数据集 <span class="arithmatex">\(D\)</span>，大小为 <span class="arithmatex">\(N\)</span>。</p>
<ol>
<li>将 <span class="arithmatex">\(D\)</span> 分成 <span class="arithmatex">\(k\)</span> 个大小相同的子集（folds）：<span class="arithmatex">\(D_1, D_2, ..., D_k\)</span></li>
<li>
<p>进行 <span class="arithmatex">\(k\)</span> 次训练和验证：</p>
</li>
<li>
<p>每次用 <span class="arithmatex">\(k-1\)</span> 个子集作为训练集</p>
</li>
<li>剩下的 1 个子集作为验证集</li>
<li>每次计算性能指标（如准确率 <span class="arithmatex">\(Acc_i\)</span>）</li>
<li>平均 <span class="arithmatex">\(k\)</span> 次性能指标得到最终估计：
   $$
   Acc_{CV} = \frac{1}{k} \sum_{i=1}^{k} Acc_i
   $$</li>
</ol>
<h4 id="_59">常用交叉验证方法<a class="headerlink" href="#_59" title="Permanent link">&para;</a></h4>
<h5 id="1-k-k-fold-cv">1. k 折交叉验证（k-Fold CV）<a class="headerlink" href="#1-k-k-fold-cv" title="Permanent link">&para;</a></h5>
<ul>
<li>将数据随机分成 <span class="arithmatex">\(k\)</span> 份，每次轮流做验证集。</li>
<li>常用 <span class="arithmatex">\(k=5\)</span> 或 <span class="arithmatex">\(k=10\)</span>。</li>
<li>优点：利用数据充分、稳定性好</li>
<li>缺点：训练次数 <span class="arithmatex">\(k\)</span> 次，计算量大</li>
</ul>
<h5 id="2-leave-one-out-loo">2. 留一法交叉验证（Leave-One-Out, LOO）<a class="headerlink" href="#2-leave-one-out-loo" title="Permanent link">&para;</a></h5>
<ul>
<li>每次留 1 个样本做验证，其余 <span class="arithmatex">\(N-1\)</span> 个样本做训练</li>
<li>优点：无偏估计</li>
<li>缺点：计算量非常大（尤其样本多时）</li>
</ul>
<h5 id="3-p-leave-p-out-lpo">3. 留 P 法交叉验证（Leave-P-Out, LPO）<a class="headerlink" href="#3-p-leave-p-out-lpo" title="Permanent link">&para;</a></h5>
<ul>
<li>每次留 <span class="arithmatex">\(p\)</span> 个样本做验证</li>
<li>泛化了 LOO</li>
</ul>
<h5 id="4-k-stratified-k-fold">4. 分层 k 折交叉验证（Stratified k-Fold）<a class="headerlink" href="#4-k-stratified-k-fold" title="Permanent link">&para;</a></h5>
<ul>
<li>适用于分类问题</li>
<li>保证每折中各类别比例与整体数据集一致</li>
<li>Python 常用 <code>StratifiedKFold</code></li>
</ul>
<h5 id="5-shuffle-split">5. 重复随机划分（Shuffle Split）<a class="headerlink" href="#5-shuffle-split" title="Permanent link">&para;</a></h5>
<ul>
<li>每次随机划分训练集和验证集</li>
<li>可以重复多次</li>
<li>优点：灵活，训练集比例可控</li>
</ul>
<h4 id="_60">数学表述<a class="headerlink" href="#_60" title="Permanent link">&para;</a></h4>
<p>设性能指标函数为 <span class="arithmatex">\(M(\cdot)\)</span>（如均方误差 MSE、准确率 Accuracy 等），<span class="arithmatex">\(k\)</span> 折交叉验证的估计为：</p>
<div class="arithmatex">\[
CV_{k} = \frac{1}{k} \sum_{i=1}^{k} M(\text{model trained on } D \setminus D_i, D_i)
\]</div>
<ul>
<li><span class="arithmatex">\(D \setminus D_i\)</span>：训练集</li>
<li><span class="arithmatex">\(D_i\)</span>：验证集</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># 数据集</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="c1"># k-Fold 交叉验证</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kf</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;5-Fold CV Accuracy:&quot;</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;平均准确率:&quot;</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># 分层 k-Fold（分类问题）</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores_strat</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">skf</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stratified 5-Fold CV Accuracy:&quot;</span><span class="p">,</span> <span class="n">scores_strat</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</code></pre></div>
<h3 id="43">4.3 网格搜索<a class="headerlink" href="#43" title="Permanent link">&para;</a></h3>
<h4 id="_61">网格搜索的核心原理<a class="headerlink" href="#_61" title="Permanent link">&para;</a></h4>
<ol>
<li>定义参数网格<br />
   创建一个包含超参数值的参数网格，即所有可能的超参数组合。例如，对于支持向量机（SVM），可以搜索参数 <code>C</code> 和 <code>gamma</code>，对于随机森林可以搜索 <code>max_depth</code> 和 <code>n_estimators</code></li>
<li>遍历参数组合<br />
   按照网格中的所有组合训练模型并评估性能。对于每一个参数组合，使用交叉验证来评估模型性能</li>
<li>选择最佳参数<br />
   根据某种评价指标（如准确率、F1分数或均方误差），选出性能最好的参数配置</li>
</ol>
<h4 id="_62">网格搜索的流程<a class="headerlink" href="#_62" title="Permanent link">&para;</a></h4>
<ul>
<li>数据准备：准备好训练集和验证集，验证集用于评估每个参数组合的性能</li>
<li>定义模型：指定需要优化的模型（例如决策树、支持向量机或深度学习模型）</li>
<li>参数范围：定义需要调节的超参数及其可能的取值范围</li>
<li>训练与评估：遍历所有参数组合，训练模型，并在验证集上评估性能</li>
<li>选择最佳参数：根据验证集的评价指标，选出性能最好的超参数组合</li>
</ul>
<h4 id="_63">优点与缺点<a class="headerlink" href="#_63" title="Permanent link">&para;</a></h4>
<p><strong>优点：</strong></p>
<ul>
<li>系统全面：通过遍历所有参数组合，保证找到全局最优解（在给定搜索空间内）</li>
<li>易于实现：各种机器学习库（如 scikit-learn）提供了简单的接口来实现网格搜索</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>计算成本高：随着参数数量和可能的取值增加，搜索空间会呈指数级增长，导致训练时间过长</li>
<li>效率低下：对于大型数据集和复杂模型，网格搜索可能会变得非常耗时</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">加载数据集</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>
<span class="n">定义模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">定义参数网格</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">],</span>
    <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;rbf&#39;</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">网格搜索</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">输出最佳参数和对应的性能</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;最佳参数:&quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;最佳准确率:&quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div>
<h3 id="44">4.4 随机搜索<a class="headerlink" href="#44" title="Permanent link">&para;</a></h3>
<h4 id="_64">随机搜索的核心原理<a class="headerlink" href="#_64" title="Permanent link">&para;</a></h4>
<ol>
<li>随机采样<br />
   与网格搜索不同，随机搜索不会穷举所有参数组合，而是从参数的候选分布中随机选择组合进行评估。这样可以大大减少计算量，尤其在超参数数量较多时，效率更高。</li>
<li>迭代优化<br />
   随机搜索通过多次迭代，每次随机生成一组超参数，训练模型并评估性能，记录最佳结果。随着迭代次数增加，找到较优解的概率提升。</li>
<li>灵活的搜索空间<br />
   支持多种概率分布（如均匀分布、对数均匀分布、正态分布等），能更好地探索参数空间，有助于跳出局部最优解。</li>
</ol>
<h4 id="_65">优点与缺点<a class="headerlink" href="#_65" title="Permanent link">&para;</a></h4>
<p><strong>优点：</strong></p>
<ul>
<li>计算效率高：相比网格搜索，计算成本显著降低，尤其适合高维参数空间</li>
<li>灵活度高：支持连续、离散参数及各种分布，减少人为设定偏差</li>
<li>易于并行：每次采样独立，可方便地并行处理</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>结果不确定性：由于随机性，不同运行结果可能有差异，不一定总能找到全局最优解</li>
<li>缺乏方向性：每次采样独立，不能利用历史信息指导搜索</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">randint</span>
<span class="n">定义模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">定义参数分布</span>
<span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">随机搜索</span>
<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_dist</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">输出最佳参数</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;最佳参数:&quot;</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;最佳得分:&quot;</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div>
<h3 id="45">4.5 贝叶斯优化<a class="headerlink" href="#45" title="Permanent link">&para;</a></h3>
<h2 id="_66">五、分类<a class="headerlink" href="#_66" title="Permanent link">&para;</a></h2>
<p>分类问题的目标是：
学习一个模型 <span class="arithmatex">\(f(x)\)</span> 来预测样本属于类别 <span class="arithmatex">\(y\)</span> 的概率。</p>
<p>常见类型：</p>
<ul>
<li><strong>二分类</strong>：<span class="arithmatex">\(y \in {0,1}\)</span></li>
<li><strong>多分类</strong>：<span class="arithmatex">\(y \in {1,2,...,K}\)</span></li>
</ul>
<p>我们定义一个损失函数 <span class="arithmatex">\(L(y, \hat{y})\)</span> 衡量预测 <span class="arithmatex">\(\hat{y}\)</span> 与真实标签 <span class="arithmatex">\(y\)</span> 之间的差距。
学习目标是最小化期望损失：</p>
<div class="arithmatex">\[
\min_\theta ; \mathbb{E}*{(x,y)} [L(y, f*\theta(x))]
\]</div>
<h3 id="51">5.1 损失函数<a class="headerlink" href="#51" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>类型</th>
<th>损失函数</th>
<th>常用于算法</th>
</tr>
</thead>
<tbody>
<tr>
<td>0-1 损失</td>
<td>理论评估</td>
<td>理论最优分类器</td>
</tr>
<tr>
<td>对数损失 / 交叉熵</td>
<td>Logistic 回归、神经网络</td>
<td>概率模型</td>
</tr>
<tr>
<td>Hinge 损失</td>
<td>支持向量机（SVM）</td>
<td>间隔最大化</td>
</tr>
<tr>
<td>指数损失</td>
<td>Adaboost</td>
<td>集成方法</td>
</tr>
<tr>
<td>Softmax + CrossEntropy</td>
<td>多分类</td>
<td>神经网络、多类逻辑回归</td>
</tr>
<tr>
<td>Focal Loss</td>
<td>目标检测（不平衡样本）</td>
<td>深度学习</td>
</tr>
</tbody>
</table>
<h4 id="1_11">1️⃣ 二分类损失函数详解<a class="headerlink" href="#1_11" title="Permanent link">&para;</a></h4>
<h5 id="1-0-1">(1) <strong>0-1 损失</strong><a class="headerlink" href="#1-0-1" title="Permanent link">&para;</a></h5>
<p><strong>定义：</strong></p>
<div class="arithmatex">\[
L(y, \hat{y}) =
\begin{cases}
0, &amp; \text{if } y = \hat{y} \\
1, &amp; \text{if } y \neq \hat{y}
\end{cases}
\]</div>
<p><strong>特点：</strong></p>
<ul>
<li>反映分类准确率</li>
<li>不可导、不连续，不能直接用于优化</li>
</ul>
<p><strong>用途：</strong></p>
<ul>
<li>理论分析（例如 Bayes 最优分类器）</li>
</ul>
<hr />
<h5 id="2_9">(2) <strong>对数损失 / 交叉熵损失</strong><a class="headerlink" href="#2_9" title="Permanent link">&para;</a></h5>
<p><strong>核心思想：</strong>
用预测概率逼近真实分布，最小化它们的差异（即交叉熵）。</p>
<p>假设输出为概率 <span class="arithmatex">\(\hat{p} = P(y=1|x)\)</span>，真实标签 <span class="arithmatex">\(y \in {0,1}\)</span>。</p>
<p><strong>公式：</strong></p>
<div class="arithmatex">\[
L(y, \hat{p}) = -[y \log(\hat{p}) + (1 - y)\log(1 - \hat{p})]
\]</div>
<p><strong>解释：</strong></p>
<ul>
<li>当预测接近真实标签时，损失趋近于 0；</li>
<li>当预测错误且置信度高时，损失巨大（惩罚更强）。</li>
</ul>
<p><strong>图像理解：</strong>
交叉熵曲线比平方误差陡峭，优化效果更好。</p>
<p><strong>代码示例：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">log_loss</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<p><strong>适用场景：</strong></p>
<ul>
<li>逻辑回归（Logistic Regression）</li>
<li>神经网络分类层（Softmax + CrossEntropy）</li>
</ul>
<hr />
<h5 id="3-hinge">(3) <strong>Hinge 损失（合页损失）</strong><a class="headerlink" href="#3-hinge" title="Permanent link">&para;</a></h5>
<p>用于支持向量机（SVM）等<strong>最大间隔分类</strong>。</p>
<p>假设标签 <span class="arithmatex">\(y \in {-1, +1}\)</span>，预测值为 <span class="arithmatex">\(f(x)\)</span>。</p>
<p><strong>公式：</strong></p>
<div class="arithmatex">\[
L(y, f(x)) = \max(0, 1 - y \cdot f(x))
\]</div>
<p><strong>解释：</strong></p>
<ul>
<li>若 <span class="arithmatex">\(y \cdot f(x) \ge 1\)</span>，说明分类正确且有足够间隔 → 损失为 0；</li>
<li>否则，损失与距离成线性增加。</li>
</ul>
<p><strong>几何意义：</strong>
希望点离分界面“更远”，从而提升鲁棒性。</p>
<p><strong>代码示例：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">hinge_loss</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">pred_decision</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">pred_decision</span><span class="p">))</span>
</code></pre></div>
<hr />
<h5 id="4-exponential-loss">(4) <strong>指数损失（Exponential Loss）</strong><a class="headerlink" href="#4-exponential-loss" title="Permanent link">&para;</a></h5>
<p>用于 Adaboost 算法。</p>
<p><strong>公式：</strong></p>
<div class="arithmatex">\[
L(y, f(x)) = e^{-y f(x)}
\]</div>
<p><strong>解释：</strong></p>
<ul>
<li>错误分类（<span class="arithmatex">\(y f(x) &lt; 0\)</span>）时惩罚呈指数增长；</li>
<li>对异常值敏感。</li>
</ul>
<p><strong>应用：</strong></p>
<ul>
<li>Boosting 系列算法的理论基础</li>
</ul>
<hr />
<h4 id="2_10">2️⃣ 多分类损失函数<a class="headerlink" href="#2_10" title="Permanent link">&para;</a></h4>
<h5 id="1-softmax-cross-entropy">(1) <strong>Softmax + Cross-Entropy</strong><a class="headerlink" href="#1-softmax-cross-entropy" title="Permanent link">&para;</a></h5>
<p>这是多分类最常见的损失。</p>
<p><strong>Softmax 层：</strong>
将模型输出 <span class="arithmatex">\(z_i\)</span> 映射为概率：</p>
<div class="arithmatex">\[
\hat{p_i} = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
\]</div>
<p><strong>Cross-Entropy：</strong>
$$
L = - \sum_{i=1}^K y_i \log(\hat{p_i})
$$</p>
<p>其中 <span class="arithmatex">\(y_i\)</span> 是独热编码（one-hot）形式。
若真实类别为 <span class="arithmatex">\(c\)</span>：</p>
<div class="arithmatex">\[
L = -\log(\hat{p_c})
\]</div>
<p><strong>解释：</strong>
预测真实类别的概率越大，损失越小。</p>
<p><strong>代码示例：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># 类别索引</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]])</span>  <span class="c1"># logits</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</code></pre></div>
<hr />
<h5 id="2-hinge">(2) <strong>多类 Hinge 损失</strong><a class="headerlink" href="#2-hinge" title="Permanent link">&para;</a></h5>
<p><strong>公式：</strong></p>
<div class="arithmatex">\[
L = \sum_{i \ne y} \max(0, f_i - f_y + \Delta)
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(f_i\)</span> 为类别 <span class="arithmatex">\(i\)</span> 的得分</li>
<li><span class="arithmatex">\(\Delta\)</span> 为间隔（通常取 1）</li>
</ul>
<p><strong>用途：</strong></p>
<ul>
<li>多类支持向量机（Multi-class SVM）</li>
</ul>
<h4 id="3-weighted-loss">3️⃣ 带权损失（Weighted Loss）<a class="headerlink" href="#3-weighted-loss" title="Permanent link">&para;</a></h4>
<p>在样本类别不平衡时（例如欺诈检测、医学诊断），
给不同类别不同权重：</p>
<div class="arithmatex">\[
L = -[w_1 y \log(\hat{p}) + w_0 (1-y)\log(1-\hat{p})]
\]</div>
<p>或多分类版本：</p>
<div class="arithmatex">\[
L = -\sum_{i=1}^K w_i y_i \log(\hat{p_i})
\]</div>
<p><strong>深度学习扩展：</strong>
Focal Loss（用于目标检测）：</p>
<div class="arithmatex">\[
L = - (1 - \hat{p_t})^\gamma \log(\hat{p_t})
\]</div>
<p>其中 <span class="arithmatex">\(\gamma &gt; 0\)</span> 控制困难样本的关注度。</p>
<h3 id="52">5.2 评估指标<a class="headerlink" href="#52" title="Permanent link">&para;</a></h3>
<h4 id="_67">🧭 一、分类任务中的四种结果类型（混淆矩阵基础）<a class="headerlink" href="#_67" title="Permanent link">&para;</a></h4>
<p>在二分类问题中（正类 Positive / 负类 Negative），每个样本预测结果可分为四类：</p>
<table>
<thead>
<tr>
<th>实际 / 预测</th>
<th>Positive</th>
<th>Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive（真实为正）</td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr>
<td>Negative（真实为负）</td>
<td>False Positive (FP)</td>
<td>True Negative (TN)</td>
</tr>
</tbody>
</table>
<p>由此形成<strong>混淆矩阵（Confusion Matrix）</strong>：</p>
<div class="arithmatex">\[
\begin{bmatrix}
TP &amp; FP \
FN &amp; TN
\end{bmatrix}
\]</div>
<h4 id="accuracy">🧮 二、准确率（Accuracy）<a class="headerlink" href="#accuracy" title="Permanent link">&para;</a></h4>
<h5 id="_68">定义：<a class="headerlink" href="#_68" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]</div>
<h5 id="_69">含义：<a class="headerlink" href="#_69" title="Permanent link">&para;</a></h5>
<p>预测正确的样本占总样本比例。</p>
<h5 id="_70">示例：<a class="headerlink" href="#_70" title="Permanent link">&para;</a></h5>
<p>若模型预测 100 个样本，其中正确 90 个，则：
$$
\text{Accuracy} = \frac{90}{100} = 0.9
$$</p>
<h5 id="_71">优缺点：<a class="headerlink" href="#_71" title="Permanent link">&para;</a></h5>
<ul>
<li>✅ 简单直观。</li>
<li>❌ 当样本严重不平衡时（例如 99% 为负类），准确率会<strong>掩盖错误</strong>。例如模型全预测为负类，准确率仍可达 99%。</li>
</ul>
<h4 id="precisionrecall">🎯 三、精确率（Precision）与召回率（Recall）<a class="headerlink" href="#precisionrecall" title="Permanent link">&para;</a></h4>
<h5 id="1-precision">1️⃣ 精确率（Precision）<a class="headerlink" href="#1-precision" title="Permanent link">&para;</a></h5>
<p>定义：
$$
\text{Precision} = \frac{TP}{TP + FP}
$$</p>
<p>含义：
被预测为正类的样本中，<strong>有多少是真的正类</strong>。</p>
<blockquote>
<p>高精确率 → 模型“慎重”，预测为正的样本大多确实是正。</p>
</blockquote>
<hr />
<h5 id="2-recall">2️⃣ 召回率（Recall）<a class="headerlink" href="#2-recall" title="Permanent link">&para;</a></h5>
<p>定义：
$$
\text{Recall} = \frac{TP}{TP + FN}
$$</p>
<p>含义：
所有真实正类样本中，<strong>模型找出了多少个</strong>。</p>
<blockquote>
<p>高召回率 → 模型“敏感”，尽可能找出所有正样本。</p>
</blockquote>
<hr />
<h5 id="_72">数值示例：<a class="headerlink" href="#_72" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>实际</th>
<th>预测</th>
</tr>
</thead>
<tbody>
<tr>
<td>100 个正样本中模型找出 80 个（TP=80，FN=20）</td>
<td></td>
</tr>
<tr>
<td>同时错误预测 10 个负样本为正（FP=10）</td>
<td></td>
</tr>
</tbody>
</table>
<p>计算：
$$
\text{Precision} = \frac{80}{80+10} = 0.8889
$$
$$
\text{Recall} = \frac{80}{80+20} = 0.8
$$</p>
<hr />
<h5 id="precision-recall-tradeoff">平衡关系（Precision-Recall Tradeoff）：<a class="headerlink" href="#precision-recall-tradeoff" title="Permanent link">&para;</a></h5>
<p>通常，<strong>提高 Recall 会降低 Precision</strong>（模型更“激进”），反之亦然。
阈值（threshold）调节能改变两者平衡。</p>
<h4 id="f1-f1-score">🔁 四、F1 分数（F1-score）<a class="headerlink" href="#f1-f1-score" title="Permanent link">&para;</a></h4>
<p>为平衡精确率与召回率，引入<strong>调和平均数</strong>：</p>
<div class="arithmatex">\[
F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]</div>
<h5 id="_73">示例：<a class="headerlink" href="#_73" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\text{Precision}=0.8889,\quad \text{Recall}=0.8
\]</div>
<p>代入：</p>
<div class="arithmatex">\[
F_1 = 2 \times \frac{0.8889\times 0.8}{0.8889+0.8} = 0.842
\]</div>
<h5 id="_74">特点：<a class="headerlink" href="#_74" title="Permanent link">&para;</a></h5>
<ul>
<li>F1 高 → 模型同时兼顾高 Precision 和 Recall；</li>
<li>常用于<strong>不平衡分类问题</strong>；</li>
<li>当正负样本比例极不均衡时，F1 比 Accuracy 更能反映模型性能。</li>
</ul>
<h4 id="f">⚖️ 五、Fβ 分数（通用形式）<a class="headerlink" href="#f" title="Permanent link">&para;</a></h4>
<p>若希望更关注 Recall 或 Precision，可用加权版：</p>
<div class="arithmatex">\[
F_\beta = (1+\beta^2)\cdot \frac{\text{Precision}\cdot\text{Recall}}{(\beta^2\cdot\text{Precision})+\text{Recall}}
\]</div>
<ul>
<li><span class="arithmatex">\(\beta&gt;1\)</span> → 更注重 Recall；</li>
<li><span class="arithmatex">\(\beta&lt;1\)</span> → 更注重 Precision。</li>
</ul>
<p>例：<span class="arithmatex">\(F_{0.5}\)</span> 偏向精确率，<span class="arithmatex">\(F_2\)</span> 偏向召回率。</p>
<h4 id="specificityfpr">📊 六、特异度（Specificity）与假阳率（FPR）<a class="headerlink" href="#specificityfpr" title="Permanent link">&para;</a></h4>
<h5 id="true-negative-rate">特异度（True Negative Rate）：<a class="headerlink" href="#true-negative-rate" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\text{Specificity} = \frac{TN}{TN + FP}
\]</div>
<h5 id="false-positive-rate">假阳率（False Positive Rate）：<a class="headerlink" href="#false-positive-rate" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\text{FPR} = \frac{FP}{FP + TN} = 1 - \text{Specificity}
\]</div>
<blockquote>
<p>在医学场景，特异度表示模型识别“健康人”的能力。</p>
</blockquote>
<h4 id="roc-aucarea-under-curve">🪶 七、ROC 曲线与 AUC（Area Under Curve）<a class="headerlink" href="#roc-aucarea-under-curve" title="Permanent link">&para;</a></h4>
<p><img alt="roc曲线.png" src="../../imgs/machine/roc%E6%9B%B2%E7%BA%BF.png" /></p>
<h5 id="1-roc">1️⃣ ROC 曲线定义：<a class="headerlink" href="#1-roc" title="Permanent link">&para;</a></h5>
<p>ROC 曲线以：</p>
<ul>
<li>横轴：假阳率（FPR）</li>
<li>纵轴：真正率（TPR）= Recall = <span class="arithmatex">\(\frac{TP}{TP + FN}\)</span></li>
</ul>
<p>当阈值从 1 降到 0 时，绘制 <span class="arithmatex">\((\text{FPR}, \text{TPR})\)</span> 点。</p>
<h5 id="2-auc">2️⃣ AUC 指标：<a class="headerlink" href="#2-auc" title="Permanent link">&para;</a></h5>
<p>AUC 是 ROC 曲线下的面积：
$$
\text{AUC} = \int_0^1 \text{TPR}(x),dx
$$</p>
<p>AUC 取值范围 <span class="arithmatex">\([0,1]\)</span>：</p>
<ul>
<li>AUC = 1：完美分类器</li>
<li>AUC = 0.5：随机猜测</li>
<li>AUC &lt; 0.5：性能比随机还差（预测结果可反转）</li>
</ul>
<h5 id="3_6">3️⃣ 直观解释：<a class="headerlink" href="#3_6" title="Permanent link">&para;</a></h5>
<p>AUC = “随机抽取一对（正样本、负样本），模型判断正样本得分更高的概率”。</p>
<h4 id="pr-precisionrecall">🧱 八、PR 曲线（Precision–Recall 曲线）<a class="headerlink" href="#pr-precisionrecall" title="Permanent link">&para;</a></h4>
<p><img alt="pr曲线.png" src="../../imgs/machine/pr%E6%9B%B2%E7%BA%BF.png" /></p>
<p>横轴为 Recall，纵轴为 Precision，曲线反映阈值变化下的取舍。</p>
<p>在类别极不平衡的情况下，<strong>PR 曲线更有意义</strong>（相比 ROC 曲线）。</p>
<h4 id="_75">🧾 九、宏平均与微平均（多分类指标）<a class="headerlink" href="#_75" title="Permanent link">&para;</a></h4>
<p>当分类任务是多类别（如 3 类、5 类）时，Precision、Recall、F1 有多种求法：</p>
<ul>
<li>
<p><strong>Macro 平均</strong>（Macro-Average）：
  对每个类别独立计算指标，然后平均：
  $$
  \text{Macro-F1} = \frac{1}{C}\sum_{i=1}^C F_1^{(i)}
  $$
  → 各类权重相同（不考虑样本数差异）</p>
</li>
<li>
<p><strong>Micro 平均</strong>（Micro-Average）：
  统计所有样本总的 TP、FP、FN 后再算：
  $$
  \text{Micro-F1} = \frac{2TP_{\text{total}}}{2TP_{\text{total}} + FP_{\text{total}} + FN_{\text{total}}}
  $$
  → 各样本权重相同（大类别占主导）</p>
</li>
</ul>
<h4 id="cohens-kappa">🧩 十、Cohen’s Kappa 系数（一致性度量）<a class="headerlink" href="#cohens-kappa" title="Permanent link">&para;</a></h4>
<h5 id="_76">定义：<a class="headerlink" href="#_76" title="Permanent link">&para;</a></h5>
<p>$$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$
其中：</p>
<ul>
<li><span class="arithmatex">\(p_o\)</span> = 实际观测一致率（即 Accuracy）</li>
<li><span class="arithmatex">\(p_e\)</span> = 随机一致的期望值</li>
</ul>
<blockquote>
<p>反映预测与真实标签之间的一致性，剔除了随机一致的影响。</p>
</blockquote>
<p>取值：</p>
<ul>
<li><span class="arithmatex">\(\kappa=1\)</span>：完全一致</li>
<li><span class="arithmatex">\(\kappa=0\)</span>：随机一致</li>
<li><span class="arithmatex">\(\kappa&lt;0\)</span>：比随机还差</li>
</ul>
<h4 id="_77">🧮 十一、示例汇总（数值计算）<a class="headerlink" href="#_77" title="Permanent link">&para;</a></h4>
<p>假设：</p>
<table>
<thead>
<tr>
<th>实际 / 预测</th>
<th>Positive</th>
<th>Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive</td>
<td>50 (TP)</td>
<td>10 (FN)</td>
</tr>
<tr>
<td>Negative</td>
<td>5 (FP)</td>
<td>35 (TN)</td>
</tr>
</tbody>
</table>
<p>计算：</p>
<ul>
<li>Accuracy = <span class="arithmatex">\((50+35)/100 = 0.85\)</span></li>
<li>Precision = <span class="arithmatex">\(50/(50+5)=0.909\)</span></li>
<li>Recall = <span class="arithmatex">\(50/(50+10)=0.833\)</span></li>
<li>F1 = <span class="arithmatex">\(2×(0.909×0.833)/(0.909+0.833)=0.87\)</span></li>
<li>Specificity = <span class="arithmatex">\(35/(35+5)=0.875\)</span></li>
<li>FPR = <span class="arithmatex">\(0.125\)</span></li>
</ul>
<table>
<thead>
<tr>
<th>指标</th>
<th>关注点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td>整体正确率</td>
<td>类别平衡时使用</td>
</tr>
<tr>
<td>Precision</td>
<td>预测为正的可信度</td>
<td>欺诈检测、垃圾邮件识别</td>
</tr>
<tr>
<td>Recall</td>
<td>覆盖正类的能力</td>
<td>疾病筛查、风险监测</td>
</tr>
<tr>
<td>F1</td>
<td>精确率与召回率平衡</td>
<td>不平衡分类</td>
</tr>
<tr>
<td>ROC–AUC</td>
<td>阈值无关的整体性能</td>
<td>各类分类任务通用</td>
</tr>
<tr>
<td>PR–AUC</td>
<td>少数类关注场景</td>
<td>严重不平衡数据</td>
</tr>
<tr>
<td>Kappa</td>
<td>剔除随机一致影响</td>
<td>多分类一致性评估</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span>
    <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="p">)</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>  <span class="c1"># 概率输出</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Precision:&quot;</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Recall:&quot;</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1:&quot;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ROC-AUC:&quot;</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h3 id="53">5.3 多分类<a class="headerlink" href="#53" title="Permanent link">&para;</a></h3>
<p><strong>多分类问题</strong> 是指：每个样本只能属于多个可能类别中的<strong>一个且仅一个</strong>。</p>
<p>例如：</p>
<ul>
<li>输入：一张图片</li>
<li>输出类别：{猫, 狗, 鸟}
  → 只能属于其中一个。</li>
</ul>
<p>数学上，给定：</p>
<ul>
<li>输入特征：<span class="arithmatex">\(x \in \mathbb{R}^d\)</span></li>
<li>输出标签：<span class="arithmatex">\(y \in {1, 2, \dots, K}\)</span></li>
</ul>
<p>任务目标是学习一个函数：
$$
f_\theta(x) = \arg\max_{k} P(y=k \mid x; \theta)
$$</p>
<h4 id="_78">⚙️ 二、模型结构与原理<a class="headerlink" href="#_78" title="Permanent link">&para;</a></h4>
<h5 id="1_12">1️⃣ 线性模型<a class="headerlink" href="#1_12" title="Permanent link">&para;</a></h5>
<p>最基本形式（如逻辑回归的多分类扩展）：
$$
z_k = w_k^\top x + b_k, \quad k = 1, 2, \dots, K
$$
其中：</p>
<ul>
<li><span class="arithmatex">\(w_k\)</span> 是第 <span class="arithmatex">\(k\)</span> 类的权重向量；</li>
<li><span class="arithmatex">\(b_k\)</span> 是偏置；</li>
<li><span class="arithmatex">\(z_k\)</span> 是该类别的“打分”（logit）。</li>
</ul>
<h5 id="2-softmax">2️⃣ Softmax 函数（归一化概率）<a class="headerlink" href="#2-softmax" title="Permanent link">&para;</a></h5>
<p>为了得到各类的概率分布，使用 Softmax：
$$
p_k = P(y=k \mid x; \theta) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
$$</p>
<h5 id="softmax">🔍 Softmax 性质：<a class="headerlink" href="#softmax" title="Permanent link">&para;</a></h5>
<ol>
<li>每个 <span class="arithmatex">\(p_k \in (0,1)\)</span>；</li>
<li><span class="arithmatex">\(\sum_k p_k = 1\)</span>。</li>
</ol>
<h4 id="cross-entropy-loss">🧮 三、损失函数（Cross-Entropy Loss）<a class="headerlink" href="#cross-entropy-loss" title="Permanent link">&para;</a></h4>
<p>我们使用<strong>交叉熵损失函数（Cross-Entropy Loss）</strong>，它是极大似然估计的负对数形式。</p>
<hr />
<h5 id="1_13">1️⃣ 极大似然推导：<a class="headerlink" href="#1_13" title="Permanent link">&para;</a></h5>
<p>给定样本 <span class="arithmatex">\((x^{(i)}, y^{(i)})\)</span>，其概率为：
$$
P(y^{(i)} \mid x^{(i)}; \theta) = \prod_{k=1}^K p_k^{\mathbf{1}(y^{(i)}=k)}
$$</p>
<p>其中 <span class="arithmatex">\(\mathbf{1}(\cdot)\)</span> 是指示函数。</p>
<p>整个训练集似然为：
$$
L(\theta) = \prod_{i=1}^m P(y^{(i)} \mid x^{(i)}; \theta)
$$</p>
<p>取对数（对数似然）：
$$
\log L(\theta) = \sum_{i=1}^m \sum_{k=1}^K \mathbf{1}(y^{(i)}=k) \log p_k^{(i)}
$$</p>
<p>负号取负对数似然：
$$
J(\theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \log p_k^{(i)}
$$</p>
<p>这就是<strong>交叉熵损失（Cross-Entropy Loss）</strong>。</p>
<hr />
<h5 id="2_11">2️⃣ 损失函数解释：<a class="headerlink" href="#2_11" title="Permanent link">&para;</a></h5>
<ul>
<li><span class="arithmatex">\(y_k^{(i)}\)</span> 是 one-hot 编码（例如类别 3 → <span class="arithmatex">\([0, 0, 1, 0, 0]\)</span>）；</li>
<li>只在真实类别对应项上起作用。</li>
</ul>
<p><strong>示例：</strong>
假设 3 类问题中真实类别为 2，预测概率为：
$$
\hat{p} = [0.2, 0.7, 0.1]
$$
则损失为：
$$
L = -\log(0.7) = 0.357
$$</p>
<h4 id="_79">📈 四、梯度推导（重要！）<a class="headerlink" href="#_79" title="Permanent link">&para;</a></h4>
<p>Softmax 的输出：
$$
p_k = \frac{e^{z_k}}{\sum_{j} e^{z_j}}
$$</p>
<p>对 <span class="arithmatex">\(z_k\)</span> 求导：</p>
<div class="arithmatex">\[
\frac{\partial p_k}{\partial z_j} =
\begin{cases}
p_k(1 - p_k), &amp; \text{if } j=k \\
* p_k p_j, &amp; \text{if } j \ne k
  \end{cases}
\]</div>
<p>对损失函数 <span class="arithmatex">\(J\)</span>：
$$
\frac{\partial J}{\partial z_k} = p_k - y_k
$$</p>
<blockquote>
<p>直观理解：模型预测 <span class="arithmatex">\(p_k\)</span> 与真实标签 <span class="arithmatex">\(y_k\)</span> 的差值就是梯度。</p>
</blockquote>
<p>这也是多分类 Softmax 回归训练的核心更新规则。</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span>

<span class="c1"># 加载数据集</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 逻辑回归多分类</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h4 id="_80">常见算法（用于多分类）<a class="headerlink" href="#_80" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>算法</th>
<th>思路</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Softmax 回归（多项逻辑回归）</strong></td>
<td>线性模型 + Softmax</td>
<td>简单可解释</td>
<td>线性可分限制</td>
</tr>
<tr>
<td><strong>SVM（多分类扩展）</strong></td>
<td>一对一 / 一对多</td>
<td>较高准确率</td>
<td>训练复杂</td>
</tr>
<tr>
<td><strong>决策树 / 随机森林</strong></td>
<td>基于划分规则</td>
<td>非线性能力强</td>
<td>可解释性有限</td>
</tr>
<tr>
<td><strong>神经网络</strong></td>
<td>多层非线性映射</td>
<td>高表达能力</td>
<td>需大量数据</td>
</tr>
<tr>
<td><strong>kNN</strong></td>
<td>基于邻居投票</td>
<td>无需训练</td>
<td>预测耗时</td>
</tr>
</tbody>
</table>
<h3 id="54">5.4 多标签<a class="headerlink" href="#54" title="Permanent link">&para;</a></h3>
<p>在<strong>多标签分类</strong>问题中，一个样本可以同时属于<strong>多个类别</strong>。
与之相对的，<strong>多分类（Multi-Class）</strong>中每个样本只属于一个类别。</p>
<ul>
<li>
<p><strong>二分类</strong>：每个样本只属于两个类别之一
  例如：垃圾邮件识别（spam / not spam）</p>
</li>
<li>
<p><strong>多分类</strong>：每个样本属于多个类别中的一个
  例如：手写数字识别（0~9）</p>
</li>
<li>
<p><strong>多标签分类</strong>：每个样本可以同时属于多个类别
  例如：
  一张图片可以同时包含：</p>
<ul>
<li>“猫” ✅</li>
<li>“狗” ✅</li>
<li>“车” ❌ </li>
<li>因此标签为 <code>[1, 1, 0]</code></li>
</ul>
</li>
</ul>
<h4 id="_81">二、数学定义<a class="headerlink" href="#_81" title="Permanent link">&para;</a></h4>
<p>假设：</p>
<ul>
<li>样本特征为 <span class="arithmatex">\(x_i \in \mathbb{R}^d\)</span></li>
<li>标签集合为 <span class="arithmatex">\(Y = {1, 2, \dots, K}\)</span></li>
<li>每个样本的真实标签为 <span class="arithmatex">\(y_i \subseteq Y\)</span></li>
</ul>
<p>对于多标签问题，我们通常将标签表示为一个长度为 <span class="arithmatex">\(K\)</span> 的<strong>二进制向量</strong>：</p>
<div class="arithmatex">\[
\mathbf{y}*i = [y*{i1}, y_{i2}, \dots, y_{iK}]
\]</div>
<p>其中：</p>
<div class="arithmatex">\[
y_{ik} =
\begin{cases}
1, &amp; \text{如果样本 } i \text{ 属于类别 } k \
0, &amp; \text{否则}
\end{cases}
\]</div>
<p>模型输出为：</p>
<div class="arithmatex">\[
\hat{\mathbf{y}}*i = [\hat{y}*{i1}, \hat{y}*{i2}, \dots, \hat{y}*{iK}]
\]</div>
<p>其中 <span class="arithmatex">\(\hat{y}_{ik} \in [0,1]\)</span> 表示属于标签 <span class="arithmatex">\(k\)</span> 的概率。</p>
<h4 id="_82">三、常见建模方法<a class="headerlink" href="#_82" title="Permanent link">&para;</a></h4>
<h5 id="1-binary-relevancebr">1. <strong>Binary Relevance（BR）</strong><a class="headerlink" href="#1-binary-relevancebr" title="Permanent link">&para;</a></h5>
<p>最常见、最简单的方式：
把多标签问题拆成 <strong>K 个独立的二分类问题</strong>。</p>
<p>每个标签独立建模：
$$
P(y_k=1|x) = \sigma(w_k^\top x)
$$</p>
<p>其中 <span class="arithmatex">\(\sigma(z)=\frac{1}{1+e^{-z}}\)</span> 是 sigmoid 函数。</p>
<p>优点：</p>
<ul>
<li>简单、易实现</li>
<li>可以并行训练</li>
</ul>
<p>缺点：</p>
<ul>
<li>忽略了标签之间的相关性（例如“猫”和“动物”强相关）</li>
</ul>
<hr />
<h5 id="2-classifier-chaincc">2. <strong>Classifier Chain（CC）</strong><a class="headerlink" href="#2-classifier-chaincc" title="Permanent link">&para;</a></h5>
<p>通过将前面标签的预测作为后续模型的输入，捕捉标签间依赖。</p>
<p>假设标签顺序为 <span class="arithmatex">\((y_1, y_2, ..., y_K)\)</span>：</p>
<div class="arithmatex">\[
P(y_1, ..., y_K | x) = \prod_{k=1}^K P(y_k | x, y_1, ..., y_{k-1})
\]</div>
<p>即第 <span class="arithmatex">\(k\)</span> 个分类器不仅使用特征 <span class="arithmatex">\(x\)</span>，还使用之前的标签预测。</p>
<hr />
<h5 id="3-label-powersetlp">3. <strong>Label Powerset（LP）</strong><a class="headerlink" href="#3-label-powersetlp" title="Permanent link">&para;</a></h5>
<p>把<strong>所有标签组合</strong>视为一个新的“复合类”。</p>
<p>例如：</p>
<ul>
<li>原始标签空间：<code>{A, B, C}</code></li>
<li>可能的组合：<code>{}</code>, <code>{A}</code>, <code>{B}</code>, <code>{C}</code>, <code>{A,B}</code>, <code>{A,C}</code>, <code>{B,C}</code>, <code>{A,B,C}</code></li>
</ul>
<p>→ 变成一个多分类问题。</p>
<p>优点：考虑了标签相关性
缺点：当标签数量多时，组合爆炸。</p>
<h4 id="loss-function_1">四、损失函数（Loss Function）<a class="headerlink" href="#loss-function_1" title="Permanent link">&para;</a></h4>
<h5 id="1-binary-cross-entropysigmoid-bce">1. <strong>Binary Cross-Entropy（Sigmoid + BCE）</strong><a class="headerlink" href="#1-binary-cross-entropysigmoid-bce" title="Permanent link">&para;</a></h5>
<p>最常用损失函数。对每个标签独立使用 sigmoid，再计算交叉熵。</p>
<p>对于单个样本：
$$
L_i = -\sum_{k=1}^{K} \Big[ y_{ik} \log(\hat{y}<em>{ik}) + (1 - y</em>{ik}) \log(1 - \hat{y}_{ik}) \Big]
$$</p>
<p>整体损失为：
$$
L = \frac{1}{N} \sum_{i=1}^{N} L_i
$$</p>
<p>Python 示例（PyTorch）：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>  <span class="c1"># 内部自带 sigmoid</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div>
<hr />
<h5 id="2-focal-loss">2. <strong>Focal Loss（处理类别不平衡）</strong><a class="headerlink" href="#2-focal-loss" title="Permanent link">&para;</a></h5>
<p>当正负样本极度不平衡时使用：
$$
L = - \sum_k \alpha (1 - \hat{y}_k)^\gamma y_k \log(\hat{y}_k)
$$</p>
<ul>
<li><span class="arithmatex">\(\alpha\)</span>：平衡正负样本</li>
<li><span class="arithmatex">\(\gamma\)</span>：聚焦难分类样本</li>
</ul>
<h4 id="evaluation-metrics">五、评估指标（Evaluation Metrics）<a class="headerlink" href="#evaluation-metrics" title="Permanent link">&para;</a></h4>
<p>对于多标签任务，我们需要按<strong>标签维度</strong>或<strong>样本维度</strong>计算指标。</p>
<h5 id="1-hamming-loss">1. <strong>Hamming Loss</strong><a class="headerlink" href="#1-hamming-loss" title="Permanent link">&para;</a></h5>
<p>衡量错误预测的比例：
$$
\text{Hamming Loss} = \frac{1}{N \times K} \sum_{i=1}^N \sum_{k=1}^K I[y_{ik} \neq \hat{y}_{ik}]
$$</p>
<p>越小越好。</p>
<hr />
<h5 id="2-subset-accuracyexact-match-ratio">2. <strong>Subset Accuracy（Exact Match Ratio）</strong><a class="headerlink" href="#2-subset-accuracyexact-match-ratio" title="Permanent link">&para;</a></h5>
<p>所有标签完全匹配才算正确：
$$
\text{Subset Accuracy} = \frac{1}{N} \sum_{i=1}^N I[\mathbf{y}_i = \hat{\mathbf{y}}_i]
$$</p>
<p>过于严格，常不推荐单独使用。</p>
<hr />
<h5 id="3-precision-recall-f1-scoremicro-macro">3. <strong>Precision / Recall / F1-score（Micro / Macro）</strong><a class="headerlink" href="#3-precision-recall-f1-scoremicro-macro" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Micro-F1</strong>：全局统计 TP, FP, FN
  更关注<strong>频繁标签</strong>的整体表现。</li>
<li><strong>Macro-F1</strong>：对每个标签单独计算 F1，再平均。
  更关注<strong>稀有标签</strong>。</li>
</ul>
<p>Python：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">f1_score</span>
<span class="n">f1_micro</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
<span class="n">f1_macro</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_multilabel_classification</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.multioutput</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultiOutputClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">f1_score</span>

<span class="c1"># 1. 构造数据</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_multilabel_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 2. 划分数据集</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 3. 模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MultiOutputClassifier</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 4. 预测与评估</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Micro-F1:&quot;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Macro-F1:&quot;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">))</span>
</code></pre></div>
<h2 id="_83">六、回归<a class="headerlink" href="#_83" title="Permanent link">&para;</a></h2>
<h3 id="61">6.1 损失函数<a class="headerlink" href="#61" title="Permanent link">&para;</a></h3>
<h4 id="1_14">1. 总体背景与符号约定<a class="headerlink" href="#1_14" title="Permanent link">&para;</a></h4>
<ul>
<li>训练样本 <span class="arithmatex">\({(x^{(i)}, y^{(i)})}_{i=1}^{m}\)</span>。</li>
<li>模型预测为 <span class="arithmatex">\(\hat{y}^{(i)} = f_\theta(x^{(i)})\)</span>，参数为 <span class="arithmatex">\(\theta\)</span>。</li>
<li>单样本损失记为 <span class="arithmatex">\(L\big(y^{(i)},\hat{y}^{(i)}\big)\)</span>，总体（平均）损失为
  $$
  J(\theta)=\frac{1}{m}\sum_{i=1}^m L\big(y^{(i)},\hat{y}^{(i)}\big).
  $$</li>
</ul>
<p>我们关心的常见回归损失：MSE、MAE、Huber、Log-cosh、Quantile Loss（分位数损失）、Poisson/NLL 等。</p>
<h4 id="2-mse-mean-squared-error">2. 均方误差（MSE / Mean Squared Error）<a class="headerlink" href="#2-mse-mean-squared-error" title="Permanent link">&para;</a></h4>
<h5 id="_84">定义<a class="headerlink" href="#_84" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
L_{\text{MSE}}(y,\hat{y}) = (y - \hat{y})^2.
\]</div>
<p>平均（样本）损失：</p>
<div class="arithmatex">\[
J_{\text{MSE}}(\theta)=\frac{1}{m}\sum_{i=1}^m (y^{(i)} - \hat{y}^{(i)})^2.
\]</div>
<h5 id="_85">含义<a class="headerlink" href="#_85" title="Permanent link">&para;</a></h5>
<p>对预测误差平方惩罚，较大误差被放大（对离群点敏感）。MSE 等价于假设误差服从均值为 0、方差为常数的高斯噪声，使用极大似然可导出 MSE。</p>
<h5 id="haty">梯度（对单样本的预测 <span class="arithmatex">\(\hat{y}\)</span>）<a class="headerlink" href="#haty" title="Permanent link">&para;</a></h5>
<p>对 <span class="arithmatex">\(\hat{y}\)</span>：
$$
\frac{\partial L_{\text{MSE}}}{\partial \hat{y}} = -2 (y - \hat{y}).
$$
若对参数 <span class="arithmatex">\(\theta\)</span>，链式法则：
$$
\nabla_\theta L_{\text{MSE}} = -2 (y - \hat{y}) \nabla_\theta \hat{y}.
$$</p>
<h5 id="_86">数值例子（逐位算）<a class="headerlink" href="#_86" title="Permanent link">&para;</a></h5>
<p>取单样本 <span class="arithmatex">\(y=3.0,\ \hat{y}=2.2\)</span>：</p>
<ul>
<li>误差 <span class="arithmatex">\(e = y - \hat{y} = 3.0 - 2.2 = 0.8\)</span>。</li>
<li>MSE 损失 <span class="arithmatex">\(L = e^2 = 0.8^2 = 0.64\)</span>。</li>
<li>梯度 w.r.t. <span class="arithmatex">\(\hat{y}\)</span>：<span class="arithmatex">\(\partial L/\partial \hat{y} = -2 \times 0.8 = -1.6\)</span>。</li>
</ul>
<p>（注意：数值计算都逐位计算，确保准确。）</p>
<h5 id="_87">优缺点<a class="headerlink" href="#_87" title="Permanent link">&para;</a></h5>
<ul>
<li>优点：解析简单、可导、常用。</li>
<li>缺点：对大误差（离群点）敏感。</li>
</ul>
<hr />
<h4 id="3-mae-mean-absolute-error">3. 平均绝对误差（MAE / Mean Absolute Error）<a class="headerlink" href="#3-mae-mean-absolute-error" title="Permanent link">&para;</a></h4>
<h5 id="_88">定义<a class="headerlink" href="#_88" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
L_{\text{MAE}}(y,\hat{y}) = |y - \hat{y}|.
\]</div>
<p>平均损失：</p>
<div class="arithmatex">\[
J_{\text{MAE}}(\theta)=\frac{1}{m}\sum_{i=1}^m |y^{(i)} - \hat{y}^{(i)}|.
\]</div>
<h5 id="_89">含义<a class="headerlink" href="#_89" title="Permanent link">&para;</a></h5>
<p>对误差按绝对值惩罚，鲁棒性比 MSE 更强（对离群点不那么敏感）。等价于假设误差服从拉普拉斯分布。</p>
<h5 id="_90">梯度（对单样本）<a class="headerlink" href="#_90" title="Permanent link">&para;</a></h5>
<p>对 <span class="arithmatex">\(\hat{y}\)</span> 的次导/符号函数（<span class="arithmatex">\(y\neq\hat{y}\)</span>）：
$$
\frac{\partial L_{\text{MAE}}}{\partial \hat{y}} =
\begin{cases}
-1, &amp; \hat{y} &lt; y,\
+1, &amp; \hat{y} &gt; y.
\end{cases}
$$
在 <span class="arithmatex">\(\hat{y}=y\)</span> 处不可导，用次梯度 <span class="arithmatex">\(\in[-1,1]\)</span>。</p>
<h5 id="_91">数值例子<a class="headerlink" href="#_91" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(y=3.0,\ \hat{y}=2.2\)</span>：</p>
<ul>
<li>误差 <span class="arithmatex">\(e=0.8\)</span>；</li>
<li>MAE <span class="arithmatex">\(=|0.8|=0.8\)</span>；</li>
<li>梯度对 <span class="arithmatex">\(\hat{y}\)</span>：因为 <span class="arithmatex">\(\hat{y}&lt;y\)</span>，<span class="arithmatex">\(\partial L/\partial \hat{y} = -1\)</span>。</li>
</ul>
<h5 id="_92">优缺点<a class="headerlink" href="#_92" title="Permanent link">&para;</a></h5>
<ul>
<li>优点：对离群点更鲁棒。</li>
<li>缺点：不可导点导致用梯度法时收敛行为不如 MSE 平滑；在很多框架中用次梯度处理即可。</li>
</ul>
<hr />
<h4 id="4-huber-mae-mse">4. Huber 损失（平滑的 MAE / MSE 混合）<a class="headerlink" href="#4-huber-mae-mse" title="Permanent link">&para;</a></h4>
<h5 id="delta0">定义（带阈值 <span class="arithmatex">\(\delta&gt;0\)</span>）<a class="headerlink" href="#delta0" title="Permanent link">&para;</a></h5>
<p>单样本：
$$
L_{\text{Huber}}(y,\hat{y}) =
\begin{cases}
\frac{1}{2}(y-\hat{y})^2, &amp; \text{if } |y-\hat{y}|\le \delta,[6pt]
\delta\big(|y-\hat{y}| - \tfrac{1}{2}\delta\big), &amp; \text{if } |y-\hat{y}|&gt;\delta.
\end{cases}
$$</p>
<h5 id="_93">含义<a class="headerlink" href="#_93" title="Permanent link">&para;</a></h5>
<p>在小误差区域使用二次（MSE），在大误差（离群点）区域使用线性（MAE）惩罚，因此兼顾平滑性与鲁棒性。<span class="arithmatex">\(\delta\)</span> 控制从二次到线性的切换点。</p>
<h5 id="haty_1">梯度（对 <span class="arithmatex">\(\hat{y}\)</span>）<a class="headerlink" href="#haty_1" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\frac{\partial L_{\text{Huber}}}{\partial \hat{y}} =
\begin{cases}
-(y-\hat{y}), &amp; |y-\hat{y}|\le \delta,[4pt]
-\delta,\mathrm{sign}(y-\hat{y}), &amp; |y-\hat{y}|&gt;\delta.
\end{cases}
\]</div>
<h5 id="delta1">数值例子（取 <span class="arithmatex">\(\delta=1\)</span>）<a class="headerlink" href="#delta1" title="Permanent link">&para;</a></h5>
<ul>
<li>
<p>若 <span class="arithmatex">\(y=3.0,\ \hat{y}=2.2\)</span>，误差 <span class="arithmatex">\(0.8\le1\)</span>：</p>
</li>
<li>
<p>损失 <span class="arithmatex">\(=\tfrac{1}{2}\cdot 0.8^2 = 0.5 \times 0.64 = 0.32\)</span>；</p>
</li>
<li>梯度 <span class="arithmatex">\(=-(3.0-2.2) = -0.8\)</span>。</li>
<li>
<p>若 <span class="arithmatex">\(y=10,\ \hat{y}=2\)</span>，误差 <span class="arithmatex">\(8&gt;1\)</span>：</p>
</li>
<li>
<p>损失 <span class="arithmatex">\(=1\cdot(8 - 0.5\cdot1)=8 - 0.5 = 7.5\)</span>；</p>
</li>
<li>梯度 <span class="arithmatex">\(=-1\cdot \mathrm{sign}(8) = -1\)</span>（即对 <span class="arithmatex">\(\hat{y}\)</span> 为 -1）。</li>
</ul>
<h5 id="_94">优缺点<a class="headerlink" href="#_94" title="Permanent link">&para;</a></h5>
<ul>
<li>优点：平滑且鲁棒，常用在回归和 RL（奖励）中。</li>
<li>缺点：需选择超参 <span class="arithmatex">\(\delta\)</span>。</li>
</ul>
<hr />
<h4 id="5-log-cosh">5. Log-cosh 损失<a class="headerlink" href="#5-log-cosh" title="Permanent link">&para;</a></h4>
<h5 id="_95">定义<a class="headerlink" href="#_95" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
L_{\text{log-cosh}}(y,\hat{y}) = \log\big(\cosh(y - \hat{y})\big).
\]</div>
<h5 id="_96">含义<a class="headerlink" href="#_96" title="Permanent link">&para;</a></h5>
<p>对小误差近似 <span class="arithmatex">\(\tfrac{1}{2}(y-\hat{y})^2\)</span>，对大误差近似 <span class="arithmatex">\(|y-\hat{y}| - \log 2\)</span>，因此行为介于 MSE 与 MAE 之间且平滑可导。</p>
<h5 id="_97">梯度<a class="headerlink" href="#_97" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\frac{\partial L_{\text{log-cosh}}}{\partial \hat{y}} = -\tanh(y - \hat{y}).
\]</div>
<h5 id="_98">数值例子<a class="headerlink" href="#_98" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(y=3.0,\ \hat{y}=2.2\)</span>，误差 <span class="arithmatex">\(0.8\)</span>：</p>
<ul>
<li><span class="arithmatex">\(\cosh(0.8) = \tfrac{e^{0.8}+e^{-0.8}}{2}\approx \tfrac{2.22554+0.44933}{2}=1.337435\)</span>（逐位计算：<span class="arithmatex">\(e^{0.8}\approx2.2255409, e^{-0.8}\approx0.449329\)</span>，和约 <span class="arithmatex">\(2.67487\)</span>, 除以 2 得 <span class="arithmatex">\(1.337435\)</span>）。</li>
<li><span class="arithmatex">\(\log\cosh(0.8)\approx \log 1.337435 \approx 0.290\)</span>（近似）。</li>
<li>梯度 <span class="arithmatex">\(= -\tanh(0.8)\approx -0.664\)</span>（<span class="arithmatex">\(\tanh(0.8)\approx0.664\)</span>）。</li>
</ul>
<p>（数值用近似值表示，保留示例说明目的。）</p>
<hr />
<h4 id="6-quantile-loss">6. Quantile Loss（分位数损失） — 用于预测分位数（如上、下界）<a class="headerlink" href="#6-quantile-loss" title="Permanent link">&para;</a></h4>
<h5 id="tauin01">定义（分位数 <span class="arithmatex">\(\tau\in(0,1)\)</span>）<a class="headerlink" href="#tauin01" title="Permanent link">&para;</a></h5>
<p>单样本：</p>
<div class="arithmatex">\[
L_{\tau}(y,\hat{y}) =
\begin{cases}
\tau (y - \hat{y}), &amp; \text{if } y \ge \hat{y},\\[4pt]
(1-\tau)(\hat{y} - y), &amp; \text{if } y &lt; \hat{y}.
\end{cases}
\]</div>
<h5 id="_99">含义<a class="headerlink" href="#_99" title="Permanent link">&para;</a></h5>
<p>用于学习条件分位数（例如中位数 <span class="arithmatex">\(\tau=0.5\)</span> 对应 MAE 的中位数解）。在预测不对称风险（对过高或过低误差惩罚不同）时非常有用。</p>
<h5 id="tau09">数值例子（<span class="arithmatex">\(\tau=0.9\)</span>）<a class="headerlink" href="#tau09" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(y=10,\ \hat{y}=8\)</span>：<span class="arithmatex">\(y\ge\hat{y}\)</span>，损失 <span class="arithmatex">\(=0.9*(10-8)=0.9*2=1.8\)</span>。</p>
<p><span class="arithmatex">\(y=6,\ \hat{y}=8\)</span>：<span class="arithmatex">\(y&lt;\hat{y}\)</span>，损失 <span class="arithmatex">\(=(1-0.9)*(8-6)=0.1*2=0.2\)</span>。</p>
<p>说明高估误差（<span class="arithmatex">\(\hat{y}&gt;y\)</span>）被轻罚，低估误差（<span class="arithmatex">\(\hat{y}&lt;y\)</span>）被重罚（或反之，取决于 <span class="arithmatex">\(\tau\)</span>）。</p>
<hr />
<h4 id="7-poisson-gamma-nll">7. Poisson / Gamma / NLL（适用于计数或特定噪声模型）<a class="headerlink" href="#7-poisson-gamma-nll" title="Permanent link">&para;</a></h4>
<h5 id="poisson-nll">Poisson NLL（计数型输出）<a class="headerlink" href="#poisson-nll" title="Permanent link">&para;</a></h5>
<p>若假设 <span class="arithmatex">\(y\sim\text{Poisson}(\lambda=\hat{y})\)</span>，负对数似然（单样本）：
$$
L_{\text{Poisson}}(y,\hat{y}) = \hat{y} - y\log \hat{y} + \log(y!).
$$
（常省去与 <span class="arithmatex">\(\hat{y}\)</span> 无关的 <span class="arithmatex">\(\log(y!)\)</span> 项用于优化）</p>
<p>适用场景：计数数据（事件次数）。</p>
<hr />
<h4 id="8_2">8. 损失选择的标准与实践建议<a class="headerlink" href="#8_2" title="Permanent link">&para;</a></h4>
<ul>
<li>若误差呈<strong>高斯噪声</strong>且你希望对大误差更敏感：选择 <strong>MSE</strong>。</li>
<li>若数据含<strong>离群点</strong>或希望鲁棒：选择 <strong>MAE</strong> 或 <strong>Huber（首选）</strong> 或 <strong>Log-cosh</strong>。</li>
<li>若需要预测<strong>分位数</strong>或不对称损失：用 <strong>Quantile Loss</strong>。</li>
<li>若输出是计数型（非负整数）：用 <strong>Poisson NLL</strong>。</li>
</ul>
<p>同时建议：</p>
<ul>
<li>在神经网络中，若使用 MSE 且输出尺度大，配合合适的学习率和标准化（target scaling）；</li>
<li>若使用 MAE/Hinge 类型不可导点的损失，可用优化器（Adam）配合 subgradient 或使用 Huber 代替。</li>
</ul>
<hr />
<h4 id="9">9. 正则化与损失结合<a class="headerlink" href="#9" title="Permanent link">&para;</a></h4>
<p>通常在最小化损失时会加正则化项（防止过拟合）：
$$
J(\theta) = \frac{1}{m}\sum_{i=1}^m L(y^{(i)},\hat{y}^{(i)}) + \lambda R(\theta),
$$
常见 <span class="arithmatex">\(R(\theta)=|\theta|_2^2\)</span>（L2）或 <span class="arithmatex">\(|\theta|_1\)</span>（L1）。</p>
<hr />
<h4 id="10">10. 评估指标（与损失不同，常用于模型评估）<a class="headerlink" href="#10" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>MSE</strong>：<span class="arithmatex">\(\text{MSE}=\frac{1}{m}\sum (y-\hat{y})^2\)</span>。</li>
<li><strong>RMSE</strong>（均方根误差）：<span class="arithmatex">\(\text{RMSE}=\sqrt{\text{MSE}}\)</span>，与原始单位一致。</li>
<li><strong>MAE</strong>：<span class="arithmatex">\(\frac{1}{m}\sum |y-\hat{y}|\)</span>。</li>
<li><strong>R²（决定系数）</strong>：
  $$
  R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2},
  $$
  其中 <span class="arithmatex">\(\bar{y}=\frac{1}{m}\sum_i y_i\)</span>。<span class="arithmatex">\(R^2\)</span> 越接近 1 越好；负值表示模型性能比常数预测差。</li>
</ul>
<hr />
<h4 id="11-python-numpy-pytorch">11. Python 实现示例（NumPy / PyTorch）<a class="headerlink" href="#11-python-numpy-pytorch" title="Permanent link">&para;</a></h4>
<h5 id="numpy-mse-mae-huber">NumPy：计算 MSE / MAE / Huber<a class="headerlink" href="#numpy-mse-mae-huber" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">])</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span>
<span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># MSE</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (0.64 + 0.25 + 1.0)/3 = 1.89/3 = 0.63</span>
<span class="c1"># 逐位计算: 0.8^2=0.64, (-0.5)^2=0.25, 1.0^2=1.0 -&gt; sum 1.89</span>

<span class="c1"># MAE</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">))</span>  <span class="c1"># (0.8 + 0.5 + 1.0)/3 = 2.3/3 ≈ 0.7666667</span>

<span class="c1"># Huber (delta=1.0)</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="n">huber</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">diff</span> <span class="o">&lt;=</span> <span class="n">delta</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">delta</span><span class="p">))</span>
<span class="n">huber_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">huber</span><span class="p">)</span>
<span class="c1"># 逐位: diff = [0.8,0.5,1.0] -&gt; huber = [0.5*0.64=0.32, 0.5*0.25=0.125, 0.5*1.0=0.5] -&gt; sum=0.945 -&gt; mean=0.315</span>
</code></pre></div>
<blockquote>
<p>注：上面 MSE 的逐位和计算已明确列出，确保数值准确。</p>
</blockquote>
<h5 id="pytorch">PyTorch：常用损失<a class="headerlink" href="#pytorch" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">])</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span>

<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">mae_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="n">huber_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">()</span>  <span class="c1"># PyTorch 的 SmoothL1 相当于 Huber（默认 beta=1.0）</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>   <span class="c1"># ≈ 0.63</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mae_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>   <span class="c1"># ≈ 0.7667</span>
<span class="nb">print</span><span class="p">(</span><span class="n">huber_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="c1"># ≈ 0.315</span>
</code></pre></div>
<hr />
<h4 id="12_1">12. 数值稳定性与实践细节<a class="headerlink" href="#12_1" title="Permanent link">&para;</a></h4>
<ul>
<li>对于 MSE，若目标值范围很大，梯度可能很大，需调小学习率或缩放 target。</li>
<li>对 MAE/Hinge 类型的不可导点，用次梯度或平滑近似（Huber / Log-cosh）。</li>
<li>对于 Poisson / NLL，预测值 <span class="arithmatex">\(\hat{y}\)</span> 必须是正（可用 softplus 激活 <span class="arithmatex">\(\log(1+e^z)\)</span>），避免 <span class="arithmatex">\(\log 0\)</span>。</li>
<li>在深度网络训练中，使用 <code>BCEWithLogitsLoss</code>、<code>CrossEntropyLoss</code> 等内置稳定实现，避免手写数值不稳。</li>
</ul>
<h3 id="62">6.2 评估指标<a class="headerlink" href="#62" title="Permanent link">&para;</a></h3>
<h4 id="_100">🌟 一、常见的回归评估指标概览<a class="headerlink" href="#_100" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>指标名称</th>
<th>英文名称</th>
<th>数学公式</th>
<th>含义</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>平均绝对误差</td>
<td>MAE (Mean Absolute Error)</td>
<td>$\displaystyle MAE = \frac{1}{n} \sum_{i=1}^{n}</td>
<td>y_i - \hat{y}_i</td>
<td>$</td>
<td>平均绝对偏差，衡量平均误差大小</td>
</tr>
<tr>
<td>均方误差</td>
<td>MSE (Mean Squared Error)</td>
<td><span class="arithmatex">\(\displaystyle MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span></td>
<td>强调大误差的惩罚</td>
<td></td>
<td></td>
</tr>
<tr>
<td>均方根误差</td>
<td>RMSE (Root Mean Squared Error)</td>
<td><span class="arithmatex">\(\displaystyle RMSE = \sqrt{MSE}\)</span></td>
<td>与原目标单位一致，更直观</td>
<td></td>
<td></td>
</tr>
<tr>
<td>平均绝对百分比误差</td>
<td>MAPE (Mean Absolute Percentage Error)</td>
<td>$\displaystyle MAPE = \frac{100%}{n} \sum_{i=1}^{n} \left</td>
<td>\frac{y_i - \hat{y}_i}{y_i} \right</td>
<td>$</td>
<td>衡量相对误差（适用于比例问题）</td>
</tr>
<tr>
<td>判定系数</td>
<td><span class="arithmatex">\(R^2\)</span> (R-squared)</td>
<td><span class="arithmatex">\(\displaystyle R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}\)</span></td>
<td>衡量模型对方差的解释能力（拟合优度）</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<hr />
<h4 id="_101">🧮 二、各指标数学原理与解释<a class="headerlink" href="#_101" title="Permanent link">&para;</a></h4>
<hr />
<h5 id="1-mae">1️⃣ 平均绝对误差（MAE）<a class="headerlink" href="#1-mae" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]</div>
<ul>
<li>含义：所有预测误差的<strong>绝对值平均值</strong>。</li>
<li>
<p>特点：</p>
<ul>
<li>对异常值（outlier）<strong>不敏感</strong>。</li>
<li>物理意义明确：平均预测误差是多少个单位。</li>
</ul>
</li>
<li>
<p>举例：</p>
<ul>
<li>若真实值为 <span class="arithmatex">\([3, 5, 7]\)</span>，预测值为 <span class="arithmatex">\([2, 6, 8]\)</span>：
$$
MAE = \frac{|3-2| + |5-6| + |7-8|}{3} = 1
$$</li>
</ul>
</li>
</ul>
<hr />
<h5 id="2-mse">2️⃣ 均方误差（MSE）<a class="headerlink" href="#2-mse" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div>
<ul>
<li>含义：误差平方的平均。</li>
<li>
<p>特点：</p>
<ul>
<li><strong>平方项</strong>会放大大误差的影响（对异常值敏感）。</li>
<li>优点是<strong>光滑可导</strong>，便于优化。</li>
</ul>
</li>
<li>
<p>举例：
  $$
  MSE = \frac{(3-2)^2 + (5-6)^2 + (7-8)^2}{3} = 1
  $$</p>
</li>
</ul>
<hr />
<h5 id="3-rmse">3️⃣ 均方根误差（RMSE）<a class="headerlink" href="#3-rmse" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
RMSE = \sqrt{MSE}
\]</div>
<ul>
<li>含义：MSE 的平方根，使单位与原数据一致。</li>
<li>
<p>特点：</p>
<ul>
<li>数值更直观（单位与 <span class="arithmatex">\(y\)</span> 相同）。</li>
<li>同样对异常值敏感。</li>
</ul>
</li>
<li>
<p>举例：
  $$
  RMSE = \sqrt{1} = 1
  $$</p>
</li>
</ul>
<hr />
<h5 id="4-mape">4️⃣ 平均绝对百分比误差（MAPE）<a class="headerlink" href="#4-mape" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
MAPE = \frac{100%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
\]</div>
<ul>
<li>含义：每个预测值相对真实值的<strong>百分比误差平均</strong>。</li>
<li>
<p>特点：</p>
<ul>
<li>适合 <strong>量纲不同</strong> 的数据比较。</li>
<li>但当 <span class="arithmatex">\(y_i = 0\)</span> 时无定义，需注意。</li>
<li>
<p>举例：</p>
</li>
<li>
<p><span class="arithmatex">\(y = [100, 200, 300]\)</span>, <span class="arithmatex">\(\hat{y} = [110, 190, 310]\)</span>：
$$
MAPE = \frac{1}{3}(0.1 + 0.05 + 0.0333) \times 100% = 6.1%
$$</p>
</li>
</ul>
</li>
</ul>
<hr />
<h5 id="5-r2">5️⃣ 判定系数（<span class="arithmatex">\(R^2\)</span>）<a class="headerlink" href="#5-r2" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(SS_{res}\)</span>：残差平方和（residual sum of squares）</li>
<li><span class="arithmatex">\(SS_{tot}\)</span>：总平方和（total sum of squares）</li>
<li><span class="arithmatex">\(\bar{y}\)</span>：样本真实值的平均数</li>
</ul>
<p><strong>含义：</strong></p>
<ul>
<li>衡量模型预测解释了目标变量方差的比例。</li>
<li><span class="arithmatex">\(R^2\)</span> 的范围通常在 <span class="arithmatex">\([0, 1]\)</span>（但在某些情况下可小于 0）。</li>
<li>值越大表示模型拟合效果越好。</li>
</ul>
<p>举例：</p>
<ul>
<li>若模型预测值完全正确，<span class="arithmatex">\(\sum (y_i - \hat{y}_i)^2 = 0\)</span>，则 <span class="arithmatex">\(R^2 = 1\)</span>；</li>
<li>若模型与均值预测无差别，则 <span class="arithmatex">\(R^2 = 0\)</span>。</li>
</ul>
<hr />
<h4 id="_102">💡 三、各指标对比总结<a class="headerlink" href="#_102" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>指标</th>
<th>是否对异常值敏感</th>
<th>是否可导</th>
<th>是否有物理意义</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>MAE</td>
<td>否</td>
<td>否（不可导点）</td>
<td>有</td>
<td>一般误差分析</td>
</tr>
<tr>
<td>MSE</td>
<td>是</td>
<td>是</td>
<td>有</td>
<td>模型训练优化</td>
</tr>
<tr>
<td>RMSE</td>
<td>是</td>
<td>是</td>
<td>强</td>
<td>模型性能解释</td>
</tr>
<tr>
<td>MAPE</td>
<td>是</td>
<td>否</td>
<td>强</td>
<td>比例型数据</td>
</tr>
<tr>
<td><span class="arithmatex">\(R^2\)</span></td>
<td>否</td>
<td>否</td>
<td>相对解释能力</td>
<td>模型整体拟合质量</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="sklearn">🧠 四、代码实战（以 <code>sklearn</code> 为例）<a class="headerlink" href="#sklearn" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># 假设真实值与预测值</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>

<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MAE = </span><span class="si">{</span><span class="n">mae</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RMSE = </span><span class="si">{</span><span class="n">rmse</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R² = </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>输出结果：</p>
<div class="highlight"><pre><span></span><code>MAE = 1.000
MSE = 1.000
RMSE = 1.000
R² = 0.875
</code></pre></div>
<hr />
<h4 id="_103">📘 五、总结<a class="headerlink" href="#_103" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>MAE</strong>：平均误差（稳健、直观）</li>
<li><strong>MSE / RMSE</strong>：平方惩罚（偏重大误差，训练常用）</li>
<li><strong>MAPE</strong>：百分比误差（适合比例型问题）</li>
<li><strong><span class="arithmatex">\(R^2\)</span></strong>：解释方差比例（拟合优度指标）</li>
</ul>
<h2 id="_104">七、特征工程<a class="headerlink" href="#_104" title="Permanent link">&para;</a></h2>
<h3 id="71">7.1 特征选择<a class="headerlink" href="#71" title="Permanent link">&para;</a></h3>
<h4 id="_105">一、什么是特征选择<a class="headerlink" href="#_105" title="Permanent link">&para;</a></h4>
<p>特征选择是指在 <strong>原始特征集合中选择对模型最有用的子集</strong>，删除冗余或无关特征，从而提高模型性能、减少过拟合、降低计算复杂度和提高模型可解释性。</p>
<p>假设：</p>
<ul>
<li>数据集 <span class="arithmatex">\(X \in \mathbb{R}^{n \times d}\)</span>，<span class="arithmatex">\(n\)</span> 为样本数，<span class="arithmatex">\(d\)</span> 为特征数。</li>
<li>标签为 <span class="arithmatex">\(y \in \mathbb{R}^n\)</span>（回归）或 <span class="arithmatex">\(y \in {0,1}^n\)</span>（分类）。</li>
<li>特征选择的目标是找到子集 <span class="arithmatex">\(S \subseteq {1,2,\dots,d}\)</span>，使得 <span class="arithmatex">\(X_S\)</span> 对预测 <span class="arithmatex">\(y\)</span> 有最大贡献。</li>
</ul>
<hr />
<h4 id="_106">二、特征选择的三大类别<a class="headerlink" href="#_106" title="Permanent link">&para;</a></h4>
<h5 id="1-filter">1️⃣ Filter（过滤法）<a class="headerlink" href="#1-filter" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>原理</strong>：独立于模型，基于 <strong>特征与标签的统计关系</strong> 进行排序和筛选。</li>
<li><strong>方法</strong>：</li>
</ul>
<h6 id="1-variance-threshold">(1) 方差选择（Variance Threshold）<a class="headerlink" href="#1-variance-threshold" title="Permanent link">&para;</a></h6>
<ul>
<li>原理：如果特征的方差很小，说明该特征几乎不变化，对预测无用。</li>
<li>公式：
  $$
  Var(x_j) = \frac{1}{n} \sum_{i=1}^n (x_{ij} - \bar{x}_j)^2
  $$</li>
<li>阈值设定 <span class="arithmatex">\(\theta\)</span>，若 <span class="arithmatex">\(Var(x_j) &lt; \theta\)</span>，则删除特征 <span class="arithmatex">\(x_j\)</span>。</li>
</ul>
<h6 id="2-correlation">(2) 相关系数（Correlation）<a class="headerlink" href="#2-correlation" title="Permanent link">&para;</a></h6>
<ul>
<li><strong>Pearson 相关系数</strong>（连续特征）：</li>
</ul>
<div class="arithmatex">\[
  \rho_{x_j,y} = \frac{Cov(x_j,y)}{\sigma_{x_j}\sigma_y} = \frac{\sum_i (x_{ij}-\bar{x}*j)(y_i-\bar{y})}{\sqrt{\sum_i (x*{ij}-\bar{x}_j)^2}\sqrt{\sum_i (y_i-\bar{y})^2}}
\]</div>
<ul>
<li>高相关（绝对值大）保留，低相关剔除。</li>
<li>对分类可用 <strong>卡方检验</strong>：
  $$
  \chi^2 = \sum_{i,j} \frac{(O_{ij}-E_{ij})^2}{E_{ij}}
  $$</li>
<li>对二分类变量或分类特征常用 Mutual Information（互信息）：
  $$
  I(X;Y) = \sum_{x\in X}\sum_{y\in Y} p(x,y)\log \frac{p(x,y)}{p(x)p(y)}
  $$</li>
</ul>
<hr />
<h5 id="2-wrapper">2️⃣ Wrapper（包裹法）<a class="headerlink" href="#2-wrapper" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>原理</strong>：使用模型性能作为评价指标，搜索特征子集。</li>
<li>
<p><strong>特点</strong>：</p>
<ul>
<li>耗时较 Filter 方法高，但通常效果更好。</li>
</ul>
</li>
<li>
<p><strong>方法</strong>：</p>
</li>
</ul>
<h6 id="1-rfe-recursive-feature-elimination">(1) 递归特征消除（RFE, Recursive Feature Elimination）<a class="headerlink" href="#1-rfe-recursive-feature-elimination" title="Permanent link">&para;</a></h6>
<ul>
<li>
<p>过程：</p>
<ol>
<li>训练模型（如线性回归或SVM）</li>
<li>根据特征重要性（权重 <span class="arithmatex">\(w_j\)</span> 或特征系数）删除最不重要的特征</li>
<li>重复迭代直到达到期望特征数量</li>
</ol>
</li>
<li>
<p>公式（线性模型重要性示例）：
  $$
  w_j = | \theta_j |
  $$</p>
</li>
</ul>
<h6 id="2-stepwise-selection">(2) 前向/后向/逐步选择（Stepwise Selection）<a class="headerlink" href="#2-stepwise-selection" title="Permanent link">&para;</a></h6>
<ul>
<li><strong>前向选择</strong>：从空集开始，逐步加入使模型性能提升最大的特征</li>
<li><strong>后向选择</strong>：从全量特征开始，逐步剔除对模型影响最小的特征</li>
<li><strong>逐步选择</strong>：前向 + 后向交替进行</li>
</ul>
<hr />
<h5 id="3-embedded">3️⃣ Embedded（嵌入法）<a class="headerlink" href="#3-embedded" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>原理</strong>：特征选择嵌入模型训练过程中，通过正则化或树模型自动评估特征重要性。</li>
<li><strong>方法</strong>：</li>
</ul>
<h6 id="1-l1-lasso">(1) L1 正则化（Lasso 回归）<a class="headerlink" href="#1-l1-lasso" title="Permanent link">&para;</a></h6>
<ul>
<li>损失函数：
  $$
  J(\theta) = \frac{1}{n}\sum_{i=1}^{n} (y_i - X_i \theta)^2 + \lambda \sum_{j=1}^{d} |\theta_j|
  $$</li>
<li>当 <span class="arithmatex">\(\lambda\)</span> 足够大时，会将部分 <span class="arithmatex">\(\theta_j\)</span> 收缩为 0 → 对应特征被剔除。</li>
</ul>
<h6 id="2_12">(2) 树模型特征重要性<a class="headerlink" href="#2_12" title="Permanent link">&para;</a></h6>
<ul>
<li>决策树、随机森林、XGBoost 等可输出每个特征的 <strong>Gini importance</strong> 或 <strong>Gain</strong>。</li>
<li>对分类问题：
  $$
  Importance(x_j) = \sum_{t\in T: split \text{ on } x_j} \Delta i(t)
  $$</li>
<li><span class="arithmatex">\(\Delta i(t)\)</span> 为节点 <span class="arithmatex">\(t\)</span> 在分裂时不纯度下降（Gini 或 Entropy）</li>
</ul>
<hr />
<h4 id="_107">三、特征选择的数学目标<a class="headerlink" href="#_107" title="Permanent link">&para;</a></h4>
<p>特征选择本质是<strong>优化子集 <span class="arithmatex">\(S\)</span></strong>：</p>
<div class="arithmatex">\[
S^* = \arg\max_{S\subseteq{1,\dots,d}} Score(S)
\]</div>
<ul>
<li>Filter：<span class="arithmatex">\(Score(S) = \sum_{j\in S} |\rho(x_j,y)|\)</span> 或互信息</li>
<li>Wrapper：<span class="arithmatex">\(Score(S) = CV_Score(Model(X_S, y))\)</span></li>
<li>Embedded：<span class="arithmatex">\(Score(S) = \sum_{j\in S} |w_j|\)</span> 或树模型重要性</li>
</ul>
<blockquote>
<p>特征选择问题是<strong>NP-hard</strong>（组合优化），实际用启发式算法（RFE、贪心、正则化）近似求解。</p>
</blockquote>
<hr />
<h4 id="_108">四、特征选择的注意事项<a class="headerlink" href="#_108" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>避免数据泄漏</strong>：在交叉验证/测试集上做特征选择会导致数据泄漏，应仅在训练集上选择特征。</li>
<li><strong>多重共线性</strong>：高相关特征可能导致模型不稳定，可剔除冗余特征或做 PCA 降维。</li>
<li>
<p><strong>类别特征编码</strong>：</p>
<ul>
<li>One-hot 编码后特征数量增加，需结合正则化或树模型进行筛选。</li>
</ul>
</li>
<li>
<p><strong>特征选择与标准化</strong>：</p>
<ul>
<li>对 L1/L2 正则化模型需要先对特征归一化。</li>
</ul>
</li>
<li>
<p><strong>特征重要性解释</strong>：</p>
<ul>
<li>Filter 方法简单可解释，但忽略特征间交互。</li>
<li>Wrapper/Embedded 方法考虑特征交互，但复杂度高。</li>
</ul>
</li>
</ol>
<hr />
<h4 id="_109">五、特征选择示例代码<a class="headerlink" href="#_109" title="Permanent link">&para;</a></h4>
<h5 id="1-filter_1">1️⃣ Filter 方法（方差 &amp; 相关系数）<a class="headerlink" href="#1-filter_1" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">VarianceThreshold</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">f_classif</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># 方差选择</span>
<span class="n">selector</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">X_var</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 卡方检验（分类任务）</span>
<span class="n">selector</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">f_classif</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">X_best</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>
<h5 id="2-wrapper-rfe">2️⃣ Wrapper 方法（RFE + 逻辑回归）<a class="headerlink" href="#2-wrapper-rfe" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">X_rfe</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rfe</span><span class="o">.</span><span class="n">support_</span><span class="p">)</span>  <span class="c1"># 输出保留特征布尔数组</span>
</code></pre></div>
<h5 id="3-embedded-lasso">3️⃣ Embedded 方法（Lasso + 树模型）<a class="headerlink" href="#3-embedded-lasso" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lasso</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Lasso 回归</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">selected_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># 树模型</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">importances</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">feature_importances_</span>
</code></pre></div>
<hr />
<h4 id="_110">六、总结<a class="headerlink" href="#_110" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>方法</th>
<th>优点</th>
<th>缺点</th>
<th>使用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>Filter</td>
<td>快、易解释</td>
<td>忽略特征间交互</td>
<td>大规模特征筛选</td>
</tr>
<tr>
<td>Wrapper</td>
<td>考虑模型性能、交互</td>
<td>计算量大</td>
<td>小规模特征、追求高精度</td>
</tr>
<tr>
<td>Embedded</td>
<td>模型训练自动筛选、效率高</td>
<td>依赖模型</td>
<td>L1/L2回归、树模型</td>
</tr>
</tbody>
</table>
<blockquote>
<p>特征选择本质是 <strong>降低噪声、减少过拟合、提升模型可解释性</strong>，实际操作时常与正则化、PCA、模型调参结合使用。</p>
</blockquote>
<h3 id="72">7.2 特征提取<a class="headerlink" href="#72" title="Permanent link">&para;</a></h3>
<h4 id="_111">一、什么是特征提取<a class="headerlink" href="#_111" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>特征选择（Feature Selection）</strong>：从原有特征中选择对模型有用的特征。</li>
<li><strong>特征提取（Feature Extraction）</strong>：通过<strong>变换原始特征</strong>生成新的特征，以更有效地表示数据。</li>
</ul>
<p>通俗理解：</p>
<ul>
<li>特征选择 → 删掉不重要的特征</li>
<li>特征提取 → <strong>生成新的、更紧凑或更有信息量的特征</strong></li>
</ul>
<p><strong>目标</strong>：</p>
<ul>
<li>降维</li>
<li>提升模型性能</li>
<li>消除冗余或噪声</li>
<li>提取数据的潜在结构</li>
</ul>
<p>假设：</p>
<ul>
<li>原始数据矩阵 <span class="arithmatex">\(X \in \mathbb{R}^{n \times d}\)</span>（<span class="arithmatex">\(n\)</span> 样本，<span class="arithmatex">\(d\)</span> 特征）</li>
<li>特征提取生成新的特征矩阵 <span class="arithmatex">\(Z \in \mathbb{R}^{n \times k}\)</span>，通常 <span class="arithmatex">\(k&lt;d\)</span>。</li>
</ul>
<hr />
<h4 id="_112">二、特征提取的核心原理<a class="headerlink" href="#_112" title="Permanent link">&para;</a></h4>
<p>特征提取通常通过<strong>数学变换或映射</strong>将原始特征空间映射到一个新的特征空间：</p>
<div class="arithmatex">\[
Z = f(X)
\]</div>
<ul>
<li><span class="arithmatex">\(f\)</span> 可以是线性或非线性映射</li>
<li>新特征 <span class="arithmatex">\(Z\)</span> 保留原始数据的关键信息，可能减少噪声或冗余</li>
</ul>
<p>特征提取常与 <strong>降维（Dimensionality Reduction）</strong> 密切相关。</p>
<hr />
<h4 id="_113">三、常见线性特征提取方法<a class="headerlink" href="#_113" title="Permanent link">&para;</a></h4>
<h5 id="1-pca-principal-component-analysis">1️⃣ 主成分分析（PCA, Principal Component Analysis）<a class="headerlink" href="#1-pca-principal-component-analysis" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>目标</strong>：找到能最大化数据方差的正交方向</li>
<li>
<p><strong>数学定义</strong>：</p>
</li>
<li>
<p>数据中心化：<span class="arithmatex">\(\tilde{X} = X - \bar{X}\)</span></p>
</li>
<li>协方差矩阵：
    $$
    \Sigma = \frac{1}{n} \tilde{X}^T \tilde{X}
    $$</li>
<li>求解特征值和特征向量：
    $$
    \Sigma v_i = \lambda_i v_i
    $$</li>
<li>选择前 <span class="arithmatex">\(k\)</span> 个最大特征值对应的特征向量组成投影矩阵 <span class="arithmatex">\(V_k\)</span>：
    $$
    Z = \tilde{X} V_k
    $$</li>
<li><strong>优点</strong>：压缩维度、去相关性</li>
<li><strong>缺点</strong>：只捕捉线性关系，特征可解释性差</li>
</ul>
<hr />
<h5 id="2-lda-linear-discriminant-analysis">2️⃣ 线性判别分析（LDA, Linear Discriminant Analysis）<a class="headerlink" href="#2-lda-linear-discriminant-analysis" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>目标</strong>：在分类问题中最大化类间距离、最小化类内方差</li>
<li>
<p><strong>公式</strong>：</p>
</li>
<li>
<p>类内散度矩阵：
    $$
    S_W = \sum_{c=1}^{C} \sum_{x_i \in c} (x_i - \mu_c)(x_i - \mu_c)^T
    $$</p>
</li>
<li>类间散度矩阵：
    $$
    S_B = \sum_{c=1}^{C} n_c (\mu_c - \mu)(\mu_c - \mu)^T
    $$</li>
<li>求解投影矩阵 <span class="arithmatex">\(W\)</span>：
    $$
    W = \arg\max_W \frac{|W^T S_B W|}{|W^T S_W W|}
    $$</li>
<li><strong>优点</strong>：适用于监督分类问题，降维后类别可分性增强</li>
<li><strong>缺点</strong>：只能提取 <span class="arithmatex">\(C-1\)</span> 个特征（类别数限制），假设数据高斯分布</li>
</ul>
<hr />
<h5 id="3-ica-independent-component-analysis">3️⃣ 独立成分分析（ICA, Independent Component Analysis）<a class="headerlink" href="#3-ica-independent-component-analysis" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>目标</strong>：将混合信号分解为<strong>统计独立的成分</strong></li>
<li>
<p><strong>公式</strong>：
  $$
  X = A S
  $$</p>
</li>
<li>
<p><span class="arithmatex">\(X\)</span>：观测数据</p>
</li>
<li><span class="arithmatex">\(A\)</span>：混合矩阵</li>
<li><span class="arithmatex">\(S\)</span>：独立成分</li>
<li><strong>优化目标</strong>：最大化非高斯性或最小化互信息：
  $$
  \max \sum_i \text{Negentropy}(s_i)
  $$</li>
</ul>
<hr />
<h4 id="_114">四、常见非线性特征提取方法<a class="headerlink" href="#_114" title="Permanent link">&para;</a></h4>
<h5 id="1-kernel-pca">1️⃣ 核主成分分析（Kernel PCA）<a class="headerlink" href="#1-kernel-pca" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>思想</strong>：对原始数据做非线性映射 <span class="arithmatex">\(\phi: X \rightarrow \mathcal{F}\)</span>，再做 PCA</li>
<li>核技巧：
  $$
  K_{ij} = \phi(x_i)^T \phi(x_j)
  $$</li>
<li>优点：可以提取非线性结构</li>
<li>缺点：计算量大，核选择敏感</li>
</ul>
<hr />
<h5 id="2-t-snet-distributed-stochastic-neighbor-embedding">2️⃣ t-SNE（t-Distributed Stochastic Neighbor Embedding）<a class="headerlink" href="#2-t-snet-distributed-stochastic-neighbor-embedding" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>目标</strong>：将高维数据嵌入低维空间（主要用于可视化）</li>
<li>
<p><strong>方法</strong>：</p>
</li>
<li>
<p>高维空间相似度：
    $$
    p_{j|i} = \frac{\exp(-|x_i - x_j|^2 / 2\sigma_i^2)}{\sum_{k\neq i} \exp(-|x_i - x_k|^2 / 2\sigma_i^2)}
    $$</p>
</li>
<li>低维空间相似度：
    $$
    q_{ij} = \frac{(1 + |y_i - y_j|^2)^{-1}}{\sum_{k\neq l} (1 + |y_k - y_l|^2)^{-1}}
    $$</li>
<li>最小化 KL 散度：
    $$
    KL(P|Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}
    $$</li>
<li><strong>优点</strong>：保持局部结构，视觉效果好</li>
<li><strong>缺点</strong>：不可解释、适合小数据集</li>
</ul>
<hr />
<h5 id="3-autoencoder">3️⃣ AutoEncoder（自编码器）<a class="headerlink" href="#3-autoencoder" title="Permanent link">&para;</a></h5>
<ul>
<li>
<p><strong>神经网络实现特征提取</strong>：</p>
</li>
<li>
<p>编码器 <span class="arithmatex">\(f_\theta: X \rightarrow Z\)</span></p>
</li>
<li>解码器 <span class="arithmatex">\(g_\phi: Z \rightarrow \hat{X}\)</span></li>
<li>最小化重建误差：
    $$
    \min_{\theta,\phi} \sum_i |x_i - g_\phi(f_\theta(x_i))|^2
    $$</li>
<li><strong>优点</strong>：可学习非线性低维表示</li>
<li><strong>缺点</strong>：训练复杂，需调参</li>
</ul>
<hr />
<h4 id="vs">五、特征提取 vs 特征选择<a class="headerlink" href="#vs" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>特点</th>
<th>特征选择</th>
<th>特征提取</th>
</tr>
</thead>
<tbody>
<tr>
<td>方法</td>
<td>选择原始特征子集</td>
<td>生成新特征</td>
</tr>
<tr>
<td>是否降维</td>
<td>不一定</td>
<td>一般降维</td>
</tr>
<tr>
<td>是否改变原始数据</td>
<td>否</td>
<td>是（线性或非线性变换）</td>
</tr>
<tr>
<td>优点</td>
<td>简单可解释</td>
<td>提取潜在结构、减少冗余</td>
</tr>
<tr>
<td>缺点</td>
<td>忽略组合关系</td>
<td>特征难解释</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="python_1">六、特征提取实战示例（Python）<a class="headerlink" href="#python_1" title="Permanent link">&para;</a></h4>
<h5 id="1-pca">1️⃣ PCA<a class="headerlink" href="#1-pca" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;新特征维度:&quot;</span><span class="p">,</span> <span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (100,3)</span>
</code></pre></div>
<h5 id="2-lda">2️⃣ LDA（监督降维）<a class="headerlink" href="#2-lda" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.discriminant_analysis</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span> <span class="k">as</span> <span class="n">LDA</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># 三类</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;新特征维度:&quot;</span><span class="p">,</span> <span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (100,2)</span>
</code></pre></div>
<h5 id="3-autoencoderpytorch">3️⃣ AutoEncoder（PyTorch示例）<a class="headerlink" href="#3-autoencoderpytorch" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="n">X_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AutoEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">z</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoEncoder</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">X_tensor</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># 提取低维特征</span>
<span class="n">_</span><span class="p">,</span> <span class="n">Z_new</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;低维特征:&quot;</span><span class="p">,</span> <span class="n">Z_new</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (100,3)</span>
</code></pre></div>
<hr />
<h4 id="_115">七、特征提取注意事项<a class="headerlink" href="#_115" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>标准化/归一化</strong>：</p>
<ul>
<li>PCA、LDA 对尺度敏感，需先标准化 <span class="arithmatex">\(X\)</span>。</li>
</ul>
</li>
<li>
<p><strong>选择维度 <span class="arithmatex">\(k\)</span></strong>：</p>
<ul>
<li>PCA 可根据累计方差贡献率选择 <span class="arithmatex">\(k\)</span>。</li>
</ul>
</li>
<li>
<p><strong>监督 vs 无监督</strong>：</p>
<ul>
<li>LDA 需标签，PCA 不需标签。</li>
</ul>
</li>
<li>
<p><strong>可解释性</strong>：</p>
<ul>
<li>PCA/AutoEncoder 特征难解释，需要结合原始特征理解。</li>
</ul>
</li>
<li>
<p><strong>非线性特征提取</strong>：</p>
<ul>
<li>t-SNE 适合可视</li>
</ul>
</li>
</ol>
<h2 id="_116">八、决策树<a class="headerlink" href="#_116" title="Permanent link">&para;</a></h2>
<h3 id="81">8.1 教程<a class="headerlink" href="#81" title="Permanent link">&para;</a></h3>
<p><img alt="决策树.png" src="../../imgs/machine/%E5%86%B3%E7%AD%96%E6%A0%91.png" /></p>
<p>在决策树中，我们希望选择 <strong>最优特征</strong> 来进行划分，让每次划分后的子集“越纯越好”。</p>
<ul>
<li><strong>纯</strong>的意思：子集中的样本尽可能属于同一类别。</li>
</ul>
<p>不同算法衡量“纯度”的方法不同：</p>
<ul>
<li><strong>ID3 → 信息增益</strong></li>
<li><strong>C4.5 → 信息增益率</strong></li>
<li>
<p><strong>CART → 基尼指数</strong></p>
</li>
<li>
<p><strong>定义</strong>：决策树是一种 <strong>树形结构模型</strong>，用于分类或回归任务。</p>
<ul>
<li><strong>节点（Node）</strong>：表示特征或属性的条件</li>
<li><strong>边（Edge）</strong>：表示条件判断的结果</li>
<li><strong>叶子节点（Leaf）</strong>：表示最终预测结果</li>
</ul>
</li>
<li>
<p><strong>分类 vs 回归</strong>：</p>
<ul>
<li>分类树（CART分类）：预测离散标签</li>
<li>回归树（CART回归）：预测连续值</li>
</ul>
</li>
</ul>
<p><strong>决策树特点</strong>：</p>
<ul>
<li>非线性、非参数模型</li>
<li>可处理多类别、多特征</li>
<li>易解释，可可视化</li>
</ul>
<h4 id="1_15">1️⃣ 分类问题<a class="headerlink" href="#1_15" title="Permanent link">&para;</a></h4>
<p>在决策树中，我们希望选择 <strong>最优特征</strong> 来进行划分，让每次划分后的子集“越纯越好”。</p>
<ul>
<li><strong>纯</strong>的意思：子集中的样本尽可能属于同一类别。</li>
</ul>
<p>不同算法衡量“纯度”的方法不同：</p>
<ul>
<li><strong>ID3 → 信息增益</strong></li>
<li><strong>C4.5 → 信息增益率</strong></li>
<li><strong>CART → 基尼指数</strong></li>
</ul>
<hr />
<h5 id="id3-information-gain"><strong>二、ID3 与信息增益（Information Gain）</strong><a class="headerlink" href="#id3-information-gain" title="Permanent link">&para;</a></h5>
<h6 id="1_16">1. 概念<a class="headerlink" href="#1_16" title="Permanent link">&para;</a></h6>
<p>信息增益来源于信息论。它衡量的是 <strong>使用某特征划分数据前后，信息的不确定性减少了多少</strong>。</p>
<ul>
<li><strong>熵（Entropy）</strong>：表示数据集的不确定性</li>
</ul>
<div class="arithmatex">\[
  H(D) = - \sum_{i=1}^k p_i \log_2 p_i
\]</div>
<ul>
<li><span class="arithmatex">\(D\)</span>：数据集</li>
<li><span class="arithmatex">\(k\)</span>：类别数</li>
<li>
<p><span class="arithmatex">\(p_i\)</span>：类别 <span class="arithmatex">\(i\)</span> 的概率</p>
</li>
<li>
<p><strong>条件熵（Conditional Entropy）</strong>：在已知某个特征 <span class="arithmatex">\(A\)</span> 的情况下，类别的不确定性</p>
</li>
</ul>
<div class="arithmatex">\[
  H(D|A) = \sum_{v \in \text{values}(A)} \frac{|D_v|}{|D|} H(D_v)
\]</div>
<ul>
<li>
<p><span class="arithmatex">\(D_v\)</span>：在特征 <span class="arithmatex">\(A\)</span> 取值 <span class="arithmatex">\(v\)</span> 的子集</p>
</li>
<li>
<p><strong>信息增益（Information Gain）</strong>：划分前后的熵减少量</p>
</li>
</ul>
<div class="arithmatex">\[
  Gain(D,A) = H(D) - H(D|A)
\]</div>
<h6 id="2_13">2. 直观解释<a class="headerlink" href="#2_13" title="Permanent link">&para;</a></h6>
<ul>
<li>熵越大 → 数据越混乱</li>
<li>划分后熵越小 → 数据越纯 → 信息增益越大</li>
<li>ID3选择 <strong>信息增益最大的特征</strong> 作为划分节点</li>
</ul>
<h6 id="3_7">3. 示例<a class="headerlink" href="#3_7" title="Permanent link">&para;</a></h6>
<p>假设数据集 <span class="arithmatex">\(D\)</span> 有 10 个样本，分为两类：<code>Yes</code> 6 个，<code>No</code> 4 个。</p>
<ul>
<li>总熵：
$$
  H(D) = - \frac{6}{10}\log_2 \frac{6}{10} - \frac{4}{10}\log_2 \frac{4}{10} \approx 0.971
$$</li>
</ul>
<p>假设一个特征 <span class="arithmatex">\(A\)</span> 有两个取值 <code>a1</code> 和 <code>a2</code>，对应子集熵分别为 0.0 和 1.0，子集比例为 0.4 和 0.6：</p>
<div class="arithmatex">\[
H(D|A) = 0.4 \cdot 0 + 0.6 \cdot 1 = 0.6
\]</div>
<p>信息增益：
$$
Gain(D,A) = 0.971 - 0.6 = 0.371
$$</p>
<hr />
<h5 id="c45-information-gain-ratio"><strong>三、C4.5 与信息增益率（Information Gain Ratio）</strong><a class="headerlink" href="#c45-information-gain-ratio" title="Permanent link">&para;</a></h5>
<h6 id="1_17">1. 问题<a class="headerlink" href="#1_17" title="Permanent link">&para;</a></h6>
<p>ID3 的信息增益有偏向性：</p>
<ul>
<li>倾向选择取值 <strong>较多的特征</strong>（如身份证号、日期等），即使这些特征对分类没太大用处。</li>
</ul>
<h6 id="2_14">2. 解决方案<a class="headerlink" href="#2_14" title="Permanent link">&para;</a></h6>
<p>C4.5 使用 <strong>信息增益率</strong> 来修正：
$$
GainRatio(D,A) = \frac{Gain(D,A)}{SplitInfo(D,A)}
$$</p>
<ul>
<li>
<p><strong>分裂信息（SplitInfo）</strong>：
$$
  SplitInfo(D,A) = - \sum_{v \in values(A)} \frac{|D_v|}{|D|} \log_2 \frac{|D_v|}{|D|}
$$</p>
</li>
<li>
<p>核心思想：<strong>用信息增益除以特征自身的划分“杂乱度”</strong>，降低取值多的特征偏好。</p>
</li>
</ul>
<h6 id="3_8">3. 直观理解<a class="headerlink" href="#3_8" title="Permanent link">&para;</a></h6>
<ul>
<li>特征值越多，SplitInfo 越大 → GainRatio 越小 → 减少偏向</li>
<li>C4.5 选择 <strong>信息增益率最大的特征</strong> 进行划分</li>
</ul>
<hr />
<h5 id="cart-gini-index"><strong>四、CART 与基尼指数（Gini Index）</strong><a class="headerlink" href="#cart-gini-index" title="Permanent link">&para;</a></h5>
<p>CART (Classification and Regression Tree) 用 <strong>基尼指数</strong> 来衡量纯度。</p>
<h6 id="1_18">1. 概念<a class="headerlink" href="#1_18" title="Permanent link">&para;</a></h6>
<p>基尼指数表示 <strong>从数据集中随机抽取两个样本，它们属于不同类别的概率</strong>：</p>
<div class="arithmatex">\[
Gini(D) = 1 - \sum_{i=1}^{k} p_i^2
\]</div>
<ul>
<li><span class="arithmatex">\(p_i\)</span>：类别 <span class="arithmatex">\(i\)</span> 的概率</li>
<li>值越小 → 数据越纯</li>
</ul>
<h6 id="2_15">2. 划分后的基尼指数<a class="headerlink" href="#2_15" title="Permanent link">&para;</a></h6>
<p>假设用特征 <span class="arithmatex">\(A\)</span> 划分数据集：
$$
Gini_{split}(D,A) = \sum_{v \in values(A)} \frac{|D_v|}{|D|} Gini(D_v)
$$</p>
<ul>
<li>选择 <strong>基尼指数最小的特征进行划分</strong></li>
<li>CART 生成 <strong>二叉树</strong>（每次分成两个子集）</li>
</ul>
<h6 id="3_9">3. 直观理解<a class="headerlink" href="#3_9" title="Permanent link">&para;</a></h6>
<ul>
<li>与信息增益不同：基尼指数关注 <strong>样本“混合度”</strong></li>
<li>越纯 → 基尼指数越低 → 更优划分</li>
</ul>
<h6 id="4_8">4. 示例<a class="headerlink" href="#4_8" title="Permanent link">&para;</a></h6>
<p>假设一个子集有 10 个样本，其中 6 个 <code>Yes</code>，4 个 <code>No</code>：
$$
Gini(D) = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 0.48
$$</p>
<hr />
<h5 id="_117"><strong>五、总结对比表</strong><a class="headerlink" href="#_117" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>算法</th>
<th>特征选择指标</th>
<th>倾向</th>
<th>划分类型</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>信息增益 Gain</td>
<td>偏向取值多的特征</td>
<td>多叉</td>
<td>简单，概念直观</td>
<td>对多值特征有偏向性</td>
</tr>
<tr>
<td>C4.5</td>
<td>信息增益率 GainRatio</td>
<td>修正 ID3 偏向</td>
<td>多叉</td>
<td>处理多值特征更合理</td>
<td>需要计算分裂信息，略复杂</td>
</tr>
<tr>
<td>CART</td>
<td>基尼指数 Gini</td>
<td>偏向纯度大子集</td>
<td>二叉</td>
<td>计算简单，生成二叉树</td>
<td>对类别比例敏感</td>
</tr>
</tbody>
</table>
<h5 id="2_16">2️⃣ 回归问题<a class="headerlink" href="#2_16" title="Permanent link">&para;</a></h5>
<ul>
<li>常用损失函数：<strong>均方误差（MSE）</strong>
  $$
  MSE(D) = \frac{1}{|D|} \sum_{i \in D} (y_i - \bar{y}_D)^2
  $$</li>
<li>划分后：
  $$
  \Delta MSE = MSE(D) - \left( \frac{|D_1|}{|D|} MSE(D_1) + \frac{|D_2|}{|D|} MSE(D_2) \right)
  $$</li>
<li>选择使 <span class="arithmatex">\(\Delta MSE\)</span> 最大的特征和切分点</li>
</ul>
<h4 id="2_17">2️⃣ 损失函数<a class="headerlink" href="#2_17" title="Permanent link">&para;</a></h4>
<h4 id="3-cart">3️⃣ 决策树数学推导过程（以CART分类树为例）<a class="headerlink" href="#3-cart" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>根节点选择</strong>：</p>
<ul>
<li>对每个特征 <span class="arithmatex">\(A_j\)</span>，尝试每个可能划分点 <span class="arithmatex">\(s\)</span></li>
<li>计算划分后的基尼指数：
  $$
  Gini(D, A_j, s) = \frac{|D_{\le s}|}{|D|} Gini(D_{\le s}) + \frac{|D_{&gt; s}|}{|D|} Gini(D_{&gt; s})
  $$</li>
</ul>
</li>
<li>
<p><strong>选择最小 <span class="arithmatex">\(Gini(D,A_j,s)\)</span></strong></p>
</li>
<li>
<p><strong>递归划分</strong>：</p>
<ul>
<li>对左右子树重复以上过程</li>
<li>
<p>直到满足停止条件：</p>
<ul>
<li>节点样本数 &lt; min_samples_split</li>
<li>树深度达到 max_depth</li>
<li>节点纯度达到阈值</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="4_9">4️⃣ 评估指标<a class="headerlink" href="#4_9" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>分类树</strong>：</p>
<ul>
<li>准确率（Accuracy）</li>
<li>精确率 / 召回率 / F1</li>
<li>ROC / AUC</li>
</ul>
</li>
<li>
<p><strong>回归树</strong>：</p>
<ul>
<li>均方误差（MSE）</li>
<li>均方根误差（RMSE）</li>
<li>平均绝对误差（MAE）</li>
<li>判定系数 <span class="arithmatex">\(R^2\)</span></li>
</ul>
</li>
</ul>
<h4 id="5_4">5️⃣ 实现代码<a class="headerlink" href="#5_4" title="Permanent link">&para;</a></h4>
<h5 id="1_19">1️⃣ 分类树<a class="headerlink" href="#1_19" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h5 id="2_18">2️⃣ 回归树<a class="headerlink" href="#2_18" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</code></pre></div>
<h5 id="3_10">3️⃣ 可视化<a class="headerlink" href="#3_10" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">export_text</span><span class="p">,</span> <span class="n">plot_tree</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># 文本展示</span>
<span class="nb">print</span><span class="p">(</span><span class="n">export_text</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">))</span>

<span class="c1"># 图形展示</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h4 id="6_4">6️⃣ 模型优化<a class="headerlink" href="#6_4" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>剪枝（Pruning）</strong>：</p>
<ul>
<li>防止过拟合</li>
<li><strong>预剪枝</strong>：设置 max_depth, min_samples_split, min_samples_leaf</li>
<li><strong>后剪枝</strong>：训练完成后删除贡献不大的节点</li>
</ul>
</li>
<li>
<p><strong>集成方法</strong>：</p>
<ul>
<li>Bagging → 随机森林（Random Forest）</li>
<li>Boosting → GBDT / XGBoost</li>
</ul>
</li>
<li>
<p><strong>特征处理</strong>：</p>
<ul>
<li>类别特征编码：One-hot 或 Ordinal</li>
<li>连续特征无需标准化</li>
</ul>
</li>
<li>
<p><strong>参数调优</strong>：</p>
<ul>
<li>max_depth, min_samples_split, min_samples_leaf, max_features, ccp_alpha</li>
</ul>
</li>
</ol>
<h4 id="7_3">7️⃣ 注意事项<a class="headerlink" href="#7_3" title="Permanent link">&para;</a></h4>
<ul>
<li>容易<strong>过拟合</strong> → 控制树深、剪枝或集成</li>
<li>对<strong>噪声敏感</strong></li>
<li>样本分布不平衡 → 影响划分质量，可用 class_weight</li>
<li>对连续特征、类别特征处理不同</li>
</ul>
<h4 id="8_3">8️⃣ 优缺点<a class="headerlink" href="#8_3" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 易理解、可解释，可可视化</td>
<td>1. 容易过拟合</td>
</tr>
<tr>
<td>2. 无需特征归一化</td>
<td>2. 对噪声敏感</td>
</tr>
<tr>
<td>3. 可处理数值和类别特征</td>
<td>3. 单棵树不稳定</td>
</tr>
<tr>
<td>4. 可以处理多类别问题</td>
<td>4. 预测精度有限（需集成提升）</td>
</tr>
</tbody>
</table>
<h2 id="knn">九、KNN<a class="headerlink" href="#knn" title="Permanent link">&para;</a></h2>
<h3 id="91-knn">9.1 KNN教程<a class="headerlink" href="#91-knn" title="Permanent link">&para;</a></h3>
<p><img alt="KNN.png" src="../../imgs/machine/KNN.png" /></p>
<h4 id="1_20">1️⃣ 原理<a class="headerlink" href="#1_20" title="Permanent link">&para;</a></h4>
<p>KNN 是一种<strong>基于实例的监督学习算法</strong>，既可以做分类（Classification），也可以做回归（Regression）。
核心思想：</p>
<ul>
<li>对于一个新的数据点 <span class="arithmatex">\(x\)</span>，找到训练集中<strong>与它最近的 <span class="arithmatex">\(K\)</span> 个点</strong>。</li>
<li>
<p>根据这些点的标签来预测新样本的标签：</p>
</li>
<li>
<p><strong>分类</strong>：多数投票（majority voting）。</p>
</li>
<li><strong>回归</strong>：平均或加权平均。</li>
</ul>
<h5 id="11_1">1.1 距离度量<a class="headerlink" href="#11_1" title="Permanent link">&para;</a></h5>
<p>常用的距离度量方法：</p>
<ol>
<li>
<p><strong>欧氏距离（Euclidean distance）</strong>：
   $$
   d(x_i, x_j) = \sqrt{\sum_{m=1}^{M} (x_{i,m} - x_{j,m})^2}
   $$</p>
</li>
<li>
<p><strong>曼哈顿距离（Manhattan distance）</strong>：
   $$
   d(x_i, x_j) = \sum_{m=1}^{M} |x_{i,m} - x_{j,m}|
   $$</p>
</li>
<li>
<p><strong>闵可夫斯基距离（Minkowski distance）</strong>：
   $$
   d(x_i, x_j) = \left(\sum_{m=1}^{M} |x_{i,m} - x_{j,m}|^p \right)^{1/p}
   $$</p>
</li>
<li>
<p>当 <span class="arithmatex">\(p=2\)</span> → 欧氏距离</p>
</li>
<li>
<p>当 <span class="arithmatex">\(p=1\)</span> → 曼哈顿距离</p>
</li>
<li>
<p><strong>加权距离</strong>（常用于 KNN 回归或加权分类）：
   $$
   w_i = \frac{1}{d(x, x_i) + \epsilon}, \quad \hat{y} = \frac{\sum_{i=1}^K w_i y_i}{\sum_{i=1}^K w_i}
   $$</p>
</li>
</ol>
<h4 id="2_19">2️⃣ 损失函数<a class="headerlink" href="#2_19" title="Permanent link">&para;</a></h4>
<p>KNN 本质是<strong>非参数、懒惰学习（lazy learning）</strong>，没有显式的参数拟合，所以 <strong>没有训练损失函数</strong>，但是可以用以下方式描述预测误差：</p>
<h5 id="21_2">2.1 分类损失<a class="headerlink" href="#21_2" title="Permanent link">&para;</a></h5>
<p>对于一个测试样本 <span class="arithmatex">\(x\)</span>，预测类别 <span class="arithmatex">\(\hat{y}\)</span> 与真实类别 <span class="arithmatex">\(y\)</span> 的损失通常用<strong>0-1 损失</strong>：
$$
L(y, \hat{y}) =
\begin{cases}
0, &amp; y = \hat{y} \
1, &amp; y \neq \hat{y}
\end{cases}
$$</p>
<h5 id="22_2">2.2 回归损失<a class="headerlink" href="#22_2" title="Permanent link">&para;</a></h5>
<p>常用 <strong>平方误差</strong> 或 <strong>绝对误差</strong>：</p>
<ul>
<li>均方误差（MSE）：
  $$
  L(y, \hat{y}) = (y - \hat{y})^2
  $$</li>
<li>平均绝对误差（MAE）：
  $$
  L(y, \hat{y}) = |y - \hat{y}|
  $$</li>
</ul>
<h4 id="3_11">3️⃣ 数学推导过程<a class="headerlink" href="#3_11" title="Permanent link">&para;</a></h4>
<h5 id="31_1">3.1 分类<a class="headerlink" href="#31_1" title="Permanent link">&para;</a></h5>
<p>假设找到 <span class="arithmatex">\(K\)</span> 个最近邻样本集合 <span class="arithmatex">\(N_K(x)\)</span>，每个邻居 <span class="arithmatex">\(x_i\)</span> 的类别为 <span class="arithmatex">\(y_i\)</span>：</p>
<div class="arithmatex">\[
\hat{y} = \arg\max_{c \in C} \sum_{x_i \in N_K(x)} \mathbf{1}(y_i = c)
\]</div>
<ul>
<li><span class="arithmatex">\(\mathbf{1}(\cdot)\)</span> 为指示函数</li>
<li><span class="arithmatex">\(C\)</span> 是类别集合</li>
</ul>
<p>加权投票：
$$
\hat{y} = \arg\max_{c \in C} \sum_{x_i \in N_K(x)} w_i \mathbf{1}(y_i = c)
$$
其中 <span class="arithmatex">\(w_i = \frac{1}{d(x, x_i) + \epsilon}\)</span></p>
<h5 id="32_3">3.2 回归<a class="headerlink" href="#32_3" title="Permanent link">&para;</a></h5>
<p>找到 <span class="arithmatex">\(K\)</span> 个邻居 <span class="arithmatex">\(N_K(x)\)</span>，取均值或加权均值：
$$
\hat{y} = \frac{1}{K} \sum_{x_i \in N_K(x)} y_i
$$
加权：
$$
\hat{y} = \frac{\sum_{x_i \in N_K(x)} w_i y_i}{\sum_{x_i \in N_K(x)} w_i}
$$</p>
<h4 id="4_10">4️⃣ 评估指标<a class="headerlink" href="#4_10" title="Permanent link">&para;</a></h4>
<h5 id="41_1">4.1 分类<a class="headerlink" href="#41_1" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>准确率（Accuracy）</strong>：
  $$
  \text{Accuracy} = \frac{\text{预测正确样本数}}{\text{总样本数}}
  $$</li>
<li><strong>精确率、召回率、F1-score</strong>：
  $$
  \text{Precision} = \frac{TP}{TP+FP}, \quad
  \text{Recall} = \frac{TP}{TP+FN}, \quad
  F1 = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
  $$</li>
</ul>
<h5 id="42_1">4.2 回归<a class="headerlink" href="#42_1" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>均方误差（MSE）</strong></li>
<li><strong>均方根误差（RMSE）</strong></li>
<li><strong>平均绝对误差（MAE）</strong></li>
<li><strong><span class="arithmatex">\(R^2\)</span> 决定系数</strong>：
  $$
  R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2}
  $$</li>
</ul>
<h4 id="5_5">5️⃣ 实现代码<a class="headerlink" href="#5_5" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># 1. 数据准备</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 2. 特征归一化（KNN 对距离敏感）</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 3. KNN 分类</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">)</span>  <span class="c1"># 权重距离加权</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 4. 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 5. 评估</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<blockquote>
<p>回归类似，只需用 <code>KNeighborsRegressor</code>，损失函数改为 MSE/MAE。</p>
</blockquote>
<h4 id="6_5">6️⃣ 模型优化<a class="headerlink" href="#6_5" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>选择合适的 K 值</strong>：</p>
<ul>
<li>K 太小 → 易受噪声影响，过拟合</li>
<li>K 太大 → 过于平滑，欠拟合</li>
<li>常用交叉验证选择最优 K</li>
</ul>
</li>
<li>
<p><strong>距离度量</strong>：</p>
<ul>
<li>对连续变量：欧氏、曼哈顿</li>
<li>对类别变量：汉明距离（Hamming distance）</li>
</ul>
</li>
<li>
<p><strong>特征缩放</strong>：</p>
<ul>
<li>KNN 对尺度敏感，建议标准化（StandardScaler）或归一化（MinMaxScaler）</li>
</ul>
</li>
<li>
<p><strong>权重选择</strong>：</p>
<ul>
<li>距离加权投票能提升预测效果，尤其是数据分布不均时</li>
</ul>
</li>
<li>
<p><strong>降维</strong>：</p>
<ul>
<li>数据维度过高 → “维度灾难”，可用 PCA 降维</li>
</ul>
</li>
</ol>
<h4 id="7_4">7️⃣ 注意事项<a class="headerlink" href="#7_4" title="Permanent link">&para;</a></h4>
<ul>
<li>KNN 是<strong>惰性学习</strong>，训练快，但预测慢（需计算所有训练样本距离）</li>
<li>高维数据会导致“距离集中现象”，影响效果</li>
<li>对异常值敏感（尤其是 K 小时）</li>
<li>需要足够内存存储全部训练集</li>
<li>特征尺度不一致需归一化或标准化</li>
</ul>
<h4 id="8_4">8️⃣ 优缺点<a class="headerlink" href="#8_4" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>简单直观，容易理解和实现</td>
<td>预测阶段计算量大，存储需求高</td>
</tr>
<tr>
<td>无需训练过程，无参数学习</td>
<td>对高维数据效果差（维度灾难）</td>
</tr>
<tr>
<td>可用于分类与回归</td>
<td>对异常值敏感</td>
</tr>
<tr>
<td>可通过距离加权提升性能</td>
<td>K 值选择对结果影响大</td>
</tr>
<tr>
<td>可处理非线性边界</td>
<td>特征尺度敏感，需要归一化</td>
</tr>
</tbody>
</table>
<h2 id="svm">十、SVM<a class="headerlink" href="#svm" title="Permanent link">&para;</a></h2>
<h3 id="101-svm">10.1 SVM教程<a class="headerlink" href="#101-svm" title="Permanent link">&para;</a></h3>
<p><img alt="SVM.png" src="../../imgs/machine/SVM.png" /></p>
<h4 id="1_21">1️⃣ 原理<a class="headerlink" href="#1_21" title="Permanent link">&para;</a></h4>
<p>SVM 是一种<strong>监督学习算法</strong>，用于分类和回归（分类最常用）。核心思想是：</p>
<ol>
<li><strong>分类问题</strong>：在特征空间中寻找一个<strong>最优超平面（Hyperplane）</strong>，将不同类别的样本分开。</li>
<li><strong>最大化间隔（Margin）</strong>：在所有可能的分隔超平面中，选择<strong>使两类样本最近点到超平面距离最大</strong>的平面。</li>
<li><strong>支持向量</strong>：离超平面最近的训练样本点称为支持向量，它们决定了超平面的位置。</li>
</ol>
<hr />
<h5 id="11_2">1.1 超平面公式<a class="headerlink" href="#11_2" title="Permanent link">&para;</a></h5>
<p>对于线性可分的情况：</p>
<div class="arithmatex">\[
\mathbf{w} \cdot \mathbf{x} + b = 0
\]</div>
<ul>
<li><span class="arithmatex">\(\mathbf{w}\)</span>：法向量，决定超平面方向</li>
<li><span class="arithmatex">\(b\)</span>：偏置项</li>
<li>分类决策函数：
  $$
  f(\mathbf{x}) = \text{sign}(\mathbf{w}\cdot \mathbf{x} + b)
  $$</li>
</ul>
<h4 id="2_20">2️⃣ 损失函数<a class="headerlink" href="#2_20" title="Permanent link">&para;</a></h4>
<h5 id="21-svm">2.1 线性可分 SVM<a class="headerlink" href="#21-svm" title="Permanent link">&para;</a></h5>
<ul>
<li>最大化间隔 <span class="arithmatex">\(\gamma = \frac{2}{|\mathbf{w}|}\)</span></li>
<li>约束条件：
  $$
  y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \ge 1, \quad i=1,\dots,n
  $$</li>
<li>优化目标（硬间隔最大化）：
  $$
  \min_{\mathbf{w},b} \frac{1}{2}|\mathbf{w}|^2
  $$</li>
</ul>
<h5 id="22-svm">2.2 线性不可分（软间隔）SVM<a class="headerlink" href="#22-svm" title="Permanent link">&para;</a></h5>
<ul>
<li>引入松弛变量 <span class="arithmatex">\(\xi_i \ge 0\)</span>：
  $$
  y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \ge 1 - \xi_i
  $$</li>
<li>损失函数（目标函数）：
  $$
  \min_{\mathbf{w},b,\xi} \frac{1}{2}|\mathbf{w}|^2 + C \sum_{i=1}^n \xi_i
  $$</li>
<li><span class="arithmatex">\(C\)</span>：正则化参数，平衡间隔最大化和分类错误</li>
</ul>
<h5 id="23-hinge">2.3 Hinge 损失函数<a class="headerlink" href="#23-hinge" title="Permanent link">&para;</a></h5>
<ul>
<li>SVM 的<strong>分类损失函数</strong>可表示为 hinge loss：
  $$
  L(y_i, f(\mathbf{x}_i)) = \max(0, 1 - y_i f(\mathbf{x}_i))
  $$</li>
<li>总损失：
  $$
  \min_{\mathbf{w},b} \frac{1}{2} |\mathbf{w}|^2 + C \sum_{i=1}^n \max(0, 1 - y_i (\mathbf{w}\cdot \mathbf{x}_i + b))
  $$</li>
</ul>
<h4 id="3_12">3️⃣ 数学推导过程<a class="headerlink" href="#3_12" title="Permanent link">&para;</a></h4>
<h5 id="31-primal-problem">3.1 原始问题（Primal Problem）<a class="headerlink" href="#31-primal-problem" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\min_{\mathbf{w},b,\xi} \frac{1}{2}|\mathbf{w}|^2 + C \sum_{i=1}^n \xi_i
\]</div>
<div class="arithmatex">\[
\text{s.t. } y_i (\mathbf{w}\cdot \mathbf{x}_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
\]</div>
<h5 id="32-dual-problem">3.2 对偶问题（Dual Problem）<a class="headerlink" href="#32-dual-problem" title="Permanent link">&para;</a></h5>
<p>通过拉格朗日乘子 <span class="arithmatex">\(\alpha_i \ge 0\)</span>：</p>
<div class="arithmatex">\[
L(\mathbf{w},b,\xi, \alpha, \mu) = \frac{1}{2}|\mathbf{w}|^2 + C \sum_i \xi_i - \sum_i \alpha_i [y_i(\mathbf{w}\cdot \mathbf{x}_i+b) - 1 + \xi_i] - \sum_i \mu_i \xi_i
\]</div>
<p>对 <span class="arithmatex">\(\mathbf{w}, b, \xi\)</span> 求偏导，得到对偶问题：</p>
<div class="arithmatex">\[
\max_{\alpha} \sum_i \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j \mathbf{x}_i \cdot \mathbf{x}_j
\]</div>
<div class="arithmatex">\[
\text{s.t. } 0 \le \alpha_i \le C, \quad \sum_i \alpha_i y_i = 0
\]</div>
<ul>
<li>预测函数：
  $$
  f(\mathbf{x}) = \text{sign}\left(\sum_{i \in SV} \alpha_i y_i \mathbf{x}_i \cdot \mathbf{x} + b \right)
  $$</li>
<li>只依赖支持向量（SV）</li>
</ul>
<h5 id="33-kernel-trick">3.3 核方法（Kernel Trick）<a class="headerlink" href="#33-kernel-trick" title="Permanent link">&para;</a></h5>
<p>线性不可分时，将数据映射到高维空间：</p>
<div class="arithmatex">\[
f(\mathbf{x}) = \text{sign}\left(\sum_{i \in SV} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b \right)
\]</div>
<p>常用核函数：</p>
<ol>
<li>线性核：<span class="arithmatex">\(K(\mathbf{x}_i,\mathbf{x}_j) = \mathbf{x}_i \cdot \mathbf{x}_j\)</span></li>
<li>多项式核：<span class="arithmatex">\(K(\mathbf{x}_i,\mathbf{x}_j) = (\mathbf{x}_i\cdot \mathbf{x}_j + 1)^d\)</span></li>
<li>RBF（高斯核）：<span class="arithmatex">\(K(\mathbf{x}_i,\mathbf{x}_j) = \exp(-\gamma |\mathbf{x}_i-\mathbf{x}_j|^2)\)</span></li>
<li>Sigmoid 核：<span class="arithmatex">\(K(\mathbf{x}_i,\mathbf{x}_j) = \tanh(\kappa \mathbf{x}_i \cdot \mathbf{x}_j + \theta)\)</span></li>
</ol>
<h4 id="4_11">4️⃣ 评估指标<a class="headerlink" href="#4_11" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>分类准确率（Accuracy）</strong></li>
<li><strong>精确率、召回率、F1-score</strong></li>
<li><strong>ROC-AUC</strong>（适合二分类）</li>
<li><strong>混淆矩阵</strong></li>
</ul>
<blockquote>
<p>回归问题可用 MSE、MAE、<span class="arithmatex">\(R^2\)</span> 等指标（SVR）。</p>
</blockquote>
<h4 id="5_6">5️⃣ 实现代码<a class="headerlink" href="#5_6" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># 1. 数据</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 2. 特征标准化</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 3. SVM 分类器</span>
<span class="n">svm_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">)</span>  <span class="c1"># RBF 核</span>
<span class="n">svm_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 4. 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">svm_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 5. 评估</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<blockquote>
<p>回归使用 <code>SVR(kernel='rbf')</code>，损失函数为 epsilon-insensitive loss。</p>
</blockquote>
<h4 id="6_6">6️⃣ 模型优化<a class="headerlink" href="#6_6" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>选择核函数</strong>：</p>
<ul>
<li>线性核适合高维稀疏数据</li>
<li>RBF 核适合大多数非线性问题</li>
</ul>
</li>
<li>
<p><strong>调节参数</strong>：</p>
<ul>
<li><span class="arithmatex">\(C\)</span>：正则化参数，过大容易过拟合，过小欠拟合</li>
<li><span class="arithmatex">\(\gamma\)</span>（RBF 核）：核宽度，过大过拟合，过小欠拟合</li>
</ul>
</li>
<li>
<p><strong>特征缩放</strong>：</p>
<ul>
<li>对距离敏感，需归一化或标准化</li>
</ul>
</li>
<li>
<p><strong>使用交叉验证选择参数</strong>：</p>
<ul>
<li><code>GridSearchCV</code> 或 <code>RandomizedSearchCV</code></li>
</ul>
</li>
<li>
<p><strong>处理类别不平衡</strong>：</p>
<ul>
<li>设置 <code>class_weight='balanced'</code></li>
</ul>
</li>
</ol>
<h4 id="7_5">7️⃣ 注意事项<a class="headerlink" href="#7_5" title="Permanent link">&para;</a></h4>
<ul>
<li>对<strong>大规模数据</strong>训练时间长</li>
<li>对<strong>噪声和异常值</strong>敏感（尤其 C 大时）</li>
<li>核函数选择和参数调节很关键</li>
<li>高维稀疏数据线性核更适合</li>
<li>SVM 本质是二分类，多分类需“一对多”或“一对一”策略</li>
</ul>
<h4 id="8_5">8️⃣ 优缺点<a class="headerlink" href="#8_5" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>对高维数据效果好</td>
<td>对大数据集训练慢，预测慢</td>
</tr>
<tr>
<td>适合非线性边界（核方法）</td>
<td>对参数（C, γ）敏感</td>
</tr>
<tr>
<td>具有全局最优解（凸优化）</td>
<td>对噪声和异常值敏感</td>
</tr>
<tr>
<td>可以处理线性和非线性问题</td>
<td>核方法不易解释</td>
</tr>
<tr>
<td>支持多分类扩展</td>
<td>对类别不平衡问题敏感</td>
</tr>
</tbody>
</table>
<h2 id="_118">十一、集成学习<a class="headerlink" href="#_118" title="Permanent link">&para;</a></h2>
<h3 id="111">11.1 教程<a class="headerlink" href="#111" title="Permanent link">&para;</a></h3>
<h4 id="1_22">1️⃣ 原理<a class="headerlink" href="#1_22" title="Permanent link">&para;</a></h4>
<p><strong>集成学习</strong>是一类将多个模型组合在一起以提高整体性能的算法思想，其核心理念是：</p>
<blockquote>
<p>“多个弱学习器结合，可以形成一个强学习器。”</p>
</blockquote>
<p>集成方法主要有两类：</p>
<h5 id="11-baggingbootstrap-aggregating">1.1 Bagging（Bootstrap Aggregating，自助聚合）<a class="headerlink" href="#11-baggingbootstrap-aggregating" title="Permanent link">&para;</a></h5>
<ul>
<li>核心思想：通过<strong>对训练集进行有放回抽样</strong>生成多个不同子集，在每个子集上训练一个模型，然后将多个模型预测结果<strong>平均（回归）或投票（分类）</strong>。</li>
<li>
<p>代表算法：</p>
<ul>
<li><strong>随机森林（Random Forest）</strong></li>
</ul>
</li>
<li>
<p>作用：<strong>降低模型方差（Variance）</strong></p>
</li>
</ul>
<p>公式：
$$
\hat{y} = \frac{1}{M} \sum_{m=1}^M h_m(x)
$$</p>
<ul>
<li><span class="arithmatex">\(h_m(x)\)</span>：第 <span class="arithmatex">\(m\)</span> 个基模型的预测</li>
<li><span class="arithmatex">\(M\)</span>：基模型数量</li>
</ul>
<hr />
<h5 id="12-boosting">1.2 Boosting（提升方法）<a class="headerlink" href="#12-boosting" title="Permanent link">&para;</a></h5>
<ul>
<li>核心思想：顺序训练模型，每个新模型<strong>重点关注前一个模型预测错误的样本</strong>。</li>
<li>
<p>代表算法：</p>
<ul>
<li><strong>AdaBoost</strong></li>
<li><strong>Gradient Boosting / XGBoost / LightGBM / CatBoost</strong></li>
</ul>
</li>
<li>
<p>作用：<strong>降低模型偏差（Bias）</strong></p>
</li>
</ul>
<p>AdaBoost 分类预测公式：
$$
H(x) = \text{sign}\left(\sum_{m=1}^M \alpha_m h_m(x)\right)
$$</p>
<ul>
<li><span class="arithmatex">\(\alpha_m\)</span>：第 <span class="arithmatex">\(m\)</span> 个模型的权重</li>
<li><span class="arithmatex">\(h_m(x)\)</span>：基模型预测结果（通常为 <span class="arithmatex">\(\pm 1\)</span>）</li>
</ul>
<hr />
<h5 id="13-stacking">1.3 Stacking（堆叠）<a class="headerlink" href="#13-stacking" title="Permanent link">&para;</a></h5>
<ul>
<li>核心思想：使用多个不同模型生成一层输出，再用一个“元学习器（Meta-learner）”综合预测结果。</li>
<li>公式：</li>
</ul>
<p>$$
  z_i = [h_1(x_i), h_2(x_i), \dots, h_M(x_i)]
  $$</p>
<p>$$
  \hat{y}_i = g(z_i)
  $$</p>
<ul>
<li><span class="arithmatex">\(h_m\)</span>：第 <span class="arithmatex">\(m\)</span> 个基模型</li>
<li><span class="arithmatex">\(g\)</span>：元学习器（如逻辑回归、线性回归）</li>
</ul>
<h4 id="2_21">2️⃣ 损失函数<a class="headerlink" href="#2_21" title="Permanent link">&para;</a></h4>
<p>集成学习的损失函数取决于基模型和任务类型：</p>
<h5 id="21_3">2.1 分类<a class="headerlink" href="#21_3" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>0-1 损失</strong>：
  $$
  L(y, \hat{y}) = \mathbf{1}(y \neq \hat{y})
  $$</li>
<li><strong>交叉熵损失（Soft Voting 可用）</strong>：
  $$
  L(y, \hat{p}) = - \sum_{c} y_c \log \hat{p}_c
  $$</li>
</ul>
<h5 id="22_3">2.2 回归<a class="headerlink" href="#22_3" title="Permanent link">&para;</a></h5>
<ul>
<li>均方误差（MSE）：
  $$
  L(y, \hat{y}) = (y - \hat{y})^2
  $$</li>
<li>平均绝对误差（MAE）：
  $$
  L(y, \hat{y}) = |y - \hat{y}|
  $$</li>
</ul>
<h4 id="3_13">3️⃣ 数学推导过程<a class="headerlink" href="#3_13" title="Permanent link">&para;</a></h4>
<h5 id="31-bagging">3.1 Bagging 方差减少公式<a class="headerlink" href="#31-bagging" title="Permanent link">&para;</a></h5>
<p>假设基模型方差为 <span class="arithmatex">\(\sigma^2\)</span>，相关性为 <span class="arithmatex">\(\rho\)</span>：
$$
\text{Var}(\hat{y}_{bag}) = \rho \sigma^2 + \frac{1-\rho}{M}\sigma^2
$$</p>
<ul>
<li><span class="arithmatex">\(M\)</span>：基模型数量</li>
<li>结论：<strong>基模型越多，方差越小，且基模型相关性越低效果越好</strong></li>
</ul>
<h5 id="32-boosting">3.2 Boosting 加权组合<a class="headerlink" href="#32-boosting" title="Permanent link">&para;</a></h5>
<p>AdaBoost：</p>
<ol>
<li>初始化样本权重 <span class="arithmatex">\(w_i = 1/n\)</span></li>
<li>训练基模型 <span class="arithmatex">\(h_m(x)\)</span>，计算加权错误率：
   $$
   \epsilon_m = \frac{\sum_{i=1}^n w_i \mathbf{1}(y_i \neq h_m(x_i))}{\sum_{i=1}^n w_i}
   $$</li>
<li>基模型权重：
   $$
   \alpha_m = \frac{1}{2} \ln \frac{1-\epsilon_m}{\epsilon_m}
   $$</li>
<li>更新样本权重：
   $$
   w_i \leftarrow w_i \cdot \exp(-\alpha_m y_i h_m(x_i))
   $$</li>
</ol>
<h4 id="4_12">4️⃣ 评估指标<a class="headerlink" href="#4_12" title="Permanent link">&para;</a></h4>
<h5 id="41_2">4.1 分类<a class="headerlink" href="#41_2" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Accuracy、Precision、Recall、F1-score</strong></li>
<li><strong>ROC-AUC</strong></li>
</ul>
<h5 id="42_2">4.2 回归<a class="headerlink" href="#42_2" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>MSE、RMSE、MAE</strong></li>
<li><strong><span class="arithmatex">\(R^2\)</span></strong></li>
</ul>
<h4 id="5_7">5️⃣ 实现代码<a class="headerlink" href="#5_7" title="Permanent link">&para;</a></h4>
<h5 id="51-bagging">5.1 Bagging（随机森林）<a class="headerlink" href="#51-bagging" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h5 id="52-boostingxgboost">5.2 Boosting（XGBoost）<a class="headerlink" href="#52-boostingxgboost" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">xgboost</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">xgb</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">xg_clf</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">xg_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">xg_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h4 id="6_7">6️⃣ 模型优化<a class="headerlink" href="#6_7" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Bagging</strong></p>
<ul>
<li>增加基模型数量（n_estimators）</li>
<li>降低基模型深度以减少过拟合</li>
<li>随机选特征（feature subsampling）</li>
</ul>
</li>
<li>
<p><strong>Boosting</strong></p>
<ul>
<li>调节学习率（learning_rate）</li>
<li>调节树深度（max_depth）</li>
<li>调节基模型数量（n_estimators）</li>
<li>使用正则化（L1/L2）</li>
</ul>
</li>
<li>
<p><strong>Stacking</strong></p>
<ul>
<li>多样化基模型类型（决策树、SVM、线性回归等）</li>
<li>使用交叉验证防止元学习器过拟合</li>
</ul>
</li>
</ol>
<h4 id="7_6">7️⃣ 注意事项<a class="headerlink" href="#7_6" title="Permanent link">&para;</a></h4>
<ul>
<li>Bagging 更适合高方差模型（如决策树）</li>
<li>Boosting 更适合高偏差模型（如浅树）</li>
<li>样本不平衡问题需使用加权或重采样</li>
<li>基模型选择与多样性影响效果</li>
<li>样本量大时计算开销大</li>
</ul>
<h4 id="8_6">8️⃣ 优缺点<a class="headerlink" href="#8_6" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>提升模型准确性和稳定性</td>
<td>训练和预测时间较长</td>
</tr>
<tr>
<td>降低过拟合风险（Bagging）</td>
<td>可解释性差</td>
</tr>
<tr>
<td>可处理高维和复杂数据</td>
<td>参数调节较多（Boosting）</td>
</tr>
<tr>
<td>灵活，可组合多种模型</td>
<td>样本量小可能效果不佳</td>
</tr>
<tr>
<td>适用于分类与回归</td>
<td>Stacking 需交叉验证，复杂</td>
</tr>
</tbody>
</table>
<h2 id="_119">十二、无监督学习<a class="headerlink" href="#_119" title="Permanent link">&para;</a></h2>
<h3 id="121">12.1 聚类<a class="headerlink" href="#121" title="Permanent link">&para;</a></h3>
<h4 id="1_23">1️⃣ 原理<a class="headerlink" href="#1_23" title="Permanent link">&para;</a></h4>
<p>聚类算法通过某种相似度或距离度量，将数据集划分为若干簇（Cluster）。</p>
<p>常见聚类方法：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>算法</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>划分式</strong></td>
<td>K-Means、K-Medoids</td>
<td>简单高效，适合凸形簇</td>
</tr>
<tr>
<td><strong>层次式</strong></td>
<td>Agglomerative、Divisive</td>
<td>构建树状结构，直观</td>
</tr>
<tr>
<td><strong>密度式</strong></td>
<td>DBSCAN、OPTICS</td>
<td>可发现任意形状簇，处理噪声</td>
</tr>
<tr>
<td><strong>模型式</strong></td>
<td>高斯混合模型（GMM）</td>
<td>基于概率模型，软聚类</td>
</tr>
<tr>
<td><strong>谱聚类</strong></td>
<td>Spectral Clustering</td>
<td>基于图论，适合复杂形状</td>
</tr>
</tbody>
</table>
<h4 id="2_22">2️⃣ 损失函数<a class="headerlink" href="#2_22" title="Permanent link">&para;</a></h4>
<h5 id="21-k-means">2.1 K-Means<a class="headerlink" href="#21-k-means" title="Permanent link">&para;</a></h5>
<ul>
<li>目标是最小化簇内平方误差（WCSS, within-cluster sum of squares）：
  $$
  J = \sum_{k=1}^{K} \sum_{x_i \in C_k} |x_i - \mu_k|^2
  $$</li>
<li><span class="arithmatex">\(\mu_k\)</span>：第 <span class="arithmatex">\(k\)</span> 个簇的质心</li>
<li>优化过程即<strong>最小化簇内平方距离</strong></li>
</ul>
<h5 id="22-gmm">2.2 高斯混合模型（GMM）<a class="headerlink" href="#22-gmm" title="Permanent link">&para;</a></h5>
<ul>
<li>最大化对数似然：
  $$
  \log L(\theta) = \sum_{i=1}^{n} \log \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k) \right)
  $$</li>
<li><span class="arithmatex">\(\pi_k\)</span>：簇权重</li>
<li><span class="arithmatex">\(\mathcal{N}\)</span>：高斯分布</li>
</ul>
<h5 id="23-dbscan">2.3 DBSCAN<a class="headerlink" href="#23-dbscan" title="Permanent link">&para;</a></h5>
<ul>
<li>
<p>没有明确损失函数，基于<strong>密度可达性</strong>：</p>
<ul>
<li>核心点：邻域内样本数 ≥ MinPts</li>
<li>簇由核心点及其可达点组成</li>
</ul>
</li>
</ul>
<h4 id="3_14">3️⃣ 数学推导过程<a class="headerlink" href="#3_14" title="Permanent link">&para;</a></h4>
<ol>
<li>初始化 <span class="arithmatex">\(K\)</span> 个质心 <span class="arithmatex">\(\mu_k\)</span></li>
<li><strong>分配步骤（Assignment）</strong>：
   $$
   C_k = { x_i : |x_i - \mu_k|^2 \le |x_i - \mu_j|^2, \forall j }
   $$</li>
<li><strong>更新步骤（Update）</strong>：
   $$
   \mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i
   $$</li>
<li>重复 2-3 步，直到质心不再变化或损失函数收敛</li>
</ol>
<blockquote>
<p>GMM 使用 <strong>EM 算法</strong>：</p>
</blockquote>
<ul>
<li>E 步：计算后验概率
  $$
  \gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_i|\mu_j, \Sigma_j)}
  $$</li>
<li>M 步：更新参数
  $$
  \mu_k = \frac{\sum_i \gamma_{ik} x_i}{\sum_i \gamma_{ik}}, \quad
  \Sigma_k = \frac{\sum_i \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_i \gamma_{ik}}, \quad
  \pi_k = \frac{\sum_i \gamma_{ik}}{n}
  $$</li>
</ul>
<h4 id="4_13">4️⃣ 评估指标<a class="headerlink" href="#4_13" title="Permanent link">&para;</a></h4>
<h5 id="41_3">4.1 内部指标（无需真实标签）<a class="headerlink" href="#41_3" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>簇内平方和（WCSS）</strong></li>
<li><strong>轮廓系数（Silhouette Score）</strong>：
  $$
  s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
  $$</li>
<li><strong>Calinski-Harabasz 指数</strong></li>
<li><strong>Davies-Bouldin 指数</strong></li>
</ul>
<h5 id="42_3">4.2 外部指标（有真实标签）<a class="headerlink" href="#42_3" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>调整兰德指数（ARI, Adjusted Rand Index）</strong></li>
<li><strong>互信息（AMI, Adjusted Mutual Information）</strong></li>
<li><strong>纯度（Purity）</strong></li>
</ul>
<h4 id="5_8">5️⃣ 实现代码<a class="headerlink" href="#5_8" title="Permanent link">&para;</a></h4>
<h5 id="51-k-means">5.1 K-Means<a class="headerlink" href="#51-k-means" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">silhouette_score</span>

<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;质心:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;轮廓系数:&quot;</span><span class="p">,</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
</code></pre></div>
<h5 id="52-dbscan">5.2 DBSCAN<a class="headerlink" href="#52-dbscan" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;簇标签:&quot;</span><span class="p">,</span> <span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>  <span class="c1"># -1表示噪声</span>
</code></pre></div>
<h5 id="53-gmm">5.3 GMM<a class="headerlink" href="#53-gmm" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.mixture</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<h4 id="6_8">6️⃣ 模型优化<a class="headerlink" href="#6_8" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>K 值选择（K-Means/GMM）</strong>：</p>
<ul>
<li>肘部法（Elbow Method）</li>
<li>轮廓系数最大化</li>
</ul>
</li>
<li>
<p><strong>特征缩放</strong>：</p>
<ul>
<li>聚类对尺度敏感，建议标准化（StandardScaler）</li>
</ul>
</li>
<li>
<p><strong>初始化优化</strong>：</p>
<ul>
<li>K-Means++ 初始化质心</li>
</ul>
</li>
<li>
<p><strong>异常值处理</strong>：</p>
<ul>
<li>密度类算法（DBSCAN）可处理噪声</li>
</ul>
</li>
<li>
<p><strong>降维</strong>：</p>
<ul>
<li>PCA / t-SNE / UMAP 用于高维数据可视化或降维聚类</li>
</ul>
</li>
</ol>
<h4 id="7_7">7️⃣ 注意事项<a class="headerlink" href="#7_7" title="Permanent link">&para;</a></h4>
<ul>
<li>K-Means 对初始质心敏感，可能陷入局部最优</li>
<li>K-Means 假设簇为凸形，无法处理任意形状簇</li>
<li>DBSCAN 对参数（eps, min_samples）敏感</li>
<li>GMM 假设数据符合高斯分布</li>
<li>高维数据易受“维度灾难”影响</li>
<li>聚类结果可能不唯一，需要多次尝试</li>
</ul>
<h4 id="8_7">8️⃣ 优缺点<a class="headerlink" href="#8_7" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>无需标签，适合无监督任务</td>
<td>结果依赖算法和参数选择</td>
</tr>
<tr>
<td>可发现隐藏结构</td>
<td>K-Means 假设簇为球形</td>
</tr>
<tr>
<td>DBSCAN 能发现任意形状簇</td>
<td>高维数据效果差（需降维）</td>
</tr>
<tr>
<td>算法种类丰富，可选性强</td>
<td>对噪声和异常值敏感（部分算法）</td>
</tr>
<tr>
<td>可结合降维方法可视化数据</td>
<td>聚类结果解释性有限</td>
</tr>
</tbody>
</table>
<h3 id="122">12.2 降维<a class="headerlink" href="#122" title="Permanent link">&para;</a></h3>
<h4 id="1_24">1️⃣ 原理<a class="headerlink" href="#1_24" title="Permanent link">&para;</a></h4>
<p><strong>降维</strong>指将高维数据映射到低维空间，同时尽量保留原始数据的主要特征信息。
目的包括：</p>
<ul>
<li><strong>去噪</strong>：去除冗余特征</li>
<li><strong>可视化</strong>：将高维数据映射到 2D 或 3D 便于展示</li>
<li><strong>减少计算量</strong>：降低算法复杂度</li>
<li><strong>缓解维度灾难</strong>：高维空间中数据稀疏，降低模型过拟合风险</li>
</ul>
<p>常见方法分类：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>方法</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>线性降维</strong></td>
<td>PCA（主成分分析）、LDA（线性判别分析）</td>
<td>简单、效率高，假设数据线性结构</td>
</tr>
<tr>
<td><strong>非线性降维</strong></td>
<td>t-SNE、UMAP、Isomap、Kernel PCA</td>
<td>适合复杂结构，高维非线性关系</td>
</tr>
</tbody>
</table>
<h4 id="2_23">2️⃣ 损失函数<a class="headerlink" href="#2_23" title="Permanent link">&para;</a></h4>
<h5 id="21-pca">2.1 PCA<a class="headerlink" href="#21-pca" title="Permanent link">&para;</a></h5>
<ul>
<li>目标是保留方差最大：
  $$
  \max_{W} \text{Tr}(W^T S W)
  $$</li>
<li><span class="arithmatex">\(S\)</span>：数据协方差矩阵</li>
<li><span class="arithmatex">\(W\)</span>：降维投影矩阵</li>
<li>等价于最小化重构误差：
  $$
  J = \sum_{i=1}^{n} |x_i - \hat{x}<em>i|^2 = \sum</em>{i=1}^{n} |x_i - WW^T x_i|^2
  $$</li>
</ul>
<h5 id="22-lda">2.2 LDA<a class="headerlink" href="#22-lda" title="Permanent link">&para;</a></h5>
<ul>
<li>目标是最大化类间散度/类内散度比：
  $$
  J(W) = \frac{|W^T S_B W|}{|W^T S_W W|}
  $$</li>
<li><span class="arithmatex">\(S_B\)</span>：类间散度矩阵</li>
<li><span class="arithmatex">\(S_W\)</span>：类内散度矩阵</li>
</ul>
<h5 id="23-t-sne-umap">2.3 t-SNE / UMAP<a class="headerlink" href="#23-t-sne-umap" title="Permanent link">&para;</a></h5>
<ul>
<li>
<p>t-SNE：最小化高维概率分布 <span class="arithmatex">\(P\)</span> 与低维概率分布 <span class="arithmatex">\(Q\)</span> 的 <strong>KL 散度</strong>：
  $$
  C = KL(P|Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}
  $$</p>
</li>
<li>
<p>UMAP：最小化图的交叉熵损失，保持局部邻域结构。</p>
</li>
</ul>
<h4 id="3_15">3️⃣ 数学推导过程<a class="headerlink" href="#3_15" title="Permanent link">&para;</a></h4>
<h5 id="31-pca">3.1 PCA<a class="headerlink" href="#31-pca" title="Permanent link">&para;</a></h5>
<ol>
<li>数据中心化：
   $$
   X \leftarrow X - \bar{X}
   $$</li>
<li>计算协方差矩阵：
   $$
   S = \frac{1}{n} X^T X
   $$</li>
<li>特征值分解：
   $$
   S W = W \Lambda
   $$</li>
<li>选择前 <span class="arithmatex">\(k\)</span> 个最大特征值对应的特征向量 <span class="arithmatex">\(W_k\)</span>，投影：
   $$
   Z = X W_k
   $$</li>
</ol>
<h5 id="32-lda">3.2 LDA<a class="headerlink" href="#32-lda" title="Permanent link">&para;</a></h5>
<ol>
<li>计算类内散度矩阵 <span class="arithmatex">\(S_W\)</span>：
   $$
   S_W = \sum_{c=1}^{C} \sum_{x_i \in c} (x_i - \mu_c)(x_i - \mu_c)^T
   $$</li>
<li>计算类间散度矩阵 <span class="arithmatex">\(S_B\)</span>：
   $$
   S_B = \sum_{c=1}^{C} n_c (\mu_c - \mu)(\mu_c - \mu)^T
   $$</li>
<li>求广义特征值问题：
   $$
   S_B W = \Lambda S_W W
   $$</li>
<li>投影：
   $$
   Z = X W
   $$</li>
</ol>
<h4 id="4_14">4️⃣ 评估指标<a class="headerlink" href="#4_14" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>保留方差（Explained Variance）</strong>（PCA）：
  $$
  \text{Explained Variance Ratio} = \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^d \lambda_i}
  $$</li>
<li><strong>重构误差（Reconstruction Error）</strong></li>
<li><strong>下游任务性能</strong>：降维后数据在分类/回归上的准确率或 <span class="arithmatex">\(R^2\)</span></li>
<li><strong>聚类保真度</strong>：降维后聚类质量（轮廓系数）</li>
</ul>
<h4 id="5_9">5️⃣ 实现代码<a class="headerlink" href="#5_9" title="Permanent link">&para;</a></h4>
<h5 id="51-pca">5.1 PCA<a class="headerlink" href="#51-pca" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_reduced</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;保留方差比:&quot;</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
</code></pre></div>
<h5 id="52-lda">5.2 LDA<a class="headerlink" href="#52-lda" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.discriminant_analysis</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span> <span class="k">as</span> <span class="n">LDA</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>
<h5 id="53-t-sne">5.3 t-SNE<a class="headerlink" href="#53-t-sne" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">TSNE</span>

<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<h4 id="6_9">6️⃣ 模型优化<a class="headerlink" href="#6_9" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>选择降维维度 <span class="arithmatex">\(k\)</span></strong>：</p>
<ul>
<li>PCA：累计方差 &gt; 90%</li>
<li>LDA：最多 <span class="arithmatex">\(C-1\)</span> 维（类别数 - 1）</li>
</ul>
</li>
<li>
<p><strong>标准化</strong>：</p>
<ul>
<li>PCA、LDA 对尺度敏感，先标准化或归一化</li>
</ul>
</li>
<li>
<p><strong>非线性方法</strong>：</p>
<ul>
<li>t-SNE/UMAP 参数调节：<code>perplexity</code>、<code>learning_rate</code>、邻域大小</li>
</ul>
</li>
<li>
<p><strong>降噪处理</strong>：</p>
<ul>
<li>可先 PCA 降噪，再使用 t-SNE 可视化</li>
</ul>
</li>
</ul>
<h4 id="7_8">7️⃣ 注意事项<a class="headerlink" href="#7_8" title="Permanent link">&para;</a></h4>
<ul>
<li>PCA 假设数据线性可分，对非线性结构可能失效</li>
<li>LDA 需要标签信息，仅用于有监督降维</li>
<li>t-SNE 随机性较大，需设置 <code>random_state</code></li>
<li>非线性方法计算量大，适合中小型数据</li>
<li>高维稀疏数据需先考虑稀疏性处理</li>
</ul>
<h4 id="8_8">8️⃣ 优缺点<a class="headerlink" href="#8_8" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>降低计算复杂度</td>
<td>可能丢失部分信息</td>
</tr>
<tr>
<td>去噪、减少冗余</td>
<td>非线性方法难以解释</td>
</tr>
<tr>
<td>可视化高维数据</td>
<td>部分方法参数敏感（t-SNE/UMAP）</td>
</tr>
<tr>
<td>缓解维度灾难，改善模型泛化</td>
<td>高维数据仍需标准化</td>
</tr>
<tr>
<td>可结合下游任务提高性能</td>
<td>LDA 受类别不平衡影响较大</td>
</tr>
</tbody>
</table>
<h2 id="_120">十三、 概率模型<a class="headerlink" href="#_120" title="Permanent link">&para;</a></h2>
<h3 id="131">13.1 朴素贝叶斯<a class="headerlink" href="#131" title="Permanent link">&para;</a></h3>
<h4 id="1_25">1️⃣ 原理<a class="headerlink" href="#1_25" title="Permanent link">&para;</a></h4>
<p>朴素贝叶斯是一种<strong>基于概率的分类方法</strong>，核心思想是：</p>
<ol>
<li>
<p><strong>贝叶斯定理</strong>：
   $$
   P(y|x) = \frac{P(x|y)P(y)}{P(x)}
   $$</p>
</li>
<li>
<p><span class="arithmatex">\(P(y|x)\)</span>：后验概率</p>
</li>
<li><span class="arithmatex">\(P(y)\)</span>：先验概率</li>
<li>
<p><span class="arithmatex">\(P(x|y)\)</span>：似然概率</p>
</li>
<li>
<p><strong>朴素假设</strong>：假设特征条件独立：
   $$
   P(x|y) = \prod_{i=1}^{n} P(x_i | y)
   $$</p>
</li>
<li>
<p><strong>分类决策</strong>：
   $$
   \hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i | y)
   $$</p>
</li>
</ol>
<h5 id="_121">特点<a class="headerlink" href="#_121" title="Permanent link">&para;</a></h5>
<ul>
<li>非常简单高效</li>
<li>适用于文本分类、垃圾邮件识别、情感分析等</li>
<li>对小样本也能工作良好</li>
</ul>
<h4 id="2_24">2️⃣ 损失函数<a class="headerlink" href="#2_24" title="Permanent link">&para;</a></h4>
<p>朴素贝叶斯通常是概率模型，训练时<strong>不显式优化传统损失函数</strong>，但可视为最大化<strong>似然函数（MLE, Maximum Likelihood Estimation）</strong>：</p>
<div class="arithmatex">\[
\mathcal{L}(\theta) = \prod_{i=1}^{N} P(y_i) \prod_{j=1}^{n} P(x_{ij} | y_i)
\]</div>
<ul>
<li>对数似然：
  $$
  \log \mathcal{L}(\theta) = \sum_{i=1}^{N} \log P(y_i) + \sum_{i=1}^{N} \sum_{j=1}^{n} \log P(x_{ij} | y_i)
  $$</li>
</ul>
<p>训练目标：<strong>最大化对数似然</strong>，得到 <span class="arithmatex">\(P(y)\)</span> 和 <span class="arithmatex">\(P(x_i|y)\)</span>。</p>
<h4 id="3_16">3️⃣ 数学推导过程<a class="headerlink" href="#3_16" title="Permanent link">&para;</a></h4>
<h5 id="31_2">3.1 分类公式<a class="headerlink" href="#31_2" title="Permanent link">&para;</a></h5>
<ol>
<li>
<p><strong>先验概率</strong>：
   $$
   P(y=c) = \frac{\text{样本数}(y=c)}{\text{总样本数}}
   $$</p>
</li>
<li>
<p><strong>似然概率</strong>：</p>
</li>
<li>
<p>离散特征（多项式 NB）：
  $$
  P(x_i = k | y=c) = \frac{\text{特征 } x_i \text{ 在类 } c \text{ 中出现次数} + \alpha}{\text{类 } c \text{ 总次数} + \alpha \cdot n_i}
  $$</p>
</li>
<li>
<p><span class="arithmatex">\(\alpha\)</span>：平滑参数（Laplace 平滑），避免零概率</p>
</li>
<li>
<p>连续特征（高斯 NB）：
  $$
  P(x_i | y=c) = \frac{1}{\sqrt{2\pi \sigma_{c,i}^2}} \exp\Big(-\frac{(x_i - \mu_{c,i})^2}{2\sigma_{c,i}^2}\Big)
  $$</p>
</li>
<li>
<p><strong>预测</strong>：
   $$
   \hat{y} = \arg\max_c P(y=c) \prod_{i=1}^n P(x_i|y=c)
   $$</p>
</li>
</ol>
<h4 id="4_15">4️⃣ 评估指标<a class="headerlink" href="#4_15" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>分类准确率（Accuracy）</strong>：
  $$
  \text{Accuracy} = \frac{\text{预测正确数}}{\text{总样本数}}
  $$</li>
<li><strong>精确率、召回率、F1-score</strong></li>
<li><strong>混淆矩阵</strong></li>
<li>对多分类问题也适用</li>
</ul>
<h4 id="5_10">5️⃣ 实现代码<a class="headerlink" href="#5_10" title="Permanent link">&para;</a></h4>
<h5 id="51_1">5.1 多项式朴素贝叶斯（文本分类常用）<a class="headerlink" href="#51_1" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># 数据</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sci.space&#39;</span><span class="p">,</span><span class="s1">&#39;rec.sport.baseball&#39;</span><span class="p">])</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 特征提取</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">X_train_vec</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_vec</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 模型训练</span>
<span class="n">nb</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">nb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_vec</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 预测</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">nb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_vec</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h5 id="52_1">5.2 高斯朴素贝叶斯（连续特征）<a class="headerlink" href="#52_1" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">gnb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">gnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">gnb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h4 id="6_10">6️⃣ 模型优化<a class="headerlink" href="#6_10" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>特征选择</strong>：去掉冗余或高度相关特征</li>
<li><strong>平滑参数 <span class="arithmatex">\(\alpha\)</span></strong>：防止零概率问题</li>
<li><strong>类别平衡</strong>：样本不均衡时可加权</li>
<li><strong>数据清洗</strong>：减少噪声，提高似然估计精度</li>
</ul>
<h4 id="7_9">7️⃣ 注意事项<a class="headerlink" href="#7_9" title="Permanent link">&para;</a></h4>
<ul>
<li>假设<strong>特征条件独立</strong>，若特征相关性强可能影响性能</li>
<li>对<strong>连续特征</strong>需选择合适分布（高斯 NB 常用）</li>
<li>对稀疏特征（如文本）效果非常好</li>
<li>对小样本表现稳健</li>
<li>输出概率可直接用于决策阈值调整</li>
</ul>
<h4 id="8_9">8️⃣ 优缺点<a class="headerlink" href="#8_9" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>简单高效，训练快</td>
<td>条件独立假设不成立可能降低精度</td>
</tr>
<tr>
<td>对小样本数据有效</td>
<td>对特征相关性敏感</td>
</tr>
<tr>
<td>可输出类别概率</td>
<td>对连续特征需分布假设</td>
</tr>
<tr>
<td>文本分类效果优</td>
<td>不适合复杂非线性边界问题</td>
</tr>
<tr>
<td>参数少，无需调参</td>
<td>数据稀疏度低时性能下降</td>
</tr>
</tbody>
</table>
<h2 id="_122">十四、其他问题<a class="headerlink" href="#_122" title="Permanent link">&para;</a></h2>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.tabs", "navigation.sections", "navigation.indexes", "content.code.copy", "content.tabs.link", "content.code.annotate", "math"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../styles/javascripts/katex.js"></script>
      
    
  </body>
</html>