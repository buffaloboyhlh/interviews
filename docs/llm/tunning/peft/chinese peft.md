å¤ªå¥½äº†ï¼Œå‡ºå‘ï¼
æˆ‘ä»¬ç»™ä½ ä¸€ä¸ªå®Œæ•´ã€å®ç”¨çš„ **ä¸­æ–‡ä»»åŠ¡ï¼šæƒ…æ„Ÿåˆ†ç±» + LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰** çš„ç¤ºèŒƒå·¥ç¨‹ã€‚ä½ å¯ä»¥åœ¨è‡ªå·±çš„ç”µè„‘æˆ– Colab ä¸Šå®Œæˆè®­ç»ƒï¼Œå¹¶æœ€ç»ˆæŠŠæ¨¡å‹éƒ¨ç½²å‡ºæ¥ã€‚

---

# âœ… ä¸­æ–‡ PEFT å¾®è°ƒå®Œæ•´æ•™ç¨‹

ä»»åŠ¡é€‰æ‹©ï¼š**ä¸­æ–‡æƒ…æ„Ÿåˆ†æï¼ˆæ­£å‘ / è´Ÿå‘ï¼‰**
æ¨¡å‹é€‰æ‹©ï¼š**Chinese-BERTï¼ˆå¦‚ bert-base-chineseï¼‰**
æ–¹æ³•é€‰æ‹©ï¼š**LoRAï¼ˆPEFTï¼‰**

è¿™ä»½æ•™ç¨‹è¯¦ç»†è§£é‡Šæ¯ä¸€æ­¥ï¼Œä¾¿äºç†è§£å’Œæ‹“å±•ã€‚

---

## ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¾èµ–

```bash
pip install torch transformers datasets peft accelerate
```

å¦‚æœ‰ GPU ç¯å¢ƒæ›´ä½³ã€‚

---

## ç¬¬äºŒæ­¥ï¼šå‡†å¤‡æ•°æ®é›†ï¼ˆä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»ï¼‰

ç¤ºä¾‹ä½¿ç”¨ Hugging Face è‡ªå¸¦çš„ä¸­æ–‡è¯„è®ºæ•°æ®é›†ï¼ˆChnSentiCorpï¼‰ã€‚

```python
from datasets import load_dataset

dataset = load_dataset("chnsenticorp")
dataset
```

æ•°æ®æ ¼å¼ï¼š

* `text`ï¼šè¯„è®ºå†…å®¹
* `label`ï¼š0 = è´Ÿé¢ï¼Œ1 = æ­£é¢

ä¸ºäº†æ›´å¿«æ¼”ç¤ºï¼Œä½ ä¹Ÿå¯ä»¥ï¼š

```python
dataset = dataset.shuffle(seed=42).select(range(3000))  # å°æ ·æœ¬æµ‹è¯•
```

---

## ç¬¬ä¸‰æ­¥ï¼šåŠ è½½æ¨¡å‹å’Œ tokenizer

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_name = "bert-base-chinese"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)
```

---

## ç¬¬å››æ­¥ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆtokenizeï¼‰

```python
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

encoded = dataset.map(tokenize_fn, batched=True)
encoded = encoded.remove_columns(["text"])
encoded.set_format("torch")
```

---

## ç¬¬äº”æ­¥ï¼šé…ç½® PEFTï¼ˆLoRAï¼‰

```python
from peft import LoraConfig, TaskType, get_peft_model

peft_config = LoraConfig(
    task_type=TaskType.SEQUENCE_CLASSIFICATION,  
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none"
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
```

è¾“å‡ºç¤ºä¾‹ï¼š
å¯è®­ç»ƒå‚æ•° < æ€»å‚æ•°çš„ 1%
è¿™å°±æ˜¯çœèµ„æºçš„é­”æ³•ï¼

---

## ç¬¬å…­æ­¥ï¼šè®­ç»ƒï¼ˆTrainerï¼‰

```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./peft-bert-chinese",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=16,
    num_train_epochs=2,
    logging_steps=50,
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded["train"],
    eval_dataset=encoded["validation"]
)

trainer.train()
```

---

## ç¬¬ä¸ƒæ­¥ï¼šè¯„ä¼°æ¨¡å‹

```python
metrics = trainer.evaluate(encoded["test"])
metrics
```

å¦‚æœè®­ç»ƒæ­£å¸¸ï¼Œå‡†ç¡®ç‡é€šå¸¸èƒ½ > 90%

---

## ç¬¬å…«æ­¥ï¼šä¿å­˜ LoRA adapter

```python
model.save_pretrained("peft_lora_chnsentic")
tokenizer.save_pretrained("peft_lora_chnsentic")
```

å¾—åˆ°çš„å°ç›®å½•å°±æ˜¯ LoRA æ¨¡å—ï¼Œå®ƒæè½»é‡å¯éƒ¨ç½²ã€‚

---

## ç¬¬ä¹æ­¥ï¼šæ¨ç†æµ‹è¯•ï¼ˆåŠ è½½ Adapterï¼‰

```python
from peft import PeftModel

base_model = AutoModelForSequenceClassification.from_pretrained(model_name)
lora_model = PeftModel.from_pretrained(base_model, "peft_lora_chnsentic")

text = "è¿™å®¶é¤å…çš„èœå¤ªéš¾åƒäº†ï¼"
tokens = tokenizer(text, return_tensors="pt")
output = lora_model(**tokens)
print(output.logits.argmax().item())  # 0 â†’ è´Ÿé¢
```

æµ‹è¯•æˆåŠŸ âœ…

---

# âœ… Bonusï¼šéƒ¨ç½²æˆ Web åº”ç”¨ï¼ˆé… Gradioï¼‰

```python
import gradio as gr
import torch

def classify(text):
    inputs = tokenizer(text, return_tensors="pt")
    logits = lora_model(**inputs).logits
    label_id = logits.argmax().item()
    return "æ­£é¢ğŸ‘" if label_id == 1 else "è´Ÿé¢ğŸ‘"

iface = gr.Interface(fn=classify, inputs="text", outputs="text", title="ä¸­æ–‡æƒ…æ„Ÿè¯†åˆ« LoRA æ¨¡å‹")
iface.launch()
```

ç«‹åˆ»ä½“éªŒï¼šâ€œè¿™ä¸ªæ¨¡å‹çœŸæ£’ï¼â€ vs â€œè¾£é¸¡é¡¹ç›®ï¼â€

---

# ğŸŒŸä½ å¯ä»¥å»¶ä¼¸çš„å®éªŒæ–¹å‘

* æ¢ä»»åŠ¡ï¼šæ‘˜è¦ã€ç¿»è¯‘ã€å¯¹è¯ã€å…³é”®è¯æå–
* æ¢åŸºç¡€æ¨¡å‹ï¼šRoBERTaã€BERT-wwm-extã€Chinese-LLM
* åŠ é€Ÿç­–ç•¥ï¼šé‡åŒ– 8bit/4bit + LoRAï¼ˆè¶…çœæ˜¾å­˜ï¼‰
* éƒ¨ç½²ï¼šHugging Face Spaces + CI/CD

é€šè¿‡ PEFTï¼Œä½ å·²ç»æŒæ¡äº† â€œå°æˆæœ¬é©¯æœå¤§æ¨¡å‹â€ çš„æŠ€èƒ½ã€‚æœªæ¥ä¸ç”¨æœ›ç€ 130B å‚æ•°æ¨¡å‹æµæ³ªï¼š

ä½ æœ‰ Adapterï¼Œå®ƒè½»ï¼Œä¹Ÿèªæ˜ã€‚

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥ç»™ä½ ï¼š

âœ… ä¸­æ–‡æ‘˜è¦ç”Ÿæˆ â€” PEFT + T5
âœ… ä¸­æ–‡å¯¹è¯æ¨¡å‹ â€” PEFT + LLaMA/Mistral/Qwen
âœ… å¤šæ¨¡æ€ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰ PEFT
âœ… å®Œæ•´é¡¹ç›®ç»“æ„ï¼ˆåŒ…å«æ•°æ®æ¸…æ´—ã€æ—¥å¿—ã€å®éªŒè®°å½•ï¼‰

ä½ å¯¹å“ªä¸ªæ–¹å‘æ›´æœ‰å…´è¶£ï¼Ÿæˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ å®šåˆ¶å®Œæ•´å­¦ä¹ è·¯çº¿ã€‚
