## 一、LLM推理基础：了解核心过程与瓶颈

大型语言模型的推理过程主要分为两个关键阶段，理解它们是后续学习所有优化技术的基础。

-   **预填充阶段**：这是处理用户输入的过程。模型会**并行处理**整个输入序列（prompt），计算每个token的键值（Key-Value，简称KV）并缓存起来。这个阶段计算密度高，能充分利用GPU的并行计算能力。
-   **解码阶段**：这是模型生成回答的过程。基于预填充的KV缓存和已生成的token，**自回归地**逐个生成新token。此阶段计算量小但访存频繁，性能主要受限于GPU的**内存带宽**，而非计算能力。

### 核心性能瓶颈：KV缓存

在自回归生成过程中，为了避免为每个新token都重新计算之前所有token的键值，模型会将它们缓存起来，这就是**KV缓存**。它是内存消耗的主要来源之一。例如，一个7B参数的模型，在序列长度为4096时，KV缓存可能占用约2GB显存。当处理多个并发请求时，KV缓存会急剧消耗显存，成为主要瓶颈。

## 二、关键性能指标：如何衡量优化效果

在深入技术前，需明确优化的目标。以下是评估推理性能的四个核心指标：

| 指标 | 定义 | 适用场景 |
| :--- | :--- | :--- |
| **首 Token 延迟（TTFT）** | 从输入提交到收到第一个输出token的时间。 | **流式应用**（如ChatGPT），影响用户体验的响应速度。 |
| **每个输出 Token 延迟（TPOT）** | 生成第一个token后，每个后续token的平均时间。 | **批处理应用**，影响总体生成效率。 |
| **吞吐量（Throughput）** | 单位时间内系统能处理的所有请求生成的token总数（tokens/s）。 | 离线生成场景，关心总体资源利用率。 |
| **延迟（Latency）** | 从输入开始到输出结束的总时间。 | 综合衡量单次请求的总体验。 |

## 三、核心优化技术详解

优化技术主要围绕如何高效利用GPU的**计算单元**和**内存带宽**展开，目标是在保证模型质量的同时，提升速度（降低延迟）和效率（提高吞吐量）。

### 1. 模型层面的“瘦身”技术：量化

**量化**是降低模型权重和激活值数值精度的过程，是效果最显著的优化技术之一。

-   **原理**：将模型参数从FP16（16位浮点数）转换为INT8或INT4（8/4位整数），显著减少模型体积和内存占用。
-   **好处**：更小的模型体积意味着在推理时，从显存加载权重到计算核心的数据传输量更小，从而缓解内存带宽瓶颈，提高速度。
-   **常用方法**：
    -   **GPTQ**：一种流行的训练后量化方法，尤其适合减少权重精度。
    -   **AWQ**：一种更先进的量化方法，试图保留对模型性能影响最大的权重，在更低精度下保持模型质量。

### 2. 计算与内存优化：注意力机制革新

注意力机制是Transformer的核心，也是优化的重点。

-   **FlashAttention**：通过**算子融合**和**智能分块**计算策略，避免在高速显存（HBM）和片上共享内存（SRAM）间频繁读写庞大的中间注意力矩阵，从而显著加快注意力计算速度并降低内存占用。
-   **多查询注意力与分组查询注意力**：
    -   **MHA**：是标准的多头注意力，每个头都有独立的K、V向量，缓存开销大。
    -   **MQA**：所有头**共享**一份K、V向量，极大减少了KV缓存，但可能牺牲模型质量。
    -   **GQA**：在MHA和MQA间取得平衡，将查询头分组，**组内共享**一份K、V向量。它在显著减少缓存的同时，能更好地保持模型性能。Llama 2 70B等模型便采用了GQA。

### 3. 系统级调度优化：让GPU保持忙碌

-   **动态批处理/持续批处理**：传统静态批处理需等一批请求全部完成后才处理下一批，效率低下。**持续批处理**会在某个请求生成结束后，**立即**将等待队列中的新请求加入该“空位”，动态更新批次组合，极大提高了GPU利用率，是实现高吞吐的关键。
-   **PagedAttention**：受操作系统虚拟内存分页思想启发，解决了KV缓存管理中的显存**碎片化**和**过度预留**问题。它将每个序列的KV缓存划分为固定大小的块，允许不连续存储。这套机制是**vLLM**推理引擎的核心，在生产环境中常带来数倍的吞吐量提升。

### 4. 利用多GPU的规模化推理：模型并行

当模型过大，单个GPU无法加载时，需将模型拆分到多个设备上。

+ **张量并行**：将模型单个层的权重矩阵**水平拆分**到多个GPU上。例如，一个大的线性层可以被分成两半，分别在两个GPU上计算。这是目前最主流的推理并行方式。
+ **流水线并行**：将模型按**层深度**拆分，不同层放在不同GPU上。一个输入依次经过各个GPU完成计算。这种方式可能因设备间通信等待而产生“气泡”，降低效率。

## 四、实践指南：从入门到精通的学习路径

为了帮助您更好地规划学习过程，下面提供一个循序渐进的学习路径。

![大模型推理优化技术.png](../../imgs/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.png)

### 阶段一：基础入门
-   **目标**：建立直观理解。
-   **行动**：
    1.  使用 **Hugging Face Transformers** 库运行一个中小模型（如 Llama-2-7B），观察预填充和解码过程。
    2.  学习使用 **模型量化（如bitsandbytes库的8位量化）**，感受显存占用和速度的变化。

### 阶段二：核心技术深入
-   **目标**：掌握核心优化技术。
-   **行动**：
    1.  学习并使用集成多种优化的推理引擎，如 **vLLM**。它开箱即用地支持了PagedAttention和持续批处理，很容易体验到性能提升。
    2.  深入研究 **TensorRT-LLM** 。这是一个由NVIDIA推出的高性能推理框架，提供了详细的官方示例。尝试将一个小模型（如Baichuan2-7B）转换为优化的TensorRT引擎，并运行基准测试，对比优化前后的性能指标（吞吐量、延迟）。

### 阶段三：高级专题与源码级优化
-   **目标**：理解技术细节，能进行定制化优化。
-   **行动**：
    1.  阅读 **FlashAttention**、**GQA** 等关键技术的原始论文。
    2.  尝试阅读 **vLLM** 或 **TensorRT-LLM** 的部分核心源码，理解其实现机制。
    3.  了解 **推测解码** 等更前沿的优化思想。

### 阶段四：融会贯通与工程实践
-   **目标**：针对复杂场景进行综合优化。
-   **行动**：
    1.  参与开源项目的社区讨论，尝试解决实际的Issue。
    2.  在真实的业务场景（如部署一个需要长上下文对话的应用）中，根据指标要求（是TTFT优先还是吞吐量优先）选择和组合不同的优化技术，进行端到端的性能调优。

希望这份指南能为您的学习提供清晰的地图。如果您对某个具体技术点特别感兴趣，我可以为您提供更深入的资料。