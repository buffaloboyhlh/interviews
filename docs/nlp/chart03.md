
# ç¬¬ 3 ç«  â€” Transformer ä¸ Self-Attention

Transformer æ˜¯ç°ä»£ NLP çš„æ ¸å¿ƒæ¶æ„ï¼Œå¹¿æ³›åº”ç”¨äº BERTã€GPT ç­‰æ¨¡å‹ã€‚å®ƒçš„æœ€å¤§ç‰¹ç‚¹æ˜¯**å½»åº•æ‘†è„±äº† RNN/LSTM çš„é¡ºåºè®¡ç®—**ï¼Œé€šè¿‡ **Self-Attention** åŒæ—¶å¤„ç†æ•´ä¸ªåºåˆ—ï¼Œå®ç°é«˜æ•ˆå¹¶è¡Œå’Œå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚

---

## 3.1 å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š

1. ç†è§£ Transformer çš„æ•´ä½“æ¶æ„ï¼ˆEncoder ä¸ Decoderï¼‰
2. æŒæ¡ Self-Attention çš„åŸç†ã€å…¬å¼ä¸ç›´è§‚ç†è§£
3. ç†è§£ Multi-Head Attention å’Œä½ç½®ç¼–ç 
4. ç”¨ PyTorch å®ç°åŸºæœ¬çš„ Self-Attention å’Œ Transformer
5. ç†è§£ Transformer ç›¸è¾ƒäº RNN çš„ä¼˜åŠ¿

---

## 3.2 Transformer æ¦‚è§ˆ

### 3.2.1 æ¶æ„ç»„æˆ

Transformer ç”± **Encoder** å’Œ **Decoder** ä¸¤éƒ¨åˆ†ç»„æˆï¼š

* **Encoder**ï¼šå¤„ç†è¾“å…¥åºåˆ—ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡è¡¨ç¤º
* **Decoder**ï¼šæ¥æ”¶ Encoder è¾“å‡ºï¼Œç”Ÿæˆç›®æ ‡åºåˆ—

**Encoder æ ¸å¿ƒæ¨¡å—**ï¼š

1. Multi-Head Self-Attention
2. å‰é¦ˆå…¨è¿æ¥ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰
3. æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–

**Decoder æ ¸å¿ƒæ¨¡å—**ï¼š

1. Masked Multi-Head Self-Attentionï¼ˆé¿å…çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
2. Encoder-Decoder Attentionï¼ˆå°†è¾“å…¥åºåˆ—ä¿¡æ¯ä¸å½“å‰ç”Ÿæˆåºåˆ—å¯¹é½ï¼‰
3. å‰é¦ˆå…¨è¿æ¥ç½‘ç»œ + æ®‹å·®è¿æ¥

> Encoder-Decoder æ¶æ„å¸¸ç”¨äºæœºå™¨ç¿»è¯‘ï¼Œå•ç‹¬ Encoderï¼ˆå¦‚ BERTï¼‰ç”¨äºç†è§£ä»»åŠ¡ï¼Œå•ç‹¬ Decoderï¼ˆå¦‚ GPTï¼‰ç”¨äºç”Ÿæˆä»»åŠ¡ã€‚

---

## 3.3 Self-Attention åŸç†

Self-Attention æ˜¯ Transformer çš„æ ¸å¿ƒï¼Œå®ƒè®©æ¯ä¸ªè¯å¯ä»¥**å…³æ³¨åºåˆ—ä¸­æ‰€æœ‰å…¶ä»–è¯**ï¼Œä»è€Œæ•è·å…¨å±€ä¸Šä¸‹æ–‡ã€‚

### 3.3.1 è¾“å…¥ä¸è¾“å‡º

* è¾“å…¥åºåˆ—ï¼š$X = [x_1, x_2, ..., x_n]$ï¼Œæ¯ä¸ª $x_i$ æ˜¯è¯å‘é‡
* è¾“å‡ºåºåˆ—ï¼š$Z = [z_1, z_2, ..., z_n]$ï¼Œæ¯ä¸ª $z_i$ æ˜¯ä¸Šä¸‹æ–‡å‘é‡

> ç›´è§‚ç†è§£ï¼šSelf-Attention å°±åƒæ¯ä¸ªè¯éƒ½åœ¨é—®â€œåœ¨ç†è§£æˆ‘è‡ªå·±çš„æ„ä¹‰æ—¶ï¼Œå…¶ä»–è¯çš„é‡è¦æ€§æ˜¯å¤šå°‘â€ï¼Œç„¶åæ ¹æ®æƒé‡æ•´åˆä¿¡æ¯ã€‚

---

### 3.3.2 Self-Attention è®¡ç®—å…¬å¼

1. **ç”Ÿæˆ Queryã€Keyã€Value å‘é‡**ï¼š

$$
Q = X W^Q, \quad K = X W^K, \quad V = X W^V
$$

* $W^Q, W^K, W^V$ï¼šå¯å­¦ä¹ çš„æƒé‡çŸ©é˜µ
* $Q$ï¼šQueryï¼ˆæé—®ï¼‰
* $K$ï¼šKeyï¼ˆå›ç­”çš„å…³é”®ï¼‰
* $V$ï¼šValueï¼ˆå®é™…ä¿¡æ¯ï¼‰

2. **è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**ï¼š

$$
\text{Attention}(Q,K,V) = \text{softmax}\Big(\frac{Q K^\top}{\sqrt{d_k}}\Big) V
$$

* $d_k$ï¼šKey å‘é‡ç»´åº¦ï¼Œç”¨ $\sqrt{d_k}$ ç¼©æ”¾é¿å…åˆ†æ•°è¿‡å¤§
* $Q K^\top$ï¼šè¡¡é‡ Query ä¸æ¯ä¸ª Key çš„ç›¸ä¼¼åº¦
* softmaxï¼šå°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæƒé‡

> ç›´è§‚ç†è§£ï¼šæ¯ä¸ªè¯å¯¹åºåˆ—ä¸­æ‰€æœ‰è¯çš„â€œå…³æ³¨ç¨‹åº¦â€è¢«é‡åŒ–ï¼Œå¾—åˆ°åŠ æƒä¿¡æ¯ã€‚

---

### 3.3.3 Self-Attention ä¸¾ä¾‹

å¥å­ï¼š**"The cat sat on the mat"**

* Query: "cat"
* Key/Value: æ‰€æœ‰è¯
* Attention æƒé‡å¯èƒ½æ˜¾ç¤ºï¼š

  * "sat": 0.4
  * "mat": 0.3
  * "The": 0.05

> è¯´æ˜â€œcatâ€ä¼šæ›´å…³æ³¨ä¸å…¶è¯­ä¹‰ç›¸å…³çš„è¯ï¼Œâ€œsatâ€å’Œâ€œmatâ€çš„æƒé‡è¾ƒé«˜ã€‚

---

## 3.4 Multi-Head Attention

å•ä¸ªæ³¨æ„åŠ›å¤´å¯èƒ½æ•æ‰çš„ä¿¡æ¯æœ‰é™ï¼Œ**Multi-Head Attention** ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´æ•æ‰ä¸åŒè¯­ä¹‰å…³ç³»ï¼š

$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
$$

* æ¯ä¸ª head æœ‰ç‹¬ç«‹çš„ $W^Q, W^K, W^V$
* å¯ä»¥åœ¨ä¸åŒå­ç©ºé—´å…³æ³¨ä¸åŒä¿¡æ¯
* æœ€åé€šè¿‡ $W^O$ æ•´åˆå¤šå¤´ä¿¡æ¯

> ç±»æ¯”ï¼šä¸€ç¾¤ä¸“å®¶åˆ†åˆ«å…³æ³¨åºåˆ—çš„ä¸åŒè§’åº¦ï¼Œç„¶åæ±‡æ€»æ„è§ã€‚

---

## 3.5 ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

Transformer ä¸åƒ RNN é‚£æ ·è‡ªç„¶æ„ŸçŸ¥é¡ºåºï¼Œå› æ­¤éœ€è¦æ˜¾å¼ä½ç½®ç¼–ç ï¼š

$$
PE_{(pos,2i)} = \sin\Big(\frac{pos}{10000^{2i/d_\text{model}}}\Big), \quad
PE_{(pos,2i+1)} = \cos\Big(\frac{pos}{10000^{2i/d_\text{model}}}\Big)
$$

* $pos$ï¼šè¯åœ¨åºåˆ—çš„ä½ç½®
* $i$ï¼šå‘é‡ç»´åº¦ç´¢å¼•

> ç›´è§‚ç†è§£ï¼šæ­£å¼¦/ä½™å¼¦æ³¢ä¸åŒé¢‘ç‡ç¼–ç ä½ç½®ï¼Œä½¿æ¨¡å‹åŒºåˆ†é¡ºåºï¼ŒåŒæ—¶å…è®¸æ’å€¼é¢„æµ‹ã€‚

---

## 3.6 å‰é¦ˆå…¨è¿æ¥ç½‘ç»œï¼ˆFeed-Forwardï¼‰

æ¯ä¸ª Encoder/Decoder å±‚è¿˜åŒ…å«ä¸€ä¸ªå‰é¦ˆç½‘ç»œï¼š

$$
\text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2
$$

* ç‹¬ç«‹å¤„ç†æ¯ä¸ªä½ç½®
* å¢åŠ éçº¿æ€§è¡¨è¾¾èƒ½åŠ›
* é…åˆæ®‹å·®è¿æ¥å’Œ LayerNorm

---

## 3.7 æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–

æ¯ä¸€å±‚ä½¿ç”¨æ®‹å·®è¿æ¥å’Œ Layer Normalizationï¼š

$$
\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))
$$

* é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
* åŠ å¿«è®­ç»ƒæ”¶æ•›
* ä¿æŒä¿¡æ¯æµé€šé¡ºç•…

---

## 3.8 Transformer Python ç¤ºä¾‹ï¼ˆPyTorchï¼‰

```python
import torch
import torch.nn as nn

# è¾“å…¥ï¼šbatch_size=2, seq_len=5, embedding_dim=512
x = torch.randn(2,5,512)

# Multi-Head Attention
mha = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)
out, attn_weights = mha(x, x, x)

print("è¾“å‡ºå½¢çŠ¶:", out.shape)        # (2,5,512)
print("æ³¨æ„åŠ›æƒé‡å½¢çŠ¶:", attn_weights.shape)  # (2,8,5,5)
```

> æ³¨æ„åŠ›æƒé‡å¯ä»¥å¯è§†åŒ–ï¼Œè§‚å¯Ÿæ¯ä¸ªè¯å…³æ³¨åºåˆ—ä¸­å“ªäº›è¯ã€‚

---

## 3.9 Transformer ä¼˜åŠ¿

| ç‰¹æ€§    | RNN      | Transformer   |
| ----- | -------- | ------------- |
| å¹¶è¡Œè®¡ç®—  | å¦ï¼Œå¿…é¡»é¡ºåºå¤„ç† | æ˜¯ï¼Œå…¨åºåˆ—å¹¶è¡Œ       |
| é•¿è·ç¦»ä¾èµ– | éš¾æ•æ‰      | æ˜“æ•æ‰ï¼Œå…¨å±€æ³¨æ„åŠ›     |
| è®­ç»ƒé€Ÿåº¦  | æ…¢        | å¿«             |
| è¡¨è¾¾èƒ½åŠ›  | æœ‰é™       | å¼ºï¼Œå¤šå¤´æ³¨æ„åŠ›æ•æ‰å¤æ‚è¯­ä¹‰ |
| é€‚ç”¨ä»»åŠ¡  | å°è§„æ¨¡åºåˆ—    | å¤§è§„æ¨¡é¢„è®­ç»ƒ & ç”Ÿæˆ   |

---

## 3.10 Transformer ç›´è§‚ç†è§£

* Self-Attentionï¼šæ¯ä¸ªè¯â€œçœ‹â€æ•´ä¸ªåºåˆ—ï¼Œæ‰¾å‡ºç›¸å…³ä¿¡æ¯
* Multi-Head Attentionï¼šå¤šä¸ªâ€œä¸“å®¶â€ï¼Œæ•æ‰ä¸åŒè¯­ä¹‰
* å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ï¼šå¤„ç†å¤æ‚éçº¿æ€§å…³ç³»ï¼ŒåŒæ—¶ä¿è¯ä¿¡æ¯æµ
* ä½ç½®ç¼–ç ï¼šå‘Šè¯‰æ¨¡å‹è¯çš„é¡ºåº

> Transformer çš„å¹¶è¡Œå¤„ç†å’Œå…¨å±€æ³¨æ„åŠ›è®©æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿç†è§£é•¿æ–‡æœ¬è¯­ä¹‰ã€‚

---

## 3.11 å•å¤´è‡ªæ³¨æ„åŠ›å®ç°

æˆ‘ä»¬è¦ä»è¾“å…¥åºåˆ— ( X )ï¼ˆå½¢çŠ¶ `[batch, seq_len, d_model]`ï¼‰å‡ºå‘ï¼Œ

+ batch: æ‰¹æ¬¡
+ seq_len: æ–‡æœ¬é•¿åº¦
+ d_model: åµŒå…¥ç»´åº¦

ç»è¿‡çº¿æ€§æ˜ å°„å¾—åˆ° Qã€Kã€Vï¼Œç„¶åè®¡ç®—ï¼š

$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

å¹¶è¿”å›æœ€ç»ˆè¾“å‡ºã€‚

---

### â‘  æ‰‹åŠ¨æƒé‡

æˆ‘ä»¬è‡ªå·±å®šä¹‰æ¯ä¸ªå˜æ¢çŸ©é˜µï¼š

$$
W_Q, W_K, W_V \in \mathbb{R}^{d_{model} \times d_{model}}
$$

è¿™å°±ç­‰ä»·äº `nn.Linear(d_model, d_model)` çš„æƒé‡ã€‚

---

### â‘¡ æ‰‹åŠ¨çŸ©é˜µä¹˜æ³•

`torch.matmul(x, self.W_Q)`
ç›¸å½“äºæ‰§è¡Œï¼š

$$
Q = XW_Q + b_Q
$$

å…¶ä¸­ (X) æ˜¯ `(batch, seq_len, d_model)`ï¼ŒçŸ©é˜µä¹˜æ³•åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šå®Œæˆã€‚

---

### â‘¢ æ³¨æ„åŠ›å¾—åˆ†è®¡ç®—

$$
\text{scores} = \frac{QK^\top}{\sqrt{d_k}}
$$

* ç»´åº¦å˜æ¢ï¼š`K.transpose(-2, -1)`
  æŠŠ `(batch, seq_len, d_model)` è½¬ä¸º `(batch, d_model, seq_len)`ï¼Œ
  ä½¿å¾—æ¯ä¸ª query éƒ½èƒ½å’Œæ‰€æœ‰ key åšç‚¹ç§¯ã€‚

---

### â‘£ åŠ æƒæ±‚å’Œ

$$
\text{Attention}(Q,K,V) = \text{softmax}(\text{scores})V
$$
`torch.matmul(attn_weights, V)` å°±æ˜¯â€œæŠŠ weighted value åŠ èµ·æ¥â€ã€‚

---

### â‘¤ è¾“å‡ºæ˜ å°„

æœ€åå†åšä¸€æ¬¡çº¿æ€§æŠ•å½±ï¼š
$$
O = (\text{Attention}(Q,K,V)) W_O + b_O
$$
ä¿è¯è¾“å‡ºç»´åº¦ä»ç„¶æ˜¯ `d_model`ã€‚

---

### ğŸ“Š è¾“å‡ºç»“æœç¤ºä¾‹

```
è¾“å…¥å½¢çŠ¶: torch.Size([2, 4, 8])
è¾“å‡ºå½¢çŠ¶: torch.Size([2, 4, 8])
æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: torch.Size([2, 4, 4])
ç¬¬1ä¸ªæ ·æœ¬æ³¨æ„åŠ›çŸ©é˜µ:
 tensor([[0.234, 0.242, 0.278, 0.246],
         [0.262, 0.218, 0.264, 0.255],
         [0.261, 0.258, 0.244, 0.237],
         [0.260, 0.255, 0.254, 0.231]])
```

æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ª token å¯¹å…¶ä»– token çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚

---

### å•å¤´æ³¨æ„åŠ›å®ç°


| é¡¹ç›®       | ç”¨ `nn.Linear` | ç”¨ `nn.Parameter` |
| -------- | ------------- | ---------------- |
| æ˜¯å¦è‡ªåŠ¨æ³¨å†Œå‚æ•° | âœ…             | âœ…ï¼ˆæ‰‹åŠ¨å®šä¹‰ï¼‰          |
| æ˜¯å¦è‡ªå¸¦å‰å‘é€»è¾‘ | âœ…             | âŒï¼ˆéœ€æ‰‹å†™ matmulï¼‰    |
| åˆå§‹åŒ–      | è‡ªåŠ¨ï¼ˆXavierï¼‰    | éœ€æ‰‹åŠ¨              |
| ä»£ç é‡      | å°‘             | å¤š                |
| é€æ˜åº¦      | ä¸€å±‚å°è£…          | å®Œå…¨æ˜¾å¼ï¼Œæ›´ç›´è§‚         |


#### å®ç°æ–¹å¼ä¸€ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SingleHeadSelfAttention_Manual(nn.Module):
    def __init__(self, d_model):
        """
        åªç”¨ nn.Parameter å®ç°çš„å•å¤´è‡ªæ³¨æ„åŠ›å±‚
        å‚æ•°:
            d_model: æ¯ä¸ª token çš„ç‰¹å¾ç»´åº¦
        """
        super().__init__()
        self.d_model = d_model

        # ------------------------------
        # 1ï¸âƒ£ æ‰‹åŠ¨å®šä¹‰ Qã€Kã€V çš„æƒé‡çŸ©é˜µ (ä¸ä½¿ç”¨ nn.Linear)
        #    æƒé‡å½¢çŠ¶: (d_model, d_model)
        # ------------------------------
        self.W_Q = nn.Parameter(torch.randn(d_model, d_model))
        self.W_K = nn.Parameter(torch.randn(d_model, d_model))
        self.W_V = nn.Parameter(torch.randn(d_model, d_model))

        # 2ï¸âƒ£ è¾“å‡ºå±‚æƒé‡ (æŠŠæ³¨æ„åŠ›è¾“å‡ºæ˜ å°„å› d_model ç»´åº¦)
        self.W_O = nn.Parameter(torch.randn(d_model, d_model))

        # 3ï¸âƒ£ å¯é€‰åç½®
        self.b_Q = nn.Parameter(torch.zeros(d_model))
        self.b_K = nn.Parameter(torch.zeros(d_model))
        self.b_V = nn.Parameter(torch.zeros(d_model))
        self.b_O = nn.Parameter(torch.zeros(d_model))

        # åˆå§‹åŒ–ï¼ˆæ¨¡ä»¿ nn.Linear çš„ Xavier åˆå§‹åŒ–ï¼‰
        nn.init.xavier_uniform_(self.W_Q)
        nn.init.xavier_uniform_(self.W_K)
        nn.init.xavier_uniform_(self.W_V)
        nn.init.xavier_uniform_(self.W_O)

    def forward(self, x, mask=None):
        """
        å‚æ•°:
            x: (batch, seq_len, d_model)
            mask: (batch, seq_len, seq_len)ï¼Œå¯é€‰
        è¿”å›:
            out: (batch, seq_len, d_model)
            attn_weights: (batch, seq_len, seq_len)
        """
        batch, seq_len, d_model = x.shape
        d_k = d_model  # å•å¤´æ—¶ï¼Œd_k = d_model

        # ------------------------------
        # 4ï¸âƒ£ çº¿æ€§å˜æ¢: XW + b
        #    æ³¨æ„ï¼šx çš„å½¢çŠ¶ (batch, seq_len, d_model)
        #    æ‰€ä»¥è¦åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸ŠåšçŸ©é˜µä¹˜æ³•
        # ------------------------------
        Q = torch.matmul(x, self.W_Q) + self.b_Q     # (batch, seq_len, d_model)
        K = torch.matmul(x, self.W_K) + self.b_K     # (batch, seq_len, d_model)
        V = torch.matmul(x, self.W_V) + self.b_V     # (batch, seq_len, d_model)

        # ------------------------------
        # 5ï¸âƒ£ è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†: QK^T / sqrt(d_k)
        # ------------------------------
        # K^T éœ€è¦è½¬ç½®æœ€åä¸¤ä¸ªç»´åº¦ (seq_len, d_model) -> (d_model, seq_len)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)  # (batch, seq_len, seq_len)

        # ------------------------------
        # 6ï¸âƒ£ mask (å¦‚æœæœ‰)
        # ------------------------------
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        # ------------------------------
        # 7ï¸âƒ£ softmax å½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        # ------------------------------
        attn_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)

        # ------------------------------
        # 8ï¸âƒ£ åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º: Attention(Q,K,V) = softmax(QK^T)V
        # ------------------------------
        out = torch.matmul(attn_weights, V)  # (batch, seq_len, d_model)

        # ------------------------------
        # 9ï¸âƒ£ è¾“å‡ºçº¿æ€§å±‚ (æ‰‹åŠ¨å®ç°)
        # ------------------------------
        out = torch.matmul(out, self.W_O) + self.b_O  # (batch, seq_len, d_model)

        return out, attn_weights


# ==============================
# ğŸ”¹æµ‹è¯•
# ==============================
if __name__ == "__main__":
    torch.manual_seed(42)

    batch_size = 2
    seq_len = 4
    d_model = 8

    x = torch.randn(batch_size, seq_len, d_model)
    mask = torch.ones(batch_size, seq_len, seq_len).bool()  # å…¨éƒ¨å¯è§

    attn = SingleHeadSelfAttention_Manual(d_model)
    out, weights = attn(x, mask)

    print("è¾“å…¥å½¢çŠ¶:", x.shape)
    print("è¾“å‡ºå½¢çŠ¶:", out.shape)
    print("æ³¨æ„åŠ›æƒé‡å½¢çŠ¶:", weights.shape)
    print("ç¬¬1ä¸ªæ ·æœ¬æ³¨æ„åŠ›çŸ©é˜µ:\n", torch.round(weights[0], decimals=3))
```

#### å®ç°æ–¹å¼äºŒï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SingleHeadSelfAttention(nn.Module):
    def __init__(self, d_model):
        """
        å•å¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
        å‚æ•°:
            d_model: è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆå³æ¯ä¸ªtokençš„embeddingç»´åº¦ï¼‰
        """
        super().__init__()

        # å®šä¹‰çº¿æ€§å±‚ï¼Œç”¨äºç”Ÿæˆ Qã€Kã€V
        # æ¯ä¸ªçº¿æ€§å±‚ä¼šæŠŠè¾“å…¥ X æ˜ å°„åˆ°åŒæ ·ç»´åº¦ d_model
        # æ³¨æ„ï¼šå•å¤´æ²¡æœ‰åˆ†å¤´æ“ä½œï¼Œæ‰€ä»¥è¾“å‡ºç»´åº¦ä¸è¾“å…¥ç›¸åŒ
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)

        # è¾“å‡ºçº¿æ€§å±‚ï¼šå°†æ³¨æ„åŠ›ç»“æœå†æ˜ å°„å›åŸå§‹ç»´åº¦
        self.fc_out = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        """
        å‰å‘ä¼ æ’­
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)
            mask: æ³¨æ„åŠ›æ©ç ï¼ˆå¯é€‰ï¼‰ï¼Œå½¢çŠ¶å¯ä¸º (batch_size, seq_len, seq_len)
        è¿”å›:
            out: è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)
            attn_weights: æ³¨æ„åŠ›æƒé‡çŸ©é˜µ (batch_size, seq_len, seq_len)
        """
        batch_size, seq_len, d_model = x.size()

        # 1ï¸âƒ£ çº¿æ€§æ˜ å°„ç”Ÿæˆ Qã€Kã€V
        Q = self.W_Q(x)  # (batch, seq_len, d_model)
        K = self.W_K(x)  # (batch, seq_len, d_model)
        V = self.W_V(x)  # (batch, seq_len, d_model)

        # 2ï¸âƒ£ è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µ scores = Q * K^T / sqrt(d_k)
        # K.transpose(-2, -1) æŠŠ (batch, seq_len, d_model) -> (batch, d_model, seq_len)
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)  # (batch, seq_len, seq_len)

        # 3ï¸âƒ£ åº”ç”¨ maskï¼ˆå¦‚æœæä¾›ï¼‰ï¼Œç”¨äºå±è”½æ— æ•ˆä½ç½®ï¼ˆä¾‹å¦‚paddingæˆ–æœªæ¥tokenï¼‰
        if mask is not None:
            # maskä¸­ä¸º0çš„åœ°æ–¹è¢«å¡«å……ä¸º -infï¼Œä½¿softmaxåè¿™äº›ä½ç½®ä¸º0
            scores = scores.masked_fill(mask == 0, float('-inf'))

        # 4ï¸âƒ£ å¯¹æ¯ä¸ªqueryçš„å¾—åˆ†æ‰§è¡Œ softmaxï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)

        # 5ï¸âƒ£ å°†æ³¨æ„åŠ›æƒé‡ä½œç”¨åˆ°Vä¸Šï¼ˆå³â€œåŠ æƒæ±‚å’Œâ€ï¼‰
        out = torch.matmul(attn_weights, V)  # (batch, seq_len, d_model)

        # 6ï¸âƒ£ é€šè¿‡çº¿æ€§å±‚æ˜ å°„è¾“å‡ºï¼ˆå¯ç†è§£ä¸ºä¿¡æ¯æ•´åˆï¼‰
        out = self.fc_out(out)  # (batch, seq_len, d_model)

        return out, attn_weights


# ğŸ”¹ æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    torch.manual_seed(0)

    batch_size = 2     # æ‰¹æ¬¡å¤§å°
    seq_len = 4        # åºåˆ—é•¿åº¦
    d_model = 8        # æ¯ä¸ªtokençš„å‘é‡ç»´åº¦

    # éšæœºç”Ÿæˆè¾“å…¥æ•°æ® (batch, seq_len, d_model)
    x = torch.randn(batch_size, seq_len, d_model)

    # å¯é€‰maskï¼šå…¨éƒ¨å¯è§
    mask = torch.ones(batch_size, seq_len, seq_len).bool()

    # å®ä¾‹åŒ–æ¨¡å‹å¹¶å‰å‘ä¼ æ’­
    attn = SingleHeadSelfAttention(d_model)
    out, weights = attn(x, mask)

    print("è¾“å…¥å½¢çŠ¶:", x.shape)
    print("è¾“å‡ºå½¢çŠ¶:", out.shape)
    print("æ³¨æ„åŠ›æƒé‡å½¢çŠ¶:", weights.shape)
    print("æ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼ˆç¬¬1ä¸ªæ ·æœ¬ï¼‰:\n", torch.round(weights[0], decimals=3))
```

## 3.12 å¸¦æ©ç çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶


### âœ… å¸¦æ©ç çš„å•å¤´è‡ªæ³¨æ„åŠ›

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SingleHeadSelfAttentionWithMask(nn.Module):
    def __init__(self, d_model):
        """
        å•å¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¸¦æ©ç Maskï¼‰
        ä»…ä½¿ç”¨ nn.Parameter å®ç°ï¼Œä¸ä¾èµ– nn.Linear
        å‚æ•°:
            d_model: æ¯ä¸ª token çš„å‘é‡ç»´åº¦
        """
        super().__init__()
        self.d_model = d_model

        # å®šä¹‰å¯è®­ç»ƒæƒé‡å‚æ•°ï¼ˆç­‰ä»·äº nn.Linearï¼‰
        self.W_Q = nn.Parameter(torch.empty(d_model, d_model))
        self.W_K = nn.Parameter(torch.empty(d_model, d_model))
        self.W_V = nn.Parameter(torch.empty(d_model, d_model))
        self.W_O = nn.Parameter(torch.empty(d_model, d_model))

        # å¯é€‰åç½®
        self.b_Q = nn.Parameter(torch.zeros(d_model))
        self.b_K = nn.Parameter(torch.zeros(d_model))
        self.b_V = nn.Parameter(torch.zeros(d_model))
        self.b_O = nn.Parameter(torch.zeros(d_model))

        # åˆå§‹åŒ–æƒé‡ï¼ˆXavierï¼‰
        nn.init.xavier_uniform_(self.W_Q)
        nn.init.xavier_uniform_(self.W_K)
        nn.init.xavier_uniform_(self.W_V)
        nn.init.xavier_uniform_(self.W_O)

    def forward(self, x, mask=None):
        """
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ (batch, seq_len, d_model)
            mask: æ©ç å¼ é‡ (batch, seq_len, seq_len)
                  mask[i,j] = 0 è¡¨ç¤ºè¯¥ä½ç½®è¢«é®ä½ï¼›1 è¡¨ç¤ºå¯è§ã€‚
        è¿”å›:
            out: è¾“å‡ºå¼ é‡ (batch, seq_len, d_model)
            attn_weights: æ³¨æ„åŠ›æƒé‡ (batch, seq_len, seq_len)
        """
        batch, seq_len, d_model = x.shape
        d_k = d_model  # å•å¤´ï¼šd_k = d_model

        # 1ï¸âƒ£ è®¡ç®— Q, K, V
        Q = torch.matmul(x, self.W_Q) + self.b_Q  # (batch, seq_len, d_model)
        K = torch.matmul(x, self.W_K) + self.b_K
        V = torch.matmul(x, self.W_V) + self.b_V

        # 2ï¸âƒ£ è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µ
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)  # (batch, seq_len, seq_len)

        # 3ï¸âƒ£ åº”ç”¨æ©ç 
        if mask is not None:
            # mask == 0 çš„åœ°æ–¹è®¾ç½®ä¸º -infï¼Œè®© softmax åæ¦‚ç‡ä¸º 0
            scores = scores.masked_fill(mask == 0, float('-inf'))

        # 4ï¸âƒ£ softmax å½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)

        # 5ï¸âƒ£ åŠ æƒæ±‚å’Œå¾—åˆ°æ³¨æ„åŠ›è¾“å‡º
        out = torch.matmul(attn_weights, V)  # (batch, seq_len, d_model)

        # 6ï¸âƒ£ è¾“å‡ºçº¿æ€§å±‚ï¼ˆæ˜ å°„å›åŸç»´åº¦ï¼‰
        out = torch.matmul(out, self.W_O) + self.b_O  # (batch, seq_len, d_model)

        return out, attn_weights


# ===============================
# ğŸ”¹ æµ‹è¯•å¸¦æ©ç çš„æ³¨æ„åŠ›æœºåˆ¶
# ===============================
if __name__ == "__main__":
    torch.manual_seed(0)

    batch_size = 1
    seq_len = 5
    d_model = 8

    x = torch.randn(batch_size, seq_len, d_model)

    # ------------------------------
    # æ„é€ ä¸‹ä¸‰è§’â€œæœªæ¥æ©ç â€ï¼ˆå› æœæ©ç ï¼‰
    # ç¡®ä¿ç¬¬ i ä¸ªä½ç½®åªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„è¯
    # ------------------------------
    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).bool()
    # mask å½¢çŠ¶: (1, 5, 5)

    print("æ©ç çŸ©é˜µ:\n", mask[0].int())

    attention = SingleHeadSelfAttentionWithMask(d_model)
    out, weights = attention(x, mask=mask)

    print("\nè¾“å‡ºå½¢çŠ¶:", out.shape)
    print("æ³¨æ„åŠ›æƒé‡å½¢çŠ¶:", weights.shape)
    print("æ³¨æ„åŠ›çŸ©é˜µï¼ˆç¬¬1ä¸ªæ ·æœ¬ï¼‰:\n", torch.round(weights[0], decimals=3))
```

---

### ğŸ§  ä¸€æ­¥æ­¥è®²è§£ï¼š

#### â‘  æ©ç çš„ç›®çš„

æ©ç ï¼ˆmaskï¼‰ç”¨æ¥**å±è”½ä¸è¯¥è¢«çœ‹åˆ°çš„éƒ¨åˆ†**ï¼š

* **Padding Mask**ï¼šå±è”½æ‰ `<pad>` ä½ç½®ï¼›
* **Look-ahead Maskï¼ˆæœªæ¥æ©ç ï¼‰**ï¼šå±è”½æœªæ¥ tokenï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ²ã€‚

ä¸¾ä¾‹ï¼ˆlook-ahead maskï¼‰ï¼š

```
mask =
[[1, 0, 0, 0, 0],
 [1, 1, 0, 0, 0],
 [1, 1, 1, 0, 0],
 [1, 1, 1, 1, 0],
 [1, 1, 1, 1, 1]]
```

ç¬¬3ä¸ªè¯åªèƒ½çœ‹åˆ°å‰3ä¸ªï¼Œåé¢çš„å…¨æ˜¯0ï¼ˆè¢«é®ä½ï¼‰ã€‚

---

#### â‘¡ å…³é”®é€»è¾‘ï¼š`masked_fill`

```python
scores = scores.masked_fill(mask == 0, float('-inf'))
```

è¿™è¡Œçš„æ„æ€æ˜¯ï¼š

* åœ¨ mask ä¸º 0 çš„åœ°æ–¹ï¼ŒæŠŠæ³¨æ„åŠ›å¾—åˆ†è®¾æˆ `-âˆ`
* ç»è¿‡ `softmax` åï¼Œè¿™äº›ä½ç½®çš„æƒé‡å°±ä¼šå˜æˆ 0ï¼Œä¸å†å½±å“è¾“å‡ºã€‚

---

#### â‘¢ Softmax åçš„æ³¨æ„åŠ›çŸ©é˜µ

æ¯ä¸€è¡Œï¼ˆå¯¹åº”ä¸€ä¸ª tokenï¼‰éƒ½ä¼šè¢«å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚
åœ¨æœ‰ mask çš„æƒ…å†µä¸‹ï¼Œè¢«é®æ‰çš„åˆ—å…¨æ˜¯ 0ã€‚

ä¾‹å¦‚ï¼ˆå‡è®¾ seq_len=4ï¼‰ï¼š

```
mask =
[[1, 0, 0, 0],
 [1, 1, 0, 0],
 [1, 1, 1, 0],
 [1, 1, 1, 1]]
```

åˆ™æ³¨æ„åŠ›æƒé‡çŸ©é˜µä¸­ï¼š

* ç¬¬ä¸€è¡Œåªå¯¹ç¬¬1ä¸ªä½ç½®æœ‰æƒé‡ï¼›
* ç¬¬äºŒè¡Œåªèƒ½çœ‹åˆ°å‰2ä¸ªï¼›
* ç¬¬ä¸‰è¡Œçœ‹åˆ°å‰ä¸‰ä¸ªï¼›
* ç¬¬å››è¡Œçœ‹åˆ°å…¨éƒ¨ã€‚

---

#### â‘£ è¾“å‡ºè§£é‡Š

è¿è¡Œç»“æœç±»ä¼¼ï¼š

```
æ©ç çŸ©é˜µ:
 tensor([[1, 0, 0, 0, 0],
         [1, 1, 0, 0, 0],
         [1, 1, 1, 0, 0],
         [1, 1, 1, 1, 0],
         [1, 1, 1, 1, 1]], dtype=torch.int32)

è¾“å‡ºå½¢çŠ¶: torch.Size([1, 5, 8])
æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: torch.Size([1, 5, 5])
æ³¨æ„åŠ›çŸ©é˜µï¼ˆç¬¬1ä¸ªæ ·æœ¬ï¼‰:
 tensor([[1.000, 0.000, 0.000, 0.000, 0.000],
         [0.501, 0.499, 0.000, 0.000, 0.000],
         [0.352, 0.328, 0.320, 0.000, 0.000],
         [0.266, 0.263, 0.236, 0.235, 0.000],
         [0.225, 0.230, 0.220, 0.171, 0.154]])
```

> å¯ä»¥çœ‹åˆ°ï¼Œéšç€è¡Œå·å¢åŠ ï¼ˆå¾€åçœ‹ï¼‰ï¼Œæ³¨æ„åŠ›â€œèƒ½çœ‹åˆ°â€çš„éƒ¨åˆ†é€æ¸å¢å¤šã€‚


### ğŸ“˜ å°ç»“

| æ­¥éª¤ | è¯´æ˜          | å¯¹åº”ä»£ç                                    |
| -- | ----------- | -------------------------------------- |
| 1  | è®¡ç®— Q,K,V    | `torch.matmul(x, self.W_Q)`            |
| 2  | ç‚¹ç§¯å¾—åˆ° scores | `torch.matmul(Q, K.transpose(-2, -1))` |
| 3  | åº”ç”¨æ©ç         | `scores.masked_fill(mask == 0, -inf)`  |
| 4  | softmax å½’ä¸€åŒ– | `F.softmax(scores, dim=-1)`            |
| 5  | åŠ æƒæ±‚å’Œ        | `torch.matmul(attn_weights, V)`        |
| 6  | è¾“å‡ºæ˜ å°„        | `torch.matmul(out, self.W_O)`          |

## 3.13 å¤šå¤´æ³¨æ„åŠ›


ç°åœ¨æˆ‘ä»¬æŠŠä¹‹å‰çš„ã€Œå•å¤´æ³¨æ„åŠ›ã€æ‰©å±•ä¸º **å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attention, MHAï¼‰**ã€‚
è¿™ä¸€æ­¥æ˜¯ Transformer çš„æ ¸å¿ƒåˆ›æ–°â€”â€”å¤šå¤´æœºåˆ¶è®©æ¨¡å‹èƒ½**ä»å¤šä¸ªå­ç©ºé—´åŒæ—¶è§‚å¯ŸåŒä¸€ä¸ªåºåˆ—çš„å…³ç³»**ã€‚

---


å®Œæ•´å®ç°ä¸€ä¸ª **Multi-Head Self-Attention** æ¨¡å—ï¼Œ**åªç”¨ `nn.Parameter`ï¼Œä¸ç”¨ `nn.Linear`**ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒæ­¥éª¤ï¼š

1. å¯¹è¾“å…¥ ( X ) åˆ†åˆ«ç”¨ç‹¬ç«‹æƒé‡ç”Ÿæˆå¤šå¤´çš„ Qã€Kã€V
2. æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
3. å°†æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥ï¼ˆconcatenateï¼‰
4. å†æ˜ å°„å›åŸå§‹ç»´åº¦ ( d_{model} )

---

### âœ… ä»£ç å®ç°ï¼ˆçº¯æ‰‹å†™ç‰ˆï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        """
        å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸ä½¿ç”¨ nn.Linearï¼‰
        å‚æ•°:
            d_model: è¾“å…¥ç‰¹å¾ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´çš„æ•°é‡
        """
        super().__init__()
        assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

        # ---- å¯è®­ç»ƒå‚æ•° ----
        # æ¯ä¸ª Q/K/V æŠ•å½±éƒ½æœ‰è‡ªå·±çš„ä¸€ç»„æƒé‡ï¼ˆå…±äº«åœ¨æ‰€æœ‰å¤´ï¼‰
        self.W_Q = nn.Parameter(torch.randn(d_model, d_model))
        self.W_K = nn.Parameter(torch.randn(d_model, d_model))
        self.W_V = nn.Parameter(torch.randn(d_model, d_model))

        # è¾“å‡ºæ˜ å°„æƒé‡ï¼ˆæ‹¼æ¥åçš„çº¿æ€§å˜æ¢ï¼‰
        self.W_O = nn.Parameter(torch.randn(d_model, d_model))

        # åç½®é¡¹
        self.b_Q = nn.Parameter(torch.zeros(d_model))
        self.b_K = nn.Parameter(torch.zeros(d_model))
        self.b_V = nn.Parameter(torch.zeros(d_model))
        self.b_O = nn.Parameter(torch.zeros(d_model))

        # åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.W_Q)
        nn.init.xavier_uniform_(self.W_K)
        nn.init.xavier_uniform_(self.W_V)
        nn.init.xavier_uniform_(self.W_O)

    def forward(self, x, mask=None):
        """
        å‚æ•°:
            x: (batch, seq_len, d_model)
            mask: (batch, 1, seq_len, seq_len)ï¼Œå¯é€‰
        è¿”å›:
            out: (batch, seq_len, d_model)
            attn_weights: (batch, num_heads, seq_len, seq_len)
        """
        batch_size, seq_len, _ = x.size()

        # 1ï¸âƒ£ ç”Ÿæˆ Q, K, V
        Q = torch.matmul(x, self.W_Q) + self.b_Q   # (batch, seq_len, d_model)
        K = torch.matmul(x, self.W_K) + self.b_K
        V = torch.matmul(x, self.W_V) + self.b_V

        # 2ï¸âƒ£ æ‹†åˆ†å¤šå¤´
        # ç»´åº¦æ‹†åˆ†åå½¢çŠ¶: (batch, num_heads, seq_len, d_k)
        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

        # 3ï¸âƒ£ è®¡ç®—æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)  # (batch, heads, seq_len, seq_len)

        # 4ï¸âƒ£ åº”ç”¨æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
        if mask is not None:
            # mask åº”è¯¥å¯å¹¿æ’­åˆ° (batch, heads, seq_len, seq_len)
            scores = scores.masked_fill(mask == 0, float('-inf'))

        # 5ï¸âƒ£ softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = F.softmax(scores, dim=-1)

        # 6ï¸âƒ£ å¯¹ V åŠ æƒæ±‚å’Œ
        head_outputs = torch.matmul(attn_weights, V)  # (batch, heads, seq_len, d_k)

        # 7ï¸âƒ£ æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
        head_outputs = head_outputs.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

        # 8ï¸âƒ£ æœ€åçš„è¾“å‡ºæŠ•å½±
        out = torch.matmul(head_outputs, self.W_O) + self.b_O  # (batch, seq_len, d_model)

        return out, attn_weights


# ==============================
# ğŸ”¹ æµ‹è¯•ç¤ºä¾‹
# ==============================
if __name__ == "__main__":
    torch.manual_seed(42)

    batch = 2
    seq_len = 5
    d_model = 16
    num_heads = 4

    x = torch.randn(batch, seq_len, d_model)

    # ä¸‹ä¸‰è§’ mask (å› æœæ©ç )
    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).bool()

    mha = MultiHeadSelfAttention(d_model, num_heads)
    out, attn = mha(x, mask)

    print("è¾“å…¥å½¢çŠ¶:", x.shape)
    print("è¾“å‡ºå½¢çŠ¶:", out.shape)
    print("æ³¨æ„åŠ›æƒé‡å½¢çŠ¶:", attn.shape)
    print("ç¬¬ä¸€ä¸ªæ ·æœ¬ç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›çŸ©é˜µ:\n", torch.round(attn[0, 0], decimals=3))
```

---

### ğŸ“Š è¾“å‡ºç¤ºä¾‹

```
è¾“å…¥å½¢çŠ¶: torch.Size([2, 5, 16])
è¾“å‡ºå½¢çŠ¶: torch.Size([2, 5, 16])
æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: torch.Size([2, 4, 5, 5])
ç¬¬ä¸€ä¸ªæ ·æœ¬ç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›çŸ©é˜µ:
 tensor([[1.000, 0.000, 0.000, 0.000, 0.000],
         [0.487, 0.513, 0.000, 0.000, 0.000],
         [0.344, 0.320, 0.336, 0.000, 0.000],
         [0.253, 0.251, 0.238, 0.259, 0.000],
         [0.214, 0.221, 0.205, 0.185, 0.175]])
```

è§£é‡Šï¼š

* æ³¨æ„åŠ›æƒé‡ shape = `(batch, num_heads, seq_len, seq_len)`
* æ¯ä¸ªå¤´çœ‹åˆ°çš„æ³¨æ„åŠ›åˆ†å¸ƒä¸åŒ
* æ©ç ç”Ÿæ•ˆï¼šä¸Šä¸‰è§’éƒ¨åˆ†å…¨æ˜¯ 0ï¼ˆä¸å¯è§ï¼‰

---

### ğŸ§  æ¦‚å¿µå›é¡¾ï¼šå¤šå¤´çš„å¥½å¤„

å•å¤´æ³¨æ„åŠ›åªå­¦ä¹ ä¸€ç§â€œç›¸å…³æ€§æ¨¡å¼â€ï¼›
å¤šå¤´æ³¨æ„åŠ›æŠŠä¿¡æ¯åˆ†æˆå¤šä¸ªå­ç©ºé—´ï¼Œæ¯ä¸ªå¤´éƒ½æœ‰è‡ªå·±çš„ä¸€å¥—æƒé‡ ( W_Q^h, W_K^h, W_V^h )ï¼Œ
å®ƒä»¬èƒ½ï¼š

* ä¸€å¤´å…³æ³¨**è¯­ä¹‰ç›¸ä¼¼åº¦**ï¼›
* ä¸€å¤´å…³æ³¨**å¥æ³•ç»“æ„**ï¼›
* ä¸€å¤´å…³æ³¨**ä½ç½®ä¾èµ–**ï¼›
* ä¸€å¤´æ•æ‰**é•¿ç¨‹ä¾èµ–**ã€‚

è¿™æ ·èƒ½æ›´å…¨é¢åœ°å»ºæ¨¡åºåˆ—çš„å…¨å±€å…³ç³»ã€‚

---

### ğŸ§© å½¢çŠ¶æ€»ç»“è¡¨

| å¼ é‡             | å½¢çŠ¶                                   | å«ä¹‰              |
| -------------- | ------------------------------------ | --------------- |
| `x`            | (batch, seq_len, d_model)            | è¾“å…¥åºåˆ—            |
| `Q, K, V`      | (batch, seq_len, d_model)            | æŠ•å½±åçš„è¡¨ç¤º          |
| `Q/K/V` æ‹†å¤´     | (batch, num_heads, seq_len, d_k)     | æ¯ä¸ªå¤´çš„è¡¨ç¤º          |
| `scores`       | (batch, num_heads, seq_len, seq_len) | æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µ         |
| `attn_weights` | åŒä¸Š                                   | softmax å½’ä¸€åŒ–åçš„æƒé‡ |
| `head_outputs` | (batch, num_heads, seq_len, d_k)     | æ¯ä¸ªå¤´çš„è¾“å‡º          |
| `out`          | (batch, seq_len, d_model)            | æ‹¼æ¥å¹¶çº¿æ€§æ˜ å°„åçš„ç»“æœ     |

---

### ğŸ”§ å°ç»“

æ ¸å¿ƒé€»è¾‘æ˜¯è¿™ä¸€è¡Œï¼š

```python
out = softmax(Q @ K^T / sqrt(d_k)) @ V
```

å¤šå¤´åªæ˜¯ï¼š

* æŠŠ Qã€Kã€V æ‹†æˆå¤šä¸ªå­ç©ºé—´ï¼›
* å¹¶è¡Œè®¡ç®—ï¼›
* å†æ‹¼æ¥ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ åŠ ä¸Š**æ³¨æ„åŠ›å¯è§†åŒ–å‡½æ•°**ï¼ˆå±•ç¤ºæ¯ä¸ªå¤´çš„æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼‰ï¼Œ
è®©ä½ ç›´è§‚çœ‹åˆ°ä¸åŒ head çš„å…³æ³¨ç„¦ç‚¹å·®å¼‚ã€‚
è¦æˆ‘æŠŠè¿™ä¸ªç‰ˆæœ¬åŠ ä¸Šå¯è§†åŒ–è¾“å‡ºå—ï¼Ÿ
