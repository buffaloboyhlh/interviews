# ğŸ§© ç¬¬ 1 ç« ï¼šè®¤è¯† NLTK

### ä»€ä¹ˆæ˜¯ NLTKï¼Ÿ

NLTKï¼ˆNatural Language Toolkitï¼‰æ˜¯ Python æœ€ç»å…¸çš„ NLP æ•™å­¦ä¸å®éªŒåº“ã€‚å®ƒæä¾›äº†ï¼š

* ä¸Šç™¾ä¸ªè¯­æ–™åº“ï¼ˆcorpusï¼‰å’Œè¯å…¸èµ„æºï¼ˆå¦‚ WordNetï¼‰ï¼›
* å„ç±»æ–‡æœ¬å¤„ç†å·¥å…·ï¼ˆåˆ†è¯ã€æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ç­‰ï¼‰ï¼›
* ç»Ÿè®¡æ¨¡å‹å’Œåˆ†ç±»ç®—æ³•ï¼ˆæœ´ç´ è´å¶æ–¯ã€å†³ç­–æ ‘ç­‰ï¼‰ï¼›
* å¼ºå¤§çš„å¯è§†åŒ–åŠŸèƒ½ï¼ˆå¥æ³•æ ‘ã€é¢‘ç‡åˆ†å¸ƒç­‰ï¼‰ã€‚

ç®€è€Œè¨€ä¹‹ï¼ŒNLTK å°±åƒæ˜¯ NLP çš„â€œç§¯æœ¨ç›’â€â€”â€”ä½ å¯ä»¥ç”¨å®ƒæ‹¼å‡ºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿã€‚

### å®‰è£…ä¸é…ç½®

```bash
pip install nltk
```

ç¬¬ä¸€æ¬¡ä½¿ç”¨ï¼š

```python
import nltk
nltk.download('all')  # æˆ–æŒ‰éœ€ä¸‹è½½éƒ¨åˆ†èµ„æº
```

---

# ğŸ§© ç¬¬ 2 ç« ï¼šæ–‡æœ¬é¢„å¤„ç†ï¼ˆText Preprocessingï¼‰

è¯­è¨€æ¨¡å‹çš„è¾“å…¥å¿…é¡»æ˜¯å¹²å‡€ã€ç»“æ„åŒ–çš„æ–‡æœ¬ã€‚
è€ŒåŸå§‹æ–‡æœ¬é€šå¸¸å……æ»¡å™ªéŸ³ï¼šæ ‡ç‚¹ç¬¦å·ã€HTML æ ‡ç­¾ã€å¤§å°å†™ä¸ç»Ÿä¸€â€¦â€¦
è¿™ä¸€æ­¥çš„ç›®æ ‡æ˜¯æŠŠè‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºâ€œè®¡ç®—æœºèƒ½ç†è§£çš„è¯åºåˆ—â€ã€‚

### 2.1 åˆ†è¯ï¼ˆTokenizationï¼‰

```python
from nltk.tokenize import word_tokenize, sent_tokenize

text = "Natural Language Processing lets computers understand human language."
print(sent_tokenize(text))
print(word_tokenize(text))
```

è¾“å‡ºï¼š

```
['Natural Language Processing lets computers understand human language.']
['Natural', 'Language', 'Processing', 'lets', 'computers', 'understand', 'human', 'language', '.']
```

### 2.2 åœç”¨è¯ï¼ˆStopwordsï¼‰

åœç”¨è¯æŒ‡å¦‚ â€œtheâ€ã€â€œisâ€ã€â€œandâ€ è¿™ç±»è¯­ä¹‰è´¡çŒ®å¾ˆå°çš„å¸¸ç”¨è¯ã€‚

```python
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))
tokens = [w for w in word_tokenize(text.lower()) if w.isalpha() and w not in stop_words]
print(tokens)
```

### 2.3 è¯å¹²æå–ï¼ˆStemmingï¼‰ä¸è¯å½¢è¿˜åŸï¼ˆLemmatizationï¼‰

ä¸¤ç§è¯å½¢æ ‡å‡†åŒ–æ–¹å¼ï¼š

```python
from nltk.stem import PorterStemmer, WordNetLemmatizer

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

print(stemmer.stem("studies"))     # stem: study
print(lemmatizer.lemmatize("studies", pos='v'))  # lemma: study
```

---

# ğŸ§© ç¬¬ 3 ç« ï¼šè¯æ€§æ ‡æ³¨ï¼ˆPOS Taggingï¼‰

è¯æ€§æ ‡æ³¨è®©è®¡ç®—æœºçŸ¥é“æ¯ä¸ªè¯åœ¨å¥å­ä¸­çš„è¯­æ³•åŠŸèƒ½ã€‚

```python
import nltk
sentence = "The quick brown fox jumps over the lazy dog."
tokens = nltk.word_tokenize(sentence)
tagged = nltk.pos_tag(tokens)
print(tagged)
```

è¾“å‡ºï¼š

```
[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ...]
```

å¸¸è§æ ‡ç­¾ï¼š

* NNï¼šåè¯
* JJï¼šå½¢å®¹è¯
* VBï¼šåŠ¨è¯
* RBï¼šå‰¯è¯

NLTK çš„è¯æ€§æ ‡æ³¨å™¨åº•å±‚æ˜¯ä¸€ä¸ª **éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰**ã€‚

---

# ğŸ§© ç¬¬ 4 ç« ï¼šå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰

NER = Named Entity Recognitionï¼Œç”¨æ¥è¯†åˆ«æ–‡æœ¬ä¸­çš„å®ä½“ï¼Œå¦‚äººåã€åœ°ç‚¹ã€ç»„ç»‡ç­‰ã€‚

```python
from nltk import ne_chunk

sentence = "Elon Musk founded SpaceX in California."
tokens = nltk.word_tokenize(sentence)
tags = nltk.pos_tag(tokens)
tree = ne_chunk(tags)
print(tree)
```

è¾“å‡ºæ ‘ä¸­ä¼šæ ‡æ³¨å®ä½“ç±»åˆ«ï¼Œå¦‚ï¼š

```
(ORGANIZATION SpaceX)
(GPE California)
(PERSON Elon Musk)
```

---

# ğŸ§© ç¬¬ 5 ç« ï¼šå¥æ³•åˆ†æï¼ˆParsingï¼‰

å¥æ³•åˆ†ææ˜¯è¯­è¨€ç†è§£çš„â€œç»“æ„â€éƒ¨åˆ†ã€‚

### ä¾‹ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•ï¼ˆCFGï¼‰

```python
from nltk import CFG
from nltk.parse import ChartParser

grammar = CFG.fromstring("""
S -> NP VP
NP -> DT NN
VP -> VB NP
DT -> 'the'
NN -> 'cat' | 'dog'
VB -> 'chased' | 'saw'
""")

parser = ChartParser(grammar)
sentence = ['the', 'dog', 'chased', 'the', 'cat']

for tree in parser.parse(sentence):
    print(tree)
    tree.draw()
```

è¿™ä¼šç»˜åˆ¶å‡ºä¸€æ£µå¥æ³•æ ‘ï¼Œè®©ä½ çœ‹åˆ°â€œä¸»è¯­-è°“è¯­-å®¾è¯­â€çš„ç»“æ„ã€‚

---

# ğŸ§© ç¬¬ 6 ç« ï¼šè¯­ä¹‰åˆ†æï¼ˆWordNetï¼‰

NLTK å†…ç½® WordNet â€”â€” ä¸€ä¸ªåºå¤§çš„è‹±è¯­è¯­ä¹‰ç½‘ã€‚

```python
from nltk.corpus import wordnet as wn

word = 'car'
synsets = wn.synsets(word)
print(synsets[0].definition())       # å®šä¹‰
print(synsets[0].lemmas())           # åŒä¹‰è¯
print(synsets[0].hypernyms())        # ä¸Šä½è¯
print(synsets[0].hyponyms())         # ä¸‹ä½è¯
```

ä½ å¯ä»¥åˆ©ç”¨ WordNet å®ç°ï¼š

* **åŒä¹‰è¯æ‰©å±•**ï¼ˆsynonym expansionï¼‰
* **è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—**
* **æ¦‚å¿µå±‚çº§åˆ†æ**

---

# ğŸ§© ç¬¬ 7 ç« ï¼šæ–‡æœ¬ç‰¹å¾æå–ä¸åˆ†ç±»

### 7.1 è¯é¢‘ç»Ÿè®¡

```python
from nltk import FreqDist

text = "Data science is the study of data using statistics and machine learning."
tokens = word_tokenize(text.lower())
fdist = FreqDist(tokens)
print(fdist.most_common(5))
fdist.plot(20)
```

### 7.2 ç®€å•æ–‡æœ¬åˆ†ç±»ï¼ˆæœ´ç´ è´å¶æ–¯ï¼‰

ä½¿ç”¨ NLTK è‡ªå¸¦çš„ç”µå½±è¯„è®ºè¯­æ–™ï¼š

```python
from nltk.corpus import movie_reviews
from nltk.classify import NaiveBayesClassifier

def extract_features(words):
    return {word: True for word in words}

data = [(extract_features(movie_reviews.words(fileid)), category)
        for category in movie_reviews.categories()
        for fileid in movie_reviews.fileids(category)]

train, test = data[:1900], data[1900:]
classifier = NaiveBayesClassifier.train(train)
print("Accuracy:", nltk.classify.accuracy(classifier, test))
classifier.show_most_informative_features(10)
```

---

# ğŸ§© ç¬¬ 8 ç« ï¼šè¿›é˜¶ä¸ç»¼åˆåº”ç”¨

### 8.1 æ­é…åˆ†æï¼ˆCollocationsï¼‰

æ‰¾å‡ºè¯è¯­æ­é…é¢‘ç¹çš„ç»„åˆï¼š

```python
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures

tokens = word_tokenize("She sells sea shells by the sea shore.")
finder = BigramCollocationFinder.from_words(tokens)
print(finder.nbest(BigramAssocMeasures.likelihood_ratio, 5))
```

### 8.2 å…³é”®è¯æå–ä¸å…±ç°ç½‘ç»œ

ä½ å¯ä»¥åŸºäºè¯é¢‘ã€TF-IDFã€äº’ä¿¡æ¯ï¼ˆPMIï¼‰æ„å»ºè¯å›¾ï¼Œæ¢ç´¢è¯­ä¹‰ç»“æ„ã€‚

### 8.3 ä¸»é¢˜å»ºæ¨¡ï¼ˆLDAï¼‰

è™½ç„¶ NLTK ä¸ç›´æ¥å®ç° LDAï¼Œä½†ä½ å¯ä»¥ç”¨å®ƒé¢„å¤„ç†æ–‡æœ¬ï¼Œç„¶åäº¤ç»™ gensim è®­ç»ƒä¸»é¢˜æ¨¡å‹ã€‚


# ğŸ§© ç¬¬ 9 ç« ï¼šæƒ…æ„Ÿåˆ†æï¼ˆSentiment Analysisï¼‰

æƒ…æ„Ÿåˆ†ææ˜¯ NLP çš„ç»å…¸ä»»åŠ¡ä¹‹ä¸€ã€‚ç›®æ ‡æ˜¯åˆ¤æ–­æ–‡æœ¬çš„æƒ…ç»ªææ€§ï¼ˆæ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§ï¼‰ã€‚

NLTK æä¾›äº†ä¸¤æ¡è·¯å¾„ï¼š

1. ç”¨è‡ªå¸¦è¯­æ–™ + ä¼ ç»Ÿåˆ†ç±»å™¨ï¼ˆå¦‚æœ´ç´ è´å¶æ–¯ï¼‰
2. ç”¨å†…ç½®çš„ VADER æ¨¡å‹ï¼ˆä¸“ä¸ºç¤¾äº¤åª’ä½“çŸ­æ–‡æœ¬ä¼˜åŒ–ï¼‰

---

## 9.1 ç”¨ç”µå½±è¯„è®ºè¯­æ–™è®­ç»ƒæœ´ç´ è´å¶æ–¯æƒ…æ„Ÿåˆ†ç±»å™¨

```python
import nltk
from nltk.corpus import movie_reviews
from nltk.classify import NaiveBayesClassifier
from nltk.classify.util import accuracy

def extract_features(words):
    return {word: True for word in words}

# åŠ è½½è¯­æ–™åº“
documents = [(extract_features(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]

train_set, test_set = documents[:1900], documents[1900:]
classifier = NaiveBayesClassifier.train(train_set)

print("Accuracy:", accuracy(classifier, test_set))
classifier.show_most_informative_features(10)
```

è¾“å‡ºç¤ºä¾‹ï¼š

```
Accuracy: 0.82
Most Informative Features
   outstanding = True           pos : neg = 9.0 : 1.0
   awful = True                 neg : pos = 8.0 : 1.0
   ...
```

**åŸç†ç®€è¿°**ï¼š
æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åŸºäºè¯è¯­çš„æ¡ä»¶æ¦‚ç‡ï¼š
[
P(\text{label}|\text{words}) \propto P(\text{label}) \times \prod_i P(w_i|\text{label})
]
å‡è®¾è¯è¯­ç‹¬ç«‹ï¼ˆè¿™æ˜¯â€œæœ´ç´ â€çš„åœ°æ–¹ï¼‰ï¼Œåœ¨å¤§è¯­æ–™ä¸Šæ•ˆæœä»ç„¶æƒŠäººåœ°ç¨³å¥ã€‚

---

## 9.2 ä½¿ç”¨ VADER åšç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æ

VADERï¼ˆValence Aware Dictionary for sEntiment Reasoningï¼‰æ˜¯ä¸€ä¸ª**åŸºäºè¯å…¸ + è§„åˆ™**çš„æƒ…æ„Ÿåˆ†æå™¨ï¼Œå°¤å…¶æ“…é•¿å¤„ç†å¸¦è¡¨æƒ…ç¬¦å·ã€ç¼©å†™ã€æ„Ÿå¹å·çš„çŸ­æ–‡æœ¬ï¼ˆå¦‚æ¨ç‰¹ã€è¯„è®ºï¼‰ã€‚

```python
from nltk.sentiment import SentimentIntensityAnalyzer

sia = SentimentIntensityAnalyzer()

sentences = [
    "I love this movie! It's amazing ğŸ˜Š",
    "The plot was terrible and boring...",
    "Not bad, but not great either."
]

for s in sentences:
    print(s, "â†’", sia.polarity_scores(s))
```

è¾“å‡ºï¼š

```
I love this movie! ... â†’ {'neg': 0.0, 'neu': 0.3, 'pos': 0.7, 'compound': 0.85}
The plot was terrible ... â†’ {'neg': 0.6, 'neu': 0.4, 'pos': 0.0, 'compound': -0.78}
```

`compound` åˆ†æ•°åœ¨ [-1, 1] é—´ï¼Œè¡¨ç¤ºæ•´ä½“æƒ…ç»ªå¼ºåº¦ã€‚
VADER ä¸éœ€è¦è®­ç»ƒï¼Œé€‚åˆå¿«é€Ÿåˆ†ææ¨ç‰¹ã€è¯„è®ºã€å¼¹å¹•ç­‰æ–‡æœ¬ã€‚

---

## 9.3 ç”¨ NLTK æ„å»ºæƒ…æ„Ÿè¯å…¸

å¦‚æœæƒ³è‡ªå®šä¹‰æƒ…æ„Ÿåˆ†æè§„åˆ™ï¼Œä½ å¯ä»¥ä» WordNet æ„å»ºè‡ªå·±çš„**æƒ…ç»ªè¯å…¸**ã€‚

```python
from nltk.corpus import wordnet as wn

positive = ['good', 'happy', 'excellent', 'fortunate', 'correct', 'superior']
negative = ['bad', 'sad', 'terrible', 'poor', 'wrong', 'inferior']

def get_synonyms(word):
    syns = set()
    for s in wn.synsets(word):
        for lemma in s.lemmas():
            syns.add(lemma.name())
    return syns

pos_lexicon = set()
neg_lexicon = set()
for w in positive: pos_lexicon.update(get_synonyms(w))
for w in negative: neg_lexicon.update(get_synonyms(w))

print("Positive words:", list(pos_lexicon)[:10])
print("Negative words:", list(neg_lexicon)[:10])
```

ä½ å¯ä»¥åŸºäºè¿™äº›è¯é›†åˆåšè¯é¢‘ç»Ÿè®¡æˆ– TF-IDF è®¡ç®—ï¼Œä»è€Œæ‰‹å·¥æ„é€ â€œæƒ…æ„Ÿå¾—åˆ†â€ã€‚

---

# ğŸ§© ç¬¬ 10 ç« ï¼šNLTK ä¸ç°ä»£ NLPï¼ˆBERT / GPT çš„è¡”æ¥ï¼‰

NLTK æ˜¯ä¼ ç»Ÿ NLP çš„åŸºçŸ³ï¼Œ
è€Œç°ä»£ NLP æ¨¡å‹ï¼ˆå¦‚ BERTã€GPTã€T5ï¼‰ä»£è¡¨çš„æ˜¯**æ·±åº¦è¯­ä¹‰ç†è§£çš„æ–°æ—¶ä»£**ã€‚
è¿™ä¸€ç« è®²å¦‚ä½•æŠŠ NLTK çš„æ•°æ®ç®¡çº¿ä¸æ·±åº¦æ¨¡å‹ç»“åˆã€‚

---

## 10.1 ä½¿ç”¨ NLTK åšå‰å¤„ç†ï¼Œé€å…¥ Transformer æ¨¡å‹

```python
from nltk.tokenize import word_tokenize
from transformers import pipeline

# 1. ç”¨ NLTK åšåˆ†è¯ã€æ¸…æ´—
text = "Natural language processing is fascinating but complex!"
tokens = [w.lower() for w in word_tokenize(text) if w.isalpha()]
print("Cleaned tokens:", tokens)

# 2. ç”¨ç°ä»£æ¨¡å‹åˆ†æ
analyzer = pipeline("sentiment-analysis")
print(analyzer(text))
```

è¾“å‡ºï¼š

```
Cleaned tokens: ['natural', 'language', 'processing', 'is', 'fascinating', 'but', 'complex']
[{'label': 'POSITIVE', 'score': 0.9998}]
```

NLTK è´Ÿè´£**é¢„å¤„ç†ä¸ç‰¹å¾åŒ–**ï¼›Transformers è´Ÿè´£**è¯­ä¹‰å»ºæ¨¡ä¸æ¨ç†**ã€‚
è¿™æ­£æ˜¯ç°ä»£ NLP çš„æœ€ä½³å®è·µã€‚

---

## 10.2 ä»è¯è¢‹åˆ°è¯å‘é‡çš„æ¼”è¿›

NLTK ä¸»è¦å¤„ç†**ç¬¦å·çº§æ–‡æœ¬**ï¼šå•è¯ã€æ ‡ç‚¹ã€å¥æ³•æ ‘ã€‚
ä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éœ€è¦**æ•°å€¼åŒ–è¡¨ç¤º**ï¼ˆword embeddingsï¼‰ã€‚

è¯è¢‹æ¨¡å‹ï¼ˆBag-of-Words, BoWï¼‰ â†’ Word2Vec â†’ GloVe â†’ Transformer Embeddings
è¿™æ˜¯ä¸€æ¡â€œä»ç»Ÿè®¡åˆ°è¯­ä¹‰â€çš„æ¼”åŒ–è·¯å¾„ã€‚

å¯ä»¥ç”¨ NLTK æ„é€  BoW ç‰¹å¾ï¼š

```python
from nltk import FreqDist
from sklearn.feature_extraction.text import CountVectorizer

corpus = ["I love NLP", "NLP loves deep learning"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names_out())
print(X.toarray())
```

è¾“å‡ºï¼š

```
['deep' 'learning' 'love' 'loves' 'nlp']
[[0 0 1 0 1]
 [1 1 0 1 1]]
```

---

## 10.3 è¯­ä¹‰ç›¸ä¼¼åº¦ï¼šç”¨ WordNet + é¢„è®­ç»ƒæ¨¡å‹ç»“åˆ

```python
from nltk.corpus import wordnet as wn

dog = wn.synset('dog.n.01')
cat = wn.synset('cat.n.01')
car = wn.synset('car.n.01')

print("dog-cat:", dog.wup_similarity(cat))
print("dog-car:", dog.wup_similarity(car))
```

è¾“å‡ºï¼š

```
dog-cat: 0.857
dog-car: 0.6
```

WordNet çš„ç›¸ä¼¼åº¦åŸºäºè¯­ä¹‰å±‚çº§è·ç¦»ã€‚
è‹¥é…åˆ BERT è¯åµŒå…¥ï¼ˆtransformersï¼‰ï¼Œä½ å¯ä»¥æ„å»ºæ··åˆæ¨¡å‹ï¼Œå®ç°ï¼š

* æ¦‚å¿µå±‚çº§çš„æ¨ç†ï¼ˆWordNetï¼‰
* è¯­å¢ƒåŒ–è¯­ä¹‰è·ç¦»ï¼ˆBERT Embeddingï¼‰

---

## 10.4 ç°ä»£ NLP ä¸ NLTK çš„å“²å­¦å·®åˆ«

| å±‚é¢   | NLTK              | Transformers   |
| ---- | ----------------- | -------------- |
| æ ¸å¿ƒæ€æƒ³ | è¯­è¨€è§„åˆ™ + ç»Ÿè®¡         | è¯­ä¹‰è¡¨å¾ + ç¥ç»ç½‘ç»œ    |
| æ•°æ®å½¢å¼ | ç¬¦å·ï¼ˆtokens, treesï¼‰ | å‘é‡ï¼ˆembeddingsï¼‰ |
| å¯è§£é‡Šæ€§ | å¼º                 | å¼±              |
| ç²¾åº¦   | ä¸­                 | æé«˜             |
| é€‚ç”¨åœºæ™¯ | æ•™å­¦ã€å¯è§†åŒ–ã€åŸºç¡€NLP      | å•†ä¸šåº”ç”¨ã€å¤§æ¨¡å‹å¾®è°ƒ     |

NLTK æ•™ä½ ç†è§£è¯­è¨€çš„ç»“æ„ï¼›
Transformer è®©æœºå™¨â€œæ„Ÿå—â€è¯­è¨€çš„è¯­ä¹‰ã€‚
äºŒè€…ç»“åˆï¼Œæ‰æ˜¯çœŸæ­£çš„â€œç²¾é€š NLPâ€ã€‚

---

## ğŸ“ å®Œæ•´å­¦ä¹ å»ºè®®è·¯çº¿

1. **æŒæ¡è¯­è¨€å­¦åŸºç¡€ï¼š**
   åˆ†è¯ â†’ è¯æ€§æ ‡æ³¨ â†’ å¥æ³•æ ‘
2. **ç†è§£ç»Ÿè®¡ NLPï¼š**
   æœ´ç´ è´å¶æ–¯ã€TF-IDFã€æ­é…åˆ†æ
3. **å­¦ä¹ è¯­ä¹‰ç½‘ç»œï¼š**
   WordNetã€ç›¸ä¼¼åº¦è®¡ç®—ã€åŒä¹‰è¯æ‰©å±•
4. **è¿‡æ¸¡åˆ°æ·±åº¦æ¨¡å‹ï¼š**
   äº†è§£ word2vecã€Transformerã€BERT
5. **ç»¼åˆå®è·µé¡¹ç›®ï¼š**
   æƒ…æ„Ÿåˆ†æã€å…³é”®è¯æå–ã€ä¸»é¢˜å»ºæ¨¡

---

NLTK å°±åƒè‡ªç„¶è¯­è¨€ä¸–ç•Œçš„æ˜¾å¾®é•œâ€”â€”
é€è¿‡å®ƒï¼Œä½ èƒ½çœ‹åˆ°è¯­è¨€çš„ç»“æ„ã€è§„å¾‹ä¸å¥‡å¦™çš„æ··æ²Œã€‚
è€Œå½“ä½ æŠŠå®ƒä¸ç°ä»£ç¥ç»ç½‘ç»œç»“åˆï¼Œä½ å°±èƒ½åœ¨è®¡ç®—æœºä¸­â€œé‡å»ºæ„ä¹‰â€ã€‚

---

æ˜¯å¦å¸Œæœ›æˆ‘åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç»§ç»­æ‰©å±•å‡ºä¸€ä»½
**ã€ŒNLTK å®æˆ˜é¡¹ç›®é›†ã€**ï¼ˆä¾‹å¦‚ï¼šæ–‡æœ¬åˆ†ç±»ã€èˆ†æƒ…åˆ†æã€å…³é”®è¯æå–ã€çŸ¥è¯†å›¾è°±æ„å»ºç­‰å®æˆ˜æ•™ç¨‹ï¼‰ï¼Ÿ

