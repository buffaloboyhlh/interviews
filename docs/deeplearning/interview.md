# æ·±åº¦å­¦ä¹ 

## ä¸€ã€ç¥ç»ç½‘ç»œ

![ç¥ç»ç½‘ç»œ.png](../imgs/deeplearning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

### ğŸ§  ä¸€ã€ç¥ç»ç½‘ç»œåŸç†

ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§**æ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒç»“æ„**çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> é€šè¿‡å¤šå±‚éçº¿æ€§å˜æ¢ï¼Œå°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ°è¾“å‡ºç»“æœã€‚

---

#### 1ï¸âƒ£ ç¥ç»å…ƒæ¨¡å‹ï¼ˆPerceptronï¼‰

æ¯ä¸ªç¥ç»å…ƒæ¥å—è¾“å…¥ $x_1, x_2, \dots, x_n$ï¼Œè®¡ç®—åŠ æƒå’Œå†åŠ ä¸Šåç½®ï¼š

$$
z = \sum_{i=1}^n w_i x_i + b
$$

ç„¶åé€šè¿‡**æ¿€æ´»å‡½æ•°** $f(z)$ å¾—åˆ°è¾“å‡ºï¼š

$$
a = f(z)
$$

å¸¸è§æ¿€æ´»å‡½æ•°ï¼š

| æ¿€æ´»å‡½æ•°    | è¡¨è¾¾å¼                                       | ç‰¹ç‚¹                 |
| ------- | ----------------------------------------- | ------------------ |
| Sigmoid | $f(z) = \frac{1}{1+e^{-z}}$               | å¹³æ»‘ã€é€‚åˆäºŒåˆ†ç±»ï¼Œä½†å¯èƒ½æ¢¯åº¦æ¶ˆå¤±   |
| ReLU    | $f(z) = \max(0, z)$                       | å¸¸ç”¨ã€æ”¶æ•›å¿«             |
| Tanh    | $f(z) = \tanh(z)$                         | è¾“å‡ºèŒƒå›´ $(-1,1)$ï¼Œå¯¹ç§°æ€§å¥½ |
| Softmax | $f_i(z) = \frac{e^{z_i}}{\sum_j e^{z_j}}$ | å¤šåˆ†ç±»è¾“å‡º              |

---

#### 2ï¸âƒ£ ç½‘ç»œç»“æ„ï¼ˆNetwork Architectureï¼‰

å…¸å‹å‰é¦ˆç¥ç»ç½‘ç»œç»“æ„ï¼š

$$
\text{Input} \rightarrow \text{Hidden Layers} \rightarrow \text{Output}
$$

æ¯ä¸€å±‚çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚

ä¾‹å¦‚ä¸€ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œï¼š

* è¾“å…¥å±‚ï¼š$x \in \mathbb{R}^n$
* éšè—å±‚ï¼š$h = f(W_1 x + b_1)$
* è¾“å‡ºå±‚ï¼š$\hat{y} = g(W_2 h + b_2)$

---

### âš™ï¸ äºŒã€æ•°å­¦æ¨å¯¼è¿‡ç¨‹

#### 1ï¸âƒ£ å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰

è¾“å…¥ $x$ ç»å„å±‚çº¿æ€§å˜æ¢ + æ¿€æ´»å‡½æ•°ï¼š
$$
z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} \
a^{(l)} = f^{(l)}(z^{(l)})
$$

æœ€ç»ˆè¾“å‡ºé¢„æµ‹ï¼š
$$
\hat{y} = a^{(L)}
$$
å…¶ä¸­ $L$ æ˜¯ç½‘ç»œçš„å±‚æ•°ã€‚

---

#### 2ï¸âƒ£ æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰

##### ï¼ˆ1ï¼‰å›å½’é—®é¢˜

å¸¸ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼š
$$
L = \frac{1}{2m}\sum_{i=1}^{m} (\hat{y}_i - y_i)^2
$$

##### ï¼ˆ2ï¼‰åˆ†ç±»é—®é¢˜

å¸¸ç”¨äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropyï¼‰ï¼š
$$
L = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_{ik} \log(\hat{y}_{ik})
$$

---

#### 3ï¸âƒ£ åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰

ç›®æ ‡ï¼šæœ€å°åŒ–æŸå¤±å‡½æ•° $L$ã€‚
ä½¿ç”¨ **æ¢¯åº¦ä¸‹é™æ³•ï¼ˆGradient Descentï¼‰** æ›´æ–°å‚æ•°ã€‚

å¯¹æƒé‡æ±‚å¯¼ï¼š
$$
\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial a^{(l)}} \cdot \frac{\partial a^{(l)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial W^{(l)}}
$$

æ›´æ–°å‚æ•°ï¼š
$$
W^{(l)} := W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}, \quad
b^{(l)} := b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}
$$

å…¶ä¸­ $\eta$ ä¸ºå­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ã€‚

---

### ğŸ“Š ä¸‰ã€è¯„ä¼°æŒ‡æ ‡

| ç±»å‹ | æŒ‡æ ‡             | å…¬å¼                                                      |
| -- | -------------- | ------------------------------------------------------- |
| åˆ†ç±» | å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰  | $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$   |
| åˆ†ç±» | ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ | $\text{Precision} = \frac{TP}{TP + FP}$                 |
| åˆ†ç±» | å¬å›ç‡ï¼ˆRecallï¼‰    | $\text{Recall} = \frac{TP}{TP + FN}$                    |
| åˆ†ç±» | F1 åˆ†æ•°          | $F1 = 2 \times \frac{P \times R}{P + R}$                |
| å›å½’ | å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰      | $\text{MSE} = \frac{1}{n}\sum(\hat{y} - y)^2$           |
| å›å½’ | $R^2$          | $R^2 = 1 - \frac{\sum(\hat{y}-y)^2}{\sum(y-\bar{y})^2}$ |

---

### ğŸ’» å››ã€å®ç°ä»£ç ï¼ˆPyTorchï¼‰

ä»¥ä¸‹ç¤ºä¾‹æ˜¯ä¸€ä¸ªç®€å•çš„**ä¸¤å±‚ç¥ç»ç½‘ç»œ**å®ç°äºŒåˆ†ç±»ä»»åŠ¡ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. æ•°æ®å‡†å¤‡
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = torch.FloatTensor(X_train)
y_train = torch.LongTensor(y_train)
X_test = torch.FloatTensor(X_test)
y_test = torch.LongTensor(y_test)

# 2. æ¨¡å‹å®šä¹‰
class NeuralNetwork(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = NeuralNetwork(2, 16, 2)

# 3. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 4. è®­ç»ƒè¿‡ç¨‹
for epoch in range(200):
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 20 == 0:
        print(f"Epoch [{epoch+1}/200], Loss: {loss.item():.4f}")

# 5. è¯„ä¼°
with torch.no_grad():
    y_pred = model(X_test)
    acc = (y_pred.argmax(1) == y_test).float().mean()
    print(f"Test Accuracy: {acc:.4f}")
```

---

### ğŸ§© äº”ã€æ¨¡å‹ä¼˜åŒ–æ–¹æ³•

| æ–¹æ³•                          | è¯´æ˜                  |
| --------------------------- | ------------------- |
| **å­¦ä¹ ç‡è°ƒæ•´ï¼ˆLR Schedulerï¼‰**     | æ§åˆ¶å­¦ä¹ ç‡è¡°å‡             |
| **æƒé‡åˆå§‹åŒ–**                   | Xavierã€He åˆå§‹åŒ–èƒ½æ”¹å–„æ”¶æ•›  |
| **æ­£åˆ™åŒ–**                     | L2 æ­£åˆ™ã€Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ |
| **Batch Normalization**     | ç¨³å®šåˆ†å¸ƒï¼ŒåŠ é€Ÿè®­ç»ƒ           |
| **æ—©åœæ³•ï¼ˆEarly Stoppingï¼‰**     | é˜²æ­¢è¿‡æ‹Ÿåˆ               |
| **æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰** | æ‰©å……æ ·æœ¬é›†ï¼Œæé«˜æ³›åŒ–æ€§         |

---

### âš ï¸ å…­ã€æ³¨æ„äº‹é¡¹

1. è¾“å…¥æ•°æ®éœ€**æ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–**ï¼›
2. é¿å…å­¦ä¹ ç‡è¿‡å¤§æˆ–è¿‡å°ï¼›
3. ReLU å¯è§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œä½†æ³¨æ„ â€œReLU æ­»äº¡â€ é—®é¢˜ï¼›
4. ç½‘ç»œå±‚æ•°è¿‡å¤šå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼›
5. ä½¿ç”¨ GPU å¯æ˜¾è‘—åŠ é€Ÿè®­ç»ƒï¼›
6. åˆç†é€‰æ‹©æ‰¹å¤§å°ï¼ˆbatch sizeï¼‰ã€‚

---

### âš–ï¸ ä¸ƒã€ä¼˜ç¼ºç‚¹æ€»ç»“

| ä¼˜ç‚¹              | ç¼ºç‚¹          |
| --------------- | ----------- |
| èƒ½å­¦ä¹ å¤æ‚éçº¿æ€§å…³ç³»      | éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æº |
| æ³›åŒ–èƒ½åŠ›å¼º           | ä¸æ˜“è§£é‡Šï¼ˆé»‘ç®±ï¼‰    |
| é€‚ç”¨èŒƒå›´å¹¿ï¼ˆåˆ†ç±»ã€å›å½’ã€ç”Ÿæˆï¼‰ | è¶…å‚æ•°è°ƒèŠ‚å›°éš¾     |
| å¯ç«¯åˆ°ç«¯å­¦ä¹           | å®¹æ˜“è¿‡æ‹Ÿåˆ       |

---

### ğŸ§­ å…«ã€å­¦ä¹ å»ºè®®ä¸è¿›é˜¶è·¯çº¿

| é˜¶æ®µ | å­¦ä¹ å†…å®¹                 | å·¥å…·                       |
| -- | -------------------- | ------------------------ |
| å…¥é—¨ | æ„ŸçŸ¥æœºã€å‰å‘ä¼ æ’­ã€æ¿€æ´»å‡½æ•°        | Numpy                    |
| è¿›é˜¶ | åå‘ä¼ æ’­ã€ä¼˜åŒ–å™¨ã€æ­£åˆ™åŒ–         | PyTorch                  |
| æå‡ | CNNã€RNNã€LSTM         | PyTorch / TensorFlow     |
| é«˜é˜¶ | Transformerã€é¢„è®­ç»ƒæ¨¡å‹    | HuggingFace Transformers |
| éƒ¨ç½² | ONNXã€TensorRTã€Triton | æ·±åº¦å­¦ä¹ éƒ¨ç½²æ¡†æ¶                 |



## äºŒã€CNN

![å·ç§¯ç¥ç»ç½‘ç»œ.png](../imgs/deeplearning/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

### ğŸ§  ä¸€ã€CNN åŸç†ï¼ˆConvolutional Neural Networkï¼‰

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸€ç±»**ä¸“ä¸ºå¤„ç†å…·æœ‰ç½‘æ ¼ç»“æ„æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰**è®¾è®¡çš„ç¥ç»ç½‘ç»œã€‚
ä¼ ç»Ÿç¥ç»ç½‘ç»œå¯¹è¾“å…¥ç‰¹å¾å®Œå…¨è¿æ¥ï¼Œè€Œ CNN é€šè¿‡ **å±€éƒ¨æ„Ÿå—é‡ï¼ˆlocal receptive fieldï¼‰** å’Œ **æƒå€¼å…±äº«ï¼ˆweight sharingï¼‰**ï¼Œæ˜¾è‘—å‡å°‘å‚æ•°æ•°é‡å¹¶æå‡ç‰¹å¾æå–èƒ½åŠ›ã€‚

---

#### 1ï¸âƒ£ CNN çš„æ ¸å¿ƒæ€æƒ³

1. **å·ç§¯å±‚ï¼ˆConvolution Layerï¼‰**ï¼šæå–å±€éƒ¨ç‰¹å¾
2. **æ± åŒ–å±‚ï¼ˆPooling Layerï¼‰**ï¼šé™ç»´ä¸é˜²æ­¢è¿‡æ‹Ÿåˆ
3. **å…¨è¿æ¥å±‚ï¼ˆFully Connected Layerï¼‰**ï¼šæ•´åˆç‰¹å¾è¿›è¡Œåˆ†ç±»æˆ–å›å½’

å…¸å‹ç»“æ„ï¼š

$$
\text{Input} \rightarrow [\text{Conv + ReLU + Pool}]^n \rightarrow \text{FC} \rightarrow \text{Output}
$$

---

#### 2ï¸âƒ£ å·ç§¯æ“ä½œï¼ˆConvolution Operationï¼‰

ä»¥äºŒç»´å·ç§¯ä¸ºä¾‹ï¼Œç»™å®šè¾“å…¥çŸ©é˜µ $X$ å’Œå·ç§¯æ ¸ï¼ˆæƒé‡çŸ©é˜µï¼‰$K$ï¼š

$$
Y(i,j) = \sum_m \sum_n X(i+m, j+n) \cdot K(m,n)
$$

è¯¥æ“ä½œç§°ä¸º**å·ç§¯ï¼ˆConvolutionï¼‰**ã€‚
å·ç§¯å±‚é€šè¿‡æ»‘åŠ¨å·ç§¯æ ¸åœ¨è¾“å…¥ä¸Šæå–å±€éƒ¨ç‰¹å¾ï¼Œå¦‚è¾¹ç¼˜ã€çº¹ç†ç­‰ã€‚

---

#### 3ï¸âƒ£ ç‰¹å¾å›¾ï¼ˆFeature Mapï¼‰

æ¯ä¸ªå·ç§¯æ ¸å¯å­¦ä¹ ä¸€ç§ç‰¹å¾æ¨¡å¼ã€‚
ä¸€ä¸ªå·ç§¯å±‚å¯ä»¥åŒ…å«å¤šä¸ªå·ç§¯æ ¸ï¼Œä»è€Œç”Ÿæˆå¤šä¸ªç‰¹å¾å›¾ï¼ˆFeature Mapï¼‰ã€‚

ä¾‹å¦‚è¾“å…¥å¤§å°ä¸º $(H, W, C_{\text{in}})$ï¼Œå·ç§¯æ ¸å¤§å°ä¸º $(k, k, C_{\text{in}}, C_{\text{out}})$ï¼Œåˆ™è¾“å‡ºç‰¹å¾å›¾å¤§å°ä¸ºï¼š

$$
H_{\text{out}} = \frac{H - k + 2p}{s} + 1, \quad
W_{\text{out}} = \frac{W - k + 2p}{s} + 1
$$

å…¶ä¸­ï¼š

* $p$ï¼špaddingï¼ˆå¡«å……ï¼‰
* $s$ï¼šstrideï¼ˆæ­¥å¹…ï¼‰

---

#### 4ï¸âƒ£ æ± åŒ–å±‚ï¼ˆPooling Layerï¼‰

ç”¨äº**é™ç»´**ä¸**ç‰¹å¾ä¸å˜æ€§æå–**ã€‚

å¸¸è§æ± åŒ–æ–¹å¼ï¼š

* **æœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰**ï¼šå–çª—å£å†…æœ€å¤§å€¼
* **å¹³å‡æ± åŒ–ï¼ˆAverage Poolingï¼‰**ï¼šå–çª—å£å†…å‡å€¼

å…¬å¼ï¼š

$$
Y(i,j) = \max_{m,n} X(i+m, j+n)
$$

---

#### 5ï¸âƒ£ æ¿€æ´»å‡½æ•°

å·ç§¯å±‚è¾“å‡ºåé€šå¸¸æ¥ **ReLU**ï¼š
$$
f(z) = \max(0, z)
$$

ReLU è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒåŠ å¿«ç½‘ç»œæ”¶æ•›ã€‚

---

#### 6ï¸âƒ£ å…¨è¿æ¥å±‚ï¼ˆFully Connected Layerï¼‰

å·ç§¯å±‚è¾“å‡ºçš„ç‰¹å¾å±•å¹³åè¾“å…¥å…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»ï¼š
$$
z = W \cdot a + b, \quad
\hat{y} = \text{Softmax}(z)
$$

---

### ğŸ§® äºŒã€æ•°å­¦æ¨å¯¼ä¸æŸå¤±å‡½æ•°

##### 1ï¸âƒ£ å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰

å¯¹äºå·ç§¯å±‚ï¼š
$$
z_{i,j}^{(l)} = \sum_{m,n,c} a_{m+i, n+j, c}^{(l-1)} \cdot w_{m,n,c}^{(l)} + b^{(l)}
$$
$$
a_{i,j}^{(l)} = f(z_{i,j}^{(l)})
$$

å¯¹äºå…¨è¿æ¥å±‚ï¼š
$$
a^{(L)} = f(W^{(L)}a^{(L-1)} + b^{(L)})
$$

---

##### 2ï¸âƒ£ æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰

**åˆ†ç±»ä»»åŠ¡**å¸¸ç”¨äº¤å‰ç†µæŸå¤±ï¼š
$$
L = -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_{ik}\log(\hat{y}_{ik})
$$

---

##### 3ï¸âƒ£ åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰

åå‘ä¼ æ’­é€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ã€‚
å¯¹äºå·ç§¯å±‚ä¸­çš„æƒé‡æ¢¯åº¦ï¼š

$$
\frac{\partial L}{\partial w_{m,n,c}} = \sum_{i,j} \frac{\partial L}{\partial z_{i,j}} \cdot a_{i+m, j+n, c}^{(l-1)}
$$

å¯¹äºåç½®ï¼š
$$
\frac{\partial L}{\partial b} = \sum_{i,j} \frac{\partial L}{\partial z_{i,j}}
$$

---

### ğŸ“Š ä¸‰ã€è¯„ä¼°æŒ‡æ ‡

| ä»»åŠ¡ç±»å‹ | å¸¸ç”¨æŒ‡æ ‡                                 |
| ---- | ------------------------------------ |
| åˆ†ç±»   | Accuracy, Precision, Recall, F1, AUC |
| å›å½’   | MSE, RMSE, MAE, $R^2$                |
| ç›®æ ‡æ£€æµ‹ | mAPï¼ˆmean Average Precisionï¼‰          |
| å›¾åƒåˆ†å‰² | IoUï¼ˆIntersection over Unionï¼‰         |

---

### ğŸ’» å››ã€å®ç°ä»£ç ï¼ˆPyTorchï¼‰

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç»å…¸çš„ CNN å›¾åƒåˆ†ç±»ç¤ºä¾‹ï¼ˆä½¿ç”¨ MNIST æ•°æ®é›†ï¼‰ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 1. æ•°æ®å‡†å¤‡
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
train_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_data = datasets.MNIST(root='./data', train=False, transform=transform)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)

# 2. æ¨¡å‹å®šä¹‰
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)   # 28x28 -> 26x26
        self.conv2 = nn.Conv2d(32, 64, 3, 1)  # 26x26 -> 24x24
        self.pool = nn.MaxPool2d(2)           # 24x24 -> 12x12
        self.fc1 = nn.Linear(64*12*12, 128)
        self.fc2 = nn.Linear(128, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = CNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 3. è®­ç»ƒè¿‡ç¨‹
for epoch in range(5):
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    print(f"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}")

# 4. æµ‹è¯•è¯„ä¼°
correct = 0
total = 0
with torch.no_grad():
    for data, target in test_loader:
        output = model(data)
        _, pred = torch.max(output.data, 1)
        total += target.size(0)
        correct += (pred == target).sum().item()

print(f"Test Accuracy: {100 * correct / total:.2f}%")
```

---

### ğŸ§© äº”ã€æ¨¡å‹ä¼˜åŒ–æŠ€å·§

| ä¼˜åŒ–æ–¹æ³•                    | è¯´æ˜                                |
| ----------------------- | --------------------------------- |
| **æ•°æ®å¢å¼º**                | ç¿»è½¬ã€æ—‹è½¬ã€è£å‰ªç­‰æ–¹å¼æ‰©å……æ•°æ®                   |
| **Dropout**             | éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ                     |
| **Batch Normalization** | ç¨³å®šè®­ç»ƒï¼Œæé«˜æ”¶æ•›é€Ÿåº¦                       |
| **å­¦ä¹ ç‡è°ƒæ•´**               | ä½¿ç”¨è°ƒåº¦å™¨ï¼ˆå¦‚ StepLRã€ReduceLROnPlateauï¼‰ |
| **æƒé‡åˆå§‹åŒ–**               | He åˆå§‹åŒ–å¸¸ç”¨äº ReLU                    |
| **è¿ç§»å­¦ä¹ **                | ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ ResNetã€VGGï¼‰å¾®è°ƒ           |

---

### âš ï¸ å…­ã€æ³¨æ„äº‹é¡¹

1. è¾“å…¥æ•°æ®éœ€ **å½’ä¸€åŒ–**ï¼ˆNormalizationï¼‰ï¼›
2. å°å·ç§¯æ ¸ï¼ˆå¦‚ 3Ã—3ï¼‰é€šå¸¸æ•ˆæœæ›´å¥½ï¼›
3. å¢åŠ å·ç§¯å±‚æ•°å¯æå–æ›´æŠ½è±¡çš„ç‰¹å¾ï¼›
4. é¿å…å·ç§¯æ ¸æ•°é‡è¿‡å¤§å¯¼è‡´è®¡ç®—é‡æš´å¢ï¼›
5. å°½é‡ä½¿ç”¨ GPU åŠ é€Ÿè®­ç»ƒï¼›
6. ä½¿ç”¨ Dropout å’Œ BN é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

---

### âš–ï¸ ä¸ƒã€ä¼˜ç¼ºç‚¹

| ä¼˜ç‚¹            | ç¼ºç‚¹        |
| ------------- | --------- |
| è‡ªåŠ¨æå–ç‰¹å¾ï¼Œæ— éœ€æ‰‹å·¥è®¾è®¡ | è®­ç»ƒæ—¶é—´é•¿     |
| å‚æ•°å…±äº«ï¼Œå‡å°‘å‚æ•°é‡    | å¯¹å°æ•°æ®é›†æ˜“è¿‡æ‹Ÿåˆ |
| å¯¹å¹³ç§»ã€ç¼©æ”¾ç­‰å…·æœ‰é²æ£’æ€§  | ä¸æ˜“è§£é‡Šï¼ˆé»‘ç®±ï¼‰  |
| é€‚åˆå›¾åƒã€è¯­éŸ³ã€è§†é¢‘ä»»åŠ¡  | ç»“æ„è®¾è®¡ä¾èµ–ç»éªŒ  |

---

### ğŸ“ˆ å…«ã€ç»å…¸ CNN æ¶æ„å‘å±•

| æ¨¡å‹        | å¹´ä»½   | ç‰¹ç‚¹                      |
| --------- | ---- | ----------------------- |
| LeNet-5   | 1998 | æœ€æ—©çš„ CNNï¼Œæ‰‹å†™æ•°å­—è¯†åˆ«          |
| AlexNet   | 2012 | ReLU + Dropout + GPU è®­ç»ƒ |
| VGG       | 2014 | ä½¿ç”¨å°å·ç§¯æ ¸å †å                 |
| GoogLeNet | 2014 | å¼•å…¥ Inception æ¨¡å—         |
| ResNet    | 2015 | æ®‹å·®è¿æ¥è§£å†³æ¢¯åº¦æ¶ˆå¤±              |
| DenseNet  | 2017 | ç‰¹å¾å¤ç”¨ï¼Œæé«˜æ¢¯åº¦æµ              |

---

### ğŸ§­ ä¹ã€å­¦ä¹ ä¸è¿›é˜¶è·¯çº¿

| é˜¶æ®µ | å­¦ä¹ å†…å®¹                    | å®è·µæ–¹å‘        |
| -- | ----------------------- | ----------- |
| å…¥é—¨ | å·ç§¯ã€æ± åŒ–ã€æ¿€æ´»å‡½æ•°              | MNIST æ‰‹å†™è¯†åˆ«  |
| è¿›é˜¶ | BatchNormã€Dropoutã€ä¼˜åŒ–å™¨   | CIFAR-10 åˆ†ç±» |
| æå‡ | ResNetã€VGGã€è¿ç§»å­¦ä¹          | ImageNet    |
| é«˜é˜¶ | Faster R-CNNã€YOLOã€U-Net | æ£€æµ‹ä¸åˆ†å‰²       |



## ä¸‰ã€RNN

![RNN.png](../imgs/deeplearning/RNN.png)

### ğŸ§  ä¸€ã€RNN åŸç†ï¼ˆRecurrent Neural Networkï¼‰

#### 1ï¸âƒ£ åŸºæœ¬æ€æƒ³

ä¼ ç»Ÿå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆå¦‚ MLPã€CNNï¼‰**è¾“å…¥ä¸è¾“å‡ºç‹¬ç«‹**ï¼Œä½†å¯¹äºåºåˆ—æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€è¯­éŸ³ã€æ—¶é—´åºåˆ—ï¼‰ï¼š

> å½“å‰æ—¶åˆ»çš„è¾“å‡ºä¸ä»…å–å†³äºå½“å‰è¾“å…¥ï¼Œè¿˜ä¾èµ–äºå‰é¢æ—¶åˆ»çš„çŠ¶æ€ã€‚

å› æ­¤ï¼ŒRNN å¼•å…¥äº†**å¾ªç¯ç»“æ„ï¼ˆRecurrent Structureï¼‰**ï¼Œèƒ½å¤Ÿâ€œè®°ä½â€å‰ä¸€æ—¶åˆ»çš„ä¿¡æ¯ã€‚

---

#### 2ï¸âƒ£ ç»“æ„å›¾ï¼ˆæ ¸å¿ƒæ¦‚å¿µï¼‰

RNN çš„åŸºæœ¬å•å…ƒå¯è¡¨ç¤ºä¸ºï¼š

$$
h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)
$$

$$
\hat{y}*t = g(W*{hy}h_t + b_y)
$$

å…¶ä¸­ï¼š

* $x_t$ï¼šæ—¶åˆ» $t$ çš„è¾“å…¥
* $h_t$ï¼šéšè—çŠ¶æ€ï¼ˆéšå«è®°å¿†ï¼‰
* $\hat{y}_t$ï¼šè¾“å‡º
* $W_{xh}$ï¼šè¾“å…¥åˆ°éšè—å±‚çš„æƒé‡
* $W_{hh}$ï¼šéšè—å±‚åˆ°éšè—å±‚çš„æƒé‡ï¼ˆå¾ªç¯ï¼‰
* $W_{hy}$ï¼šéšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡

---

### âš™ï¸ äºŒã€æ•°å­¦æ¨å¯¼è¿‡ç¨‹

#### 1ï¸âƒ£ å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰

è¾“å…¥åºåˆ— $x = [x_1, x_2, ..., x_T]$ï¼š

éšè—çŠ¶æ€æ›´æ–°ï¼š
$$
h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)
$$

è¾“å‡ºï¼š
$$
\hat{y}*t = g(W*{hy}h_t + b_y)
$$

å…¶ä¸­ $f$ é€šå¸¸ä¸º $\tanh$ æˆ– $\text{ReLU}$ï¼Œ$g$ å¸¸ä¸º $\text{Softmax}$ã€‚

---

#### 2ï¸âƒ£ æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰

å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œé€šå¸¸ä½¿ç”¨ **äº¤å‰ç†µæŸå¤±**ï¼š

$$
L = -\frac{1}{T}\sum_{t=1}^{T} y_t \log(\hat{y}_t)
$$

---

#### 3ï¸âƒ£ åå‘ä¼ æ’­ï¼ˆBackpropagation Through Time, BPTTï¼‰

RNN çš„æ¢¯åº¦è¦æ²¿æ—¶é—´å±•å¼€ï¼Œåå‘ä¼ æ’­åˆ°æ¯ä¸ªæ—¶é—´æ­¥ã€‚

æ¢¯åº¦è®¡ç®—å…¬å¼ï¼š

$$
\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^{T} \frac{\partial L_t}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_{hh}}
$$

è€Œç”±äºéšè—çŠ¶æ€é—´å­˜åœ¨ä¾èµ–å…³ç³»ï¼š

$$
\frac{\partial h_t}{\partial W_{hh}} = \frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial W_{hh}} + \frac{\partial h_t}{\partial W_{hh}}
$$

å› æ­¤ï¼Œä¼šå‡ºç° **æ¢¯åº¦æ¶ˆå¤± / æ¢¯åº¦çˆ†ç‚¸** é—®é¢˜ã€‚
è§£å†³æ–¹æ¡ˆï¼šæ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰ã€LSTMã€GRUã€‚

---

### ğŸ“Š ä¸‰ã€è¯„ä¼°æŒ‡æ ‡

| ä»»åŠ¡ç±»å‹ | å¸¸ç”¨æŒ‡æ ‡                                  |
| ---- | ------------------------------------- |
| åˆ†ç±»ä»»åŠ¡ | Accuracy, Precision, Recall, F1-score |
| åºåˆ—ç”Ÿæˆ | Perplexity (å›°æƒ‘åº¦)                      |
| å›å½’ä»»åŠ¡ | MSE, RMSE                             |
| è¯­è¨€æ¨¡å‹ | BLEU, ROUGEï¼ˆè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼‰                   |

---

### ğŸ’» å››ã€å®ç°ä»£ç ï¼ˆPyTorchï¼‰

ä»¥ä¸€ä¸ªå­—ç¬¦åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼ˆRNN åŸºæœ¬ç»“æ„ï¼‰ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim

# æ¨¡å‹å®šä¹‰
class RNNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):
        super(RNNModel, self).__init__()
        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = out[:, -1, :]  # å–æœ€åæ—¶åˆ»çš„è¾“å‡º
        out = self.fc(out)
        return out

# æ¨¡æ‹Ÿæ•°æ®
X = torch.randn(100, 10, 8)  # (batch, seq_len, input_dim)
y = torch.randint(0, 2, (100,))

# è¶…å‚æ•°
input_dim = 8
hidden_dim = 32
output_dim = 2

model = RNNModel(input_dim, hidden_dim, output_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# è®­ç»ƒ
for epoch in range(50):
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/50], Loss: {loss.item():.4f}")
```

---

### ğŸ§© äº”ã€RNN çš„å¸¸è§å˜ä½“

| æ¨¡å‹                 | ç‰¹ç‚¹              | å…¬å¼                                                  |
| ------------------ | --------------- | --------------------------------------------------- |
| **LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ï¼‰**    | å¼•å…¥â€œé—¨æ§æœºåˆ¶â€é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±  | $f_t, i_t, o_t, c_t$                                |
| **GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰**    | ç®€åŒ– LSTM ç»“æ„ï¼Œå‚æ•°æ›´å°‘ | $z_t, r_t$                                          |
| **Bi-RNNï¼ˆåŒå‘ RNNï¼‰** | åŒæ—¶è€ƒè™‘å‰åä¿¡æ¯        | $h_t = [\overrightarrow{h_t}, \overleftarrow{h_t}]$ |

---

### ğŸ§® å…­ã€æ¨¡å‹ä¼˜åŒ–æ–¹æ³•

| ä¼˜åŒ–æ‰‹æ®µ                        | è¯´æ˜             |
| --------------------------- | -------------- |
| **æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰** | é™åˆ¶æ¢¯åº¦èŒƒæ•°ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸  |
| **ä½¿ç”¨ LSTM / GRU**           | è§£å†³é•¿æœŸä¾èµ–ä¸æ¢¯åº¦æ¶ˆå¤±é—®é¢˜  |
| **Batch Normalization**     | åŠ é€Ÿæ”¶æ•›           |
| **Dropout**                 | é˜²æ­¢è¿‡æ‹Ÿåˆ          |
| **å­¦ä¹ ç‡è°ƒæ•´ï¼ˆLR Schedulerï¼‰**     | åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡        |
| **Embedding å±‚**             | å¯¹ç¦»æ•£è¾“å…¥ï¼ˆå¦‚è¯ï¼‰åšç¨ å¯†è¡¨ç¤º |

---

### âš ï¸ ä¸ƒã€æ³¨æ„äº‹é¡¹

1. è¾“å…¥åºåˆ—éœ€ç»Ÿä¸€é•¿åº¦ï¼Œå¯ä½¿ç”¨ **padding**ï¼›
2. è®­ç»ƒæ—¶å¯ä½¿ç”¨ **PackedSequence** æå‡æ•ˆç‡ï¼›
3. é¿å…æ—¶é—´æ­¥è¿‡é•¿ï¼Œå¦åˆ™æ¢¯åº¦ä¼ æ’­å›°éš¾ï¼›
4. é€‚å½“ä½¿ç”¨ **Dropout / LayerNorm**ï¼›
5. è‹¥æ˜¯æ–‡æœ¬ä»»åŠ¡ï¼Œæ¨èä½¿ç”¨ **LSTM / GRU**ï¼›
6. å»ºè®®ä½¿ç”¨ GPUï¼ˆCUDAï¼‰åŠ é€Ÿã€‚

---

### âš–ï¸ å…«ã€ä¼˜ç¼ºç‚¹

| ä¼˜ç‚¹          | ç¼ºç‚¹                      |
| ----------- | ----------------------- |
| èƒ½æ•è·åºåˆ—ä¾èµ–å…³ç³»   | é•¿åºåˆ—ä¸­æ¢¯åº¦æ¶ˆå¤±                |
| å‚æ•°å…±äº«ï¼Œæ¨¡å‹è§„æ¨¡è¾ƒå° | è®­ç»ƒæ—¶é—´é•¿                   |
| èƒ½å¤„ç†å˜é•¿è¾“å…¥     | æ— æ³•å¹¶è¡Œè®¡ç®—                  |
| å¯¹æ—¶åºä»»åŠ¡æ•ˆæœå¥½    | å¯¹é•¿ä¾èµ–å»ºæ¨¡æœ‰é™ï¼ˆéœ€ LSTM/GRU æ”¹è¿›ï¼‰ |

---

### ğŸ“ˆ ä¹ã€å…¸å‹åº”ç”¨åœºæ™¯

| åº”ç”¨     | ä»»åŠ¡ç±»å‹  | ç¤ºä¾‹          |
| ------ | ----- | ----------- |
| è¯­è¨€å»ºæ¨¡   | åºåˆ—é¢„æµ‹  | ä¸‹ä¸€ä¸ªè¯é¢„æµ‹      |
| æ–‡æœ¬åˆ†ç±»   | åˆ†ç±»    | æƒ…æ„Ÿåˆ†æ        |
| åºåˆ—æ ‡æ³¨   | æ ‡æ³¨    | å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ |
| è¯­éŸ³è¯†åˆ«   | åºåˆ—åˆ°åºåˆ— | éŸ³é¢‘è½¬æ–‡å­—       |
| æ—¶é—´åºåˆ—é¢„æµ‹ | å›å½’    | è‚¡ç¥¨/å¤©æ°”é¢„æµ‹     |

---

### ğŸ§­ åã€å­¦ä¹ è·¯çº¿å»ºè®®

| é˜¶æ®µ | å†…å®¹                    | å®è·µä»»åŠ¡      |
| -- | --------------------- | --------- |
| å…¥é—¨ | åŸºæœ¬ RNN ç†è®ºä¸ç»“æ„          | ç®€å•åºåˆ—åˆ†ç±»    |
| è¿›é˜¶ | LSTMã€GRU ç†è§£ä¸å®ç°        | æ–‡æœ¬åˆ†ç±»      |
| æå‡ | åŒå‘ RNNã€Seq2Seq        | æœºå™¨ç¿»è¯‘      |
| é«˜é˜¶ | Attentionã€Transformer | é«˜çº§ NLP ä»»åŠ¡ |


## å››ã€LSTM

![LSTM.png](../imgs/deeplearning/LSTM.png)


### ğŸ§  ä¸€ã€LSTM åŸç†ï¼ˆLong Short-Term Memoryï¼‰

#### 1ï¸âƒ£ ä¸ºä»€ä¹ˆéœ€è¦ LSTMï¼Ÿ

åœ¨æ™®é€š RNN ä¸­ï¼š
$$
h_t = f(W_{xh}x_t + W_{hh}h_{t-1})
$$

æ¢¯åº¦åœ¨æ—¶é—´ä¸Šä¼ é€’æ—¶å®¹æ˜“ï¼š

* **æ¢¯åº¦æ¶ˆå¤±**ï¼ˆé•¿æœŸä¾èµ–ä¿¡æ¯æ— æ³•ä¿ç•™ï¼‰
* **æ¢¯åº¦çˆ†ç‚¸**ï¼ˆè®­ç»ƒä¸ç¨³å®šï¼‰

ğŸ”¹ **LSTM** é€šè¿‡å¼•å…¥â€œé—¨æ§æœºåˆ¶ï¼ˆGating Mechanismï¼‰â€æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œ
ä½¿å¾—ç½‘ç»œèƒ½å¤Ÿâ€œå†³å®šâ€å“ªäº›ä¿¡æ¯ä¿ç•™ã€å“ªäº›é—å¿˜ã€‚

---

#### 2ï¸âƒ£ LSTM ç»“æ„å›¾

LSTM çš„æ¯ä¸ªå•å…ƒç”±ä¸‰ä¸ªé—¨ï¼ˆGateï¼‰å’Œä¸€ä¸ªè®°å¿†å•å…ƒï¼ˆCell Stateï¼‰ç»„æˆï¼š

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
x_t â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ è¾“å…¥é—¨ i_t   â”‚
h_{t-1} â”€â”€â”€â–¶â”‚ é—å¿˜é—¨ f_t   â”‚â”€â”€â”€â”
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                               â–¼
                          c_{t-1}
                               â”‚
                               â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ ç»†èƒçŠ¶æ€ c_tâ”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ è¾“å‡ºé—¨ o_t  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                              h_t
```

---

### âš™ï¸ äºŒã€LSTM æ•°å­¦æ¨å¯¼è¿‡ç¨‹

åœ¨æ—¶é—´æ­¥ $t$ï¼š

è¾“å…¥ï¼š$x_t$ã€å‰ä¸€éšè—çŠ¶æ€ $h_{t-1}$ã€å‰ä¸€ç»†èƒçŠ¶æ€ $c_{t-1}$ã€‚

---

#### 1ï¸âƒ£ é—¨æ§æœºåˆ¶å…¬å¼

##### ï¼ˆ1ï¼‰é—å¿˜é—¨ï¼ˆForget Gateï¼‰

å†³å®šè¦â€œå¿˜è®°â€å¤šå°‘æ—§ä¿¡æ¯ï¼š

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

---

##### ï¼ˆ2ï¼‰è¾“å…¥é—¨ï¼ˆInput Gateï¼‰

å†³å®šè¦æ·»åŠ å¤šå°‘æ–°ä¿¡æ¯ï¼š

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

##### ï¼ˆ3ï¼‰å€™é€‰çŠ¶æ€ï¼ˆCandidate Cellï¼‰

è®¡ç®—å½“å‰è¾“å…¥çš„å€™é€‰è®°å¿†ï¼š

$$
\tilde{c}*t = \tanh(W_c \cdot [h*{t-1}, x_t] + b_c)
$$

---

#### 2ï¸âƒ£ çŠ¶æ€æ›´æ–°

##### ï¼ˆ4ï¼‰æ›´æ–°ç»†èƒçŠ¶æ€ï¼ˆCell Stateï¼‰

å°†æ—§è®°å¿†ä¸æ–°è®°å¿†ç»“åˆï¼š

$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$

---

##### ï¼ˆ5ï¼‰è¾“å‡ºé—¨ï¼ˆOutput Gateï¼‰

å†³å®šè¾“å‡ºå¤šå°‘å†…éƒ¨è®°å¿†ï¼š

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$

---

##### ï¼ˆ6ï¼‰è®¡ç®—éšè—çŠ¶æ€ï¼ˆHidden Stateï¼‰

$$
h_t = o_t \odot \tanh(c_t)
$$

---

#### 3ï¸âƒ£ æŸå¤±å‡½æ•°

å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ï¼‰ï¼Œå¸¸ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼š

$$
L = -\sum_{t=1}^{T} y_t \log(\hat{y}_t)
$$

è‹¥è¾“å‡ºä¸ºè¿ç»­å€¼ï¼ˆå›å½’ä»»åŠ¡ï¼‰ï¼Œåˆ™ä½¿ç”¨ MSEï¼š

$$
L = \frac{1}{T} \sum_{t=1}^{T} (y_t - \hat{y}_t)^2
$$

---

#### 4ï¸âƒ£ æ¢¯åº¦ä¼ æ’­ï¼ˆBPTTï¼‰

LSTM ä»é€šè¿‡â€œæ—¶é—´åå‘ä¼ æ’­ï¼ˆBackpropagation Through Time, BPTTï¼‰â€è®­ç»ƒã€‚
ä¸åŒäº RNN çš„æ¢¯åº¦è¿ä¹˜ï¼ŒLSTM çš„ **ç»†èƒçŠ¶æ€ $c_t$** èƒ½é€šè¿‡â€œæ’ç­‰ä¼ é€’â€éƒ¨åˆ†æ¢¯åº¦ï¼Œå› æ­¤ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ã€‚

---

### ğŸ“Š ä¸‰ã€è¯„ä¼°æŒ‡æ ‡

| ä»»åŠ¡ç±»å‹ | å¸¸ç”¨æŒ‡æ ‡                                  |
| ---- | ------------------------------------- |
| åˆ†ç±»ä»»åŠ¡ | Accuracy, Precision, Recall, F1-score |
| å›å½’ä»»åŠ¡ | MSE, MAE, RMSE                        |
| åºåˆ—é¢„æµ‹ | Perplexity, BLEU, ROUGE               |

---

### ğŸ’» å››ã€PyTorch å®ç°ä»£ç 

ä»¥ä¸‹æ˜¯ä¸€ä¸ª **LSTM æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹**ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim

# LSTM æ¨¡å‹å®šä¹‰
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        out, (h_n, c_n) = self.lstm(x)
        out = self.fc(out[:, -1, :])  # å–æœ€åæ—¶åˆ»è¾“å‡º
        return out

# æ¨¡æ‹Ÿæ•°æ®
X = torch.randn(64, 10, 8)  # (batch, seq_len, input_dim)
y = torch.randint(0, 2, (64,))

# è¶…å‚æ•°
model = LSTMModel(input_dim=8, hidden_dim=32, output_dim=2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# è®­ç»ƒå¾ªç¯
for epoch in range(30):
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 5 == 0:
        print(f"Epoch [{epoch+1}/30], Loss: {loss.item():.4f}")
```

---

### ğŸ§© äº”ã€LSTM ä¸ RNN çš„åŒºåˆ«

| ç‰¹æ€§     | RNN          | LSTM                     |
| ------ | ------------ | ------------------------ |
| è®°å¿†æœºåˆ¶   | å•ä¸€éšè—çŠ¶æ€ $h_t$ | æ‹¥æœ‰ç»†èƒçŠ¶æ€ $c_t$ ä¸éšè—çŠ¶æ€ $h_t$ |
| é•¿æœŸä¾èµ–èƒ½åŠ› | å·®ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰      | å¼ºï¼ˆé€šè¿‡é—¨æ§æœºåˆ¶æ§åˆ¶ï¼‰              |
| å‚æ•°é‡    | å°‘            | å¤š                        |
| è®¡ç®—å¼€é”€   | å°            | å¤§                        |
| é€‚ç”¨åœºæ™¯   | çŸ­åºåˆ—ã€ç®€å•å…³ç³»     | é•¿åºåˆ—ã€å¤æ‚ä¾èµ–                 |

---

### ğŸ§® å…­ã€æ¨¡å‹ä¼˜åŒ–ç­–ç•¥

| æ–¹æ³•                        | è¯´æ˜                               |
| ------------------------- | -------------------------------- |
| **æ¢¯åº¦è£å‰ª**                  | é™åˆ¶æ¢¯åº¦èŒƒæ•°ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸                    |
| **Dropout**               | é¿å…è¿‡æ‹Ÿåˆï¼ˆLSTM å†…ç½®æ”¯æŒï¼‰                 |
| **åŒå‘ LSTM**               | åŒæ—¶æ•è·å‰åæ–‡ä¿¡æ¯                        |
| **å¤šå±‚ LSTM**               | æé«˜ç‰¹å¾è¡¨è¾¾èƒ½åŠ›                         |
| **BatchNorm / LayerNorm** | åŠ é€Ÿæ”¶æ•›ã€ç¨³å®šè®­ç»ƒ                        |
| **å­¦ä¹ ç‡è°ƒåº¦**                 | ä½¿ç”¨ CosineAnnealing / StepLR ç­‰ç­–ç•¥  |
| **é¢„è®­ç»ƒè¯å‘é‡**                | ä½¿ç”¨ Word2Vecã€GloVeã€BERT Embedding |

---

### âš ï¸ ä¸ƒã€æ³¨æ„äº‹é¡¹

1. **åºåˆ—é•¿åº¦è¿‡é•¿** æ—¶è®­ç»ƒå›°éš¾ï¼Œå¯ä½¿ç”¨æˆªæ–­ BPTTï¼›
2. **Batch å†…åºåˆ—é•¿åº¦ä¸ä¸€** æ—¶ï¼Œç”¨ `pack_padded_sequence`ï¼›
3. è‹¥ä»»åŠ¡éœ€è¦åŒå‘ä¿¡æ¯ï¼Œç”¨ `bidirectional=True`ï¼›
4. è¾“å‡ºç»´åº¦è¦åŒ¹é…ä»»åŠ¡ï¼ˆåˆ†ç±» vs å›å½’ï¼‰ï¼›
5. å°½é‡ä½¿ç”¨ GPUï¼›
6. é¿å…å­¦ä¹ ç‡è¿‡å¤§ï¼›
7. Dropout ä¸å®œè¿‡é«˜ï¼ˆä¸€èˆ¬ 0.3ï½0.5ï¼‰ã€‚

---

### âš–ï¸ å…«ã€ä¼˜ç¼ºç‚¹æ€»ç»“

| ä¼˜ç‚¹       | ç¼ºç‚¹                      |
| -------- | ----------------------- |
| èƒ½æ•æ‰é•¿è·ç¦»ä¾èµ– | è®­ç»ƒæ…¢ï¼Œå‚æ•°å¤š                 |
| è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ | å¯¹é•¿åºåˆ—ä»æœ‰é™åˆ¶                |
| æ³›åŒ–èƒ½åŠ›å¼º    | ä¸æ”¯æŒå¹¶è¡Œè®¡ç®—ï¼ˆç›¸è¾ƒ Transformerï¼‰ |
| è¡¨è¾¾èƒ½åŠ›é«˜    | è°ƒå‚è¾ƒå¤æ‚                   |

---

### ğŸ“ˆ ä¹ã€å…¸å‹åº”ç”¨åœºæ™¯

| é¢†åŸŸ   | ä»»åŠ¡     | ç¤ºä¾‹        |
| ---- | ------ | --------- |
| NLP  | æ–‡æœ¬åˆ†ç±»   | æƒ…æ„Ÿåˆ†æ      |
| NLP  | åºåˆ—ç”Ÿæˆ   | æœºå™¨ç¿»è¯‘      |
| è¯­éŸ³   | è¯­éŸ³è¯†åˆ«   | ASR       |
| æ—¶é—´åºåˆ— | é¢„æµ‹     | è‚¡ç¥¨ã€å¤©æ°”ã€ä¼ æ„Ÿå™¨ |
| åŒ»ç–—   | ç”Ÿç†ä¿¡å·åˆ†æ | ECG å¿ƒç”µå›¾é¢„æµ‹ |

---

### ğŸ§­ åã€å­¦ä¹ æ‹“å±•è·¯å¾„

| é˜¶æ®µ | å†…å®¹               | æ¨èå­¦ä¹           |
| -- | ---------------- | ------------- |
| å…¥é—¨ | LSTM ç†è®ºä¸ç»“æ„       | ç†è§£ä¸‰é—¨æœºåˆ¶        |
| è¿›é˜¶ | å¤šå±‚ / åŒå‘ LSTM     | æ–‡æœ¬åˆ†ç±»ä»»åŠ¡        |
| æå‡ | Seq2Seq + LSTM   | æœºå™¨ç¿»è¯‘          |
| é«˜é˜¶ | Attention + LSTM | æ–‡æœ¬ç”Ÿæˆ / å¯¹è¯ç³»ç»Ÿ   |
| æ‹“å±• | Transformer      | è¶…è¶Š LSTM çš„åºåˆ—å»ºæ¨¡ |



## äº”ã€GAN 

![GAN.png](../imgs/deeplearning/GAN.png)

### ğŸ§  ä¸€ã€GAN åŸç†ï¼ˆGenerative Adversarial Networkï¼‰

#### 1ï¸âƒ£ åŸºæœ¬æ€æƒ³

GAN ç”± **ç”Ÿæˆå™¨ï¼ˆGenerator, Gï¼‰** å’Œ **åˆ¤åˆ«å™¨ï¼ˆDiscriminator, Dï¼‰** ç»„æˆã€‚

* **ç”Ÿæˆå™¨ G**ï¼šè¯•å›¾ä»å™ªå£°ä¸­ç”Ÿæˆé€¼çœŸçš„æ ·æœ¬ï¼Œæ¬ºéª—åˆ¤åˆ«å™¨ï¼›
* **åˆ¤åˆ«å™¨ D**ï¼šè¯•å›¾åŒºåˆ†è¾“å…¥æ ·æœ¬æ˜¯çœŸå®æ•°æ®è¿˜æ˜¯ç”Ÿæˆæ•°æ®ã€‚

ä¸¤è€…å½¢æˆä¸€ä¸ª **å¯¹æŠ—åšå¼ˆï¼ˆminimax gameï¼‰**ï¼š

> G æƒ³â€œéª—è¿‡â€ Dï¼Œè€Œ D æƒ³â€œè¯†ç ´â€ Gã€‚
> æœ€ç»ˆè¾¾åˆ°ä¸€ä¸ªçº³ä»€å¹³è¡¡ï¼šG ç”Ÿæˆçš„æ ·æœ¬ä¸çœŸå®æ•°æ®å‡ ä¹æ— æ³•åŒºåˆ†ã€‚

---

#### 2ï¸âƒ£ GAN ç»“æ„ç¤ºæ„å›¾

```
      éšæœºå™ªå£° z ~ p(z)
               â”‚
               â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ç”Ÿæˆå™¨ G   â”‚â”€â”€â”€â–¶ ç”Ÿæˆæ ·æœ¬ G(z)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚         åˆ¤åˆ«å™¨ D          â”‚
     â”‚  è¾“å‡ºï¼šP(çœŸå® or ä¼ªé€ )   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–²
         çœŸå®æ ·æœ¬ x ~ p_data(x)
```

---

### âš™ï¸ äºŒã€æ•°å­¦æ¨å¯¼è¿‡ç¨‹

#### 1ï¸âƒ£ ç›®æ ‡å‡½æ•°ï¼ˆMinimax å¯¹æŠ—ï¼‰

GAN çš„æ ¸å¿ƒä¼˜åŒ–ç›®æ ‡ä¸ºï¼š

$$
\min_G \max_D V(D, G) = \mathbb{E}*{x \sim p*{\text{data}}(x)}[\log D(x)] +
\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$$

å…¶ä¸­ï¼š

* $D(x)$ è¡¨ç¤ºè¾“å…¥ä¸ºçœŸå®æ ·æœ¬çš„æ¦‚ç‡ï¼›
* $G(z)$ è¡¨ç¤ºç”Ÿæˆçš„ä¼ªæ ·æœ¬ï¼›
* $p_{\text{data}}$ æ˜¯çœŸå®æ•°æ®åˆ†å¸ƒï¼›
* $p_z$ æ˜¯å™ªå£°åˆ†å¸ƒï¼ˆå¦‚é«˜æ–¯åˆ†å¸ƒï¼‰ã€‚

---

#### 2ï¸âƒ£ åˆ¤åˆ«å™¨ç›®æ ‡

å›ºå®šç”Ÿæˆå™¨ $G$ æ—¶ï¼Œåˆ¤åˆ«å™¨ $D$ çš„æœ€ä¼˜è§£ä¸ºï¼š

$$
D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}
$$

å…¶ä¸­ $p_g(x)$ æ˜¯ç”Ÿæˆå™¨åˆ†å¸ƒã€‚

---

#### 3ï¸âƒ£ æœ€ä¼˜æƒ…å†µä¸‹çš„ç›®æ ‡å€¼

å°† $D^*(x)$ ä»£å…¥ç›®æ ‡å‡½æ•°ï¼Œæœ‰ï¼š

$$
V(G, D^*) = -\log 4 + 2 \cdot \text{JSD}(p_{\text{data}} \parallel p_g)
$$

å³ï¼ŒGAN çš„è®­ç»ƒç­‰ä»·äº **æœ€å°åŒ–æ•°æ®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒçš„ Jensenâ€“Shannon æ•£åº¦ï¼ˆJSDï¼‰**ã€‚

---

#### 4ï¸âƒ£ æŸå¤±å‡½æ•°å½¢å¼

è®­ç»ƒä¸­é€šå¸¸ä½¿ç”¨ä¸¤ä¸ªæŸå¤±å‡½æ•°ï¼š

* åˆ¤åˆ«å™¨æŸå¤±ï¼š
  $$
  L_D = -\mathbb{E}*{x \sim p*{\text{data}}}[\log D(x)] - \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
  $$

* ç”Ÿæˆå™¨æŸå¤±ï¼ˆåŸå§‹å½¢å¼ï¼‰ï¼š
  $$
  L_G = -\mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
  $$

> ä½†å®è·µä¸­å¸¸ç”¨ **éé¥±å’Œå½¢å¼**ï¼š
> $$
> L_G = -\mathbb{E}_{z \sim p_z}[\log D(G(z))]
> $$
> è¿™æ ·å¯ä»¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

---

### ğŸ“Š ä¸‰ã€è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡                                   | å«ä¹‰                 | è¯´æ˜      |
| ------------------------------------ | ------------------ | ------- |
| **FID (FrÃ©chet Inception Distance)** | è¡¡é‡ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒçš„ç‰¹å¾åˆ†å¸ƒå·®è· | è¶Šä½è¶Šå¥½    |
| **IS (Inception Score)**             | è¡¡é‡ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§ä¸è´¨é‡      | è¶Šé«˜è¶Šå¥½    |
| **Precision / Recall for GANs**      | è¡¡é‡çœŸå®æ€§ä¸å¤šæ ·æ€§          | å¹³è¡¡æŒ‡æ ‡    |
| **è§†è§‰è¯„ä¼°**                             | äººå·¥ä¸»è§‚è´¨é‡             | å¸¸ç”¨äºå›¾åƒä»»åŠ¡ |

---

### ğŸ’» å››ã€PyTorch å®ç°ä»£ç 

ä»¥ä¸‹ä¸ºä¸€ä¸ªæœ€å°å¯è¿è¡Œçš„ **GAN å®ä¾‹**ï¼ˆMNIST æ•°æ®é›†ï¼‰ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

# 1. å®šä¹‰ç”Ÿæˆå™¨
class Generator(nn.Module):
    def __init__(self, noise_dim=100, output_dim=784):
        super(Generator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(noise_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, output_dim),
            nn.Tanh()
        )

    def forward(self, z):
        return self.net(z)

# 2. å®šä¹‰åˆ¤åˆ«å™¨
class Discriminator(nn.Module):
    def __init__(self, input_dim=784):
        super(Discriminator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

# 3. åˆå§‹åŒ–æ¨¡å‹ä¸ä¼˜åŒ–å™¨
G = Generator()
D = Discriminator()
criterion = nn.BCELoss()
optimizer_G = optim.Adam(G.parameters(), lr=0.0002)
optimizer_D = optim.Adam(D.parameters(), lr=0.0002)

# 4. æ•°æ®åŠ è½½
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
data_loader = torch.utils.data.DataLoader(
    datasets.MNIST('.', train=True, download=True, transform=transform),
    batch_size=64, shuffle=True
)

# 5. è®­ç»ƒå¾ªç¯
for epoch in range(10):
    for real_imgs, _ in data_loader:
        bs = real_imgs.size(0)
        real_imgs = real_imgs.view(bs, -1)
        z = torch.randn(bs, 100)
        fake_imgs = G(z)

        # æ ‡ç­¾
        real_label = torch.ones(bs, 1)
        fake_label = torch.zeros(bs, 1)

        # --- åˆ¤åˆ«å™¨è®­ç»ƒ ---
        optimizer_D.zero_grad()
        real_loss = criterion(D(real_imgs), real_label)
        fake_loss = criterion(D(fake_imgs.detach()), fake_label)
        d_loss = real_loss + fake_loss
        d_loss.backward()
        optimizer_D.step()

        # --- ç”Ÿæˆå™¨è®­ç»ƒ ---
        optimizer_G.zero_grad()
        g_loss = criterion(D(fake_imgs), real_label)
        g_loss.backward()
        optimizer_G.step()

    print(f"Epoch [{epoch+1}/10]  D Loss: {d_loss.item():.4f}  G Loss: {g_loss.item():.4f}")
```

---

### ğŸ§© äº”ã€å¸¸è§ GAN å˜ä½“

| æ¨¡å‹                         | ç‰¹ç‚¹                | æŸå¤±å‡½æ•°æ”¹è¿›             |
| -------------------------- | ----------------- | ------------------ |
| **DCGAN**                  | å·ç§¯ç»“æ„ï¼ˆå›¾åƒç”Ÿæˆï¼‰        | ç¨³å®šè®­ç»ƒ               |
| **WGAN**                   | ä½¿ç”¨ Wasserstein è·ç¦» | æ”¹å–„æ¨¡å¼å´©æºƒ             |
| **WGAN-GP**                | åŠ å…¥æ¢¯åº¦æƒ©ç½š            | æ”¶æ•›æ›´ç¨³å®š              |
| **Conditional GAN (cGAN)** | æ¡ä»¶ç”Ÿæˆ              | $G(z, y), D(x, y)$ |
| **CycleGAN**               | æ— éœ€é…å¯¹æ ·æœ¬çš„å›¾åƒè½¬æ¢       | ç”¨å¾ªç¯ä¸€è‡´æ€§æŸå¤±           |
| **StyleGAN**               | æ§åˆ¶ç”Ÿæˆå›¾åƒé£æ ¼          | é«˜è´¨é‡å›¾åƒç”Ÿæˆ            |

---

### ğŸ§® å…­ã€æ¨¡å‹ä¼˜åŒ–æŠ€å·§

| æ–¹æ³•                         | è¯´æ˜                   |
| -------------------------- | -------------------- |
| **æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰**  | å°†çœŸå®æ ‡ç­¾ä» 1 æ”¹ä¸º 0.9ï¼Œç¨³å®šè®­ç»ƒ |
| **ç‰¹å¾åŒ¹é…ï¼ˆFeature Matchingï¼‰** | ç”¨ä¸­é—´ç‰¹å¾å±‚è®­ç»ƒç”Ÿæˆå™¨          |
| **ä½¿ç”¨ Wasserstein æŸå¤±**      | ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ã€æ¨¡å¼å´©æºƒ          |
| **æ¢¯åº¦æƒ©ç½šï¼ˆGradient Penaltyï¼‰** | ä¿è¯ 1-Lipschitz æ¡ä»¶    |
| **è°±å½’ä¸€åŒ–ï¼ˆSpectral Normï¼‰**    | é™åˆ¶æƒé‡èŒƒæ•°               |
| **å­¦ä¹ ç‡åˆ†ç¦»**                  | G çš„å­¦ä¹ ç‡ç•¥é«˜äº D          |

---

### âš ï¸ ä¸ƒã€æ³¨æ„äº‹é¡¹

1. GAN è®­ç»ƒ **æä¸ç¨³å®š**ï¼›
2. D å¤ªå¼º â‡’ G æ— æ¢¯åº¦ï¼›D å¤ªå¼± â‡’ G æ¬ºéª—å®¹æ˜“ï¼›
3. åˆå§‹å­¦ä¹ ç‡éœ€å°ï¼›
4. éœ€åŒæ—¶ç›‘æ§ D ä¸ G çš„æŸå¤±ï¼›
5. å»ºè®®ä½¿ç”¨ **WGAN / WGAN-GP**ï¼›
6. å¯¹å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œæ¨è **DCGAN æ¶æ„**ï¼›
7. è®­ç»ƒä¸­å¯åŠ¨æ€è°ƒæ•´åˆ¤åˆ«å™¨è®­ç»ƒæ¬¡æ•°ã€‚

---

### âš–ï¸ å…«ã€ä¼˜ç¼ºç‚¹

| ä¼˜ç‚¹         | ç¼ºç‚¹           |
| ---------- | ------------ |
| ç”Ÿæˆè´¨é‡é«˜      | è®­ç»ƒä¸ç¨³å®š        |
| æ— éœ€æ˜¾å¼å»ºæ¨¡æ¦‚ç‡å¯†åº¦ | æ¨¡å¼å´©æºƒï¼ˆç”Ÿæˆæ ·æœ¬å•ä¸€ï¼‰ |
| ç†è®ºä¸Šå¯ç”Ÿæˆä»»æ„åˆ†å¸ƒ | è¯„ä»·æŒ‡æ ‡ä¸å®Œç¾      |
| åº”ç”¨å¹¿æ³›       | å¯¹è¶…å‚æ•°æ•æ„Ÿ       |

---

### ğŸ“ˆ ä¹ã€å…¸å‹åº”ç”¨åœºæ™¯

| é¢†åŸŸ      | åº”ç”¨             | ç¤ºä¾‹              |
| ------- | -------------- | --------------- |
| å›¾åƒç”Ÿæˆ    | æ‰‹å†™æ•°å­—ã€äººè„¸ç”Ÿæˆ      | DCGAN, StyleGAN |
| å›¾åƒåˆ°å›¾åƒè½¬æ¢ | é©¬â†”æ–‘é©¬ã€å¤â†”å†¬       | CycleGAN        |
| è¶…åˆ†è¾¨ç‡é‡å»º  | SRGAN          | å›¾åƒæ¸…æ™°åŒ–           |
| æ•°æ®å¢å¼º    | åŒ»å­¦å½±åƒã€è¯­éŸ³ç”Ÿæˆ      | cGAN            |
| æ–‡æœ¬åˆ°å›¾åƒ   | Text2Image GAN | æ–‡æœ¬ç”Ÿæˆå›¾åƒ          |

---

### ğŸ§­ åã€å­¦ä¹ æ‹“å±•è·¯çº¿

| é˜¶æ®µ   | å­¦ä¹ ç›®æ ‡                | ç¤ºä¾‹       |
| ---- | ------------------- | -------- |
| å…¥é—¨   | ç†è§£ GAN å¯¹æŠ—æœºåˆ¶         | è®­ç»ƒåŸå§‹ GAN |
| è¿›é˜¶   | DCGAN / WGAN-GP ç†è§£  | å›¾åƒç”Ÿæˆä»»åŠ¡   |
| æå‡   | æ¡ä»¶ç”Ÿæˆã€CycleGAN       | é£æ ¼è¿ç§»     |
| é«˜é˜¶   | StyleGAN, Diffusion | é«˜åˆ†è¾¨ç‡äººè„¸ç”Ÿæˆ |
| ç ”ç©¶æ–¹å‘ | ç†è®ºç¨³å®šæ€§ã€å¯¹æŠ—é²æ£’æ€§         | GAN ç†è®ºåˆ†æ |

